



Normal distribution - Wikipedia
document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );
(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Normal_distribution","wgTitle":"Normal distribution","wgCurRevisionId":774515637,"wgRevisionId":774515637,"wgArticleId":21462,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from June 2011","Pages with login required references or sources","Pages using ISBN magic links","Use mdy dates from August 2012","Pages using deprecated image syntax","Articles with unsourced statements from June 2010","CS1 Latin-language sources (la)","Articles with example Pascal code","Continuous distributions","Conjugate prior distributions","Normal distribution","Exponential family distributions","Stable distributions","Location-scale family probability distributions"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Normal_distribution","wgRelevantArticleId":21462,"wgRequestId":"WOl@0wpAMEwAAGS7HVAAAAAT","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":false,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesBetaFeatureEnabled":false,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q133871","wgCentralAuthMobileDomain":false,"wgVisualEditorToolbarScrollOffset":0,"wgEditSubmitButtonLabelPublish":false});mw.loader.state({"ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"loading","user.tokens":"loading","ext.math.styles":"ready","ext.cite.styles":"ready","ext.pygments":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["ext.math.scripts","ext.cite.a11y","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.legacy.wikibits","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.gadget.featured-articles-links","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});



















 






Normal distribution

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search

This article is about the univariate normal distribution. For normally distributed vectors, see Multivariate normal distribution.
"Bell curve" redirects here. For other uses, see Bell curve (disambiguation).

Normal distribution


Probability density function

The red curve is the standard normal distribution



Cumulative distribution function



Notation






N


(
μ
,


σ

2


)


{\displaystyle {\mathcal {N}}(\mu ,\,\sigma ^{2})}




Parameters
μ ∈ R — mean (location)
σ2 > 0 — variance (squared scale)


Support
x ∈ R


PDF






1

2

σ

2


π





e

−



(
x
−
μ

)

2




2

σ

2









{\displaystyle {\frac {1}{\sqrt {2\sigma ^{2}\pi }}}\,e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}}




CDF






1
2



[
1
+
erf
⁡

(



x
−
μ


σ


2





)

]



{\displaystyle {\frac {1}{2}}\left[1+\operatorname {erf} \left({\frac {x-\mu }{\sigma {\sqrt {2}}}}\right)\right]}




Quantile




μ
+
σ


2



erf

−
1


⁡
(
2
F
−
1
)


{\displaystyle \mu +\sigma {\sqrt {2}}\operatorname {erf} ^{-1}(2F-1)}




Mean
μ


Median
μ


Mode
μ


Variance





σ

2





{\displaystyle \sigma ^{2}\,}




Skewness
0


Ex. kurtosis
0


Entropy







1
2



ln
⁡
(
2
π

e


σ

2


)


{\displaystyle {\tfrac {1}{2}}\ln(2\pi \,e\,\sigma ^{2})}




MGF




exp
⁡
{
μ
t
+


1
2



σ

2



t

2


}


{\displaystyle \exp\{\mu t+{\frac {1}{2}}\sigma ^{2}t^{2}\}}




CF




exp
⁡
{
i
μ
t
−


1
2



σ

2



t

2


}


{\displaystyle \exp\{i\mu t-{\frac {1}{2}}\sigma ^{2}t^{2}\}}




Fisher information






(



1

/


σ

2




0




0


1

/

(
2

σ

4


)



)




{\displaystyle {\begin{pmatrix}1/\sigma ^{2}&0\\0&1/(2\sigma ^{4})\end{pmatrix}}}




In probability theory, the normal (or Gaussian) distribution is a very common continuous probability distribution. Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known.[1][2]
The normal distribution is useful because of the central limit theorem. In its most general form, under some conditions (which include finite variance), it states that averages of random variables independently drawn from independent distributions converge in distribution to the normal, that is, become normally distributed when the number of random variables is sufficiently large. Physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have distributions that are nearly normal.[3] Moreover, many results and methods (such as propagation of uncertainty and least squares parameter fitting) can be derived analytically in explicit form when the relevant variables are normally distributed.
The normal distribution is sometimes informally called the bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions). Even the term Gaussian bell curve is ambiguous because it may be used to refer to some function defined in terms of the Gaussian function which is not a probability distribution because it is not normalized in that it does not integrate to 1.
The probability density of the normal distribution is:





f
(
x


|


μ
,

σ

2


)
=


1

2
π

σ

2







e

−



(
x
−
μ

)

2




2

σ

2









{\displaystyle f(x\;|\;\mu ,\sigma ^{2})={\frac {1}{\sqrt {2\pi \sigma ^{2}}}}\;e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}}



Where:





μ


{\displaystyle \mu }

 is mean or expectation of the distribution (and also its median and mode).




σ


{\displaystyle \sigma }

 is standard deviation





σ

2




{\displaystyle \sigma ^{2}}

 is variance

A random variable with a Gaussian distribution is said to be normally distributed and is called a normal deviate.



Contents


1 Definition

1.1 Standard normal distribution
1.2 General normal distribution
1.3 Notation
1.4 Alternative parameterizations


2 Properties

2.1 Symmetries and derivatives

2.1.1 Differential equation


2.2 Moments
2.3 Fourier transform and characteristic function
2.4 Moment and cumulant generating functions


3 Cumulative distribution function

3.1 Standard deviation and coverage
3.2 Quantile function


4 Zero-variance limit
5 Central limit theorem
6 Maximum entropy
7 Operations on normal deviates

7.1 Infinite divisibility and Cramér's theorem
7.2 Bernstein's theorem


8 Other properties
9 Related distributions

9.1 Operations on a single random variable
9.2 Combination of two independent random variables
9.3 Combination of two or more independent random variables
9.4 Operations on the density function
9.5 Extensions


10 Normality tests
11 Estimation of parameters

11.1 Sample mean
11.2 Sample variance
11.3 Confidence intervals


12 Bayesian analysis of the normal distribution

12.1 Sum of two quadratics

12.1.1 Scalar form
12.1.2 Vector form


12.2 Sum of differences from the mean
12.3 With known variance
12.4 With known mean
12.5 With unknown mean and unknown variance


13 Occurrence and applications

13.1 Exact normality
13.2 Approximate normality
13.3 Assumed normality
13.4 Produced normality


14 Generating values from normal distribution
15 Numerical approximations for the normal CDF
16 History

16.1 Development
16.2 Naming


17 See also
18 Notes
19 Citations
20 References
21 External links



Definition[edit]
Standard normal distribution[edit]
The simplest case of a normal distribution is known as the standard normal distribution. This is a special case when μ = 0 and σ = 1, and it is described by this probability density function:





φ
(
x
)
=



e

−



1


2




x

2





2
π






{\displaystyle \varphi (x)={\frac {e^{-{\frac {\scriptscriptstyle 1}{\scriptscriptstyle 2}}x^{2}}}{\sqrt {2\pi }}}\,}



The factor 



1

/



2
π




{\displaystyle 1/{\sqrt {2\pi }}}

 in this expression ensures that the total area under the curve 



φ
(
x
)


{\displaystyle \varphi (x)}

 is equal to one.[4] The factor 



1

/

2


{\displaystyle 1/2}

 in the exponent ensures that the distribution has unit variance (and therefore also unit standard deviation). This function is symmetric around 



x
=
0


{\displaystyle x=0}

, where it attains its maximum value 



1

/



2
π




{\displaystyle 1/{\sqrt {2\pi }}}

 and has inflection points at 



x
=
+
1


{\displaystyle x=+1}

 and 



x
=
−
1


{\displaystyle x=-1}

.
Authors may differ also on which normal distribution should be called the "standard" one. Gauss defined the standard normal as having variance 




σ

2


=


1
2




{\displaystyle \sigma ^{2}={\frac {1}{2}}}

, that is





φ
(
x
)
=



e

−

x

2





π






{\displaystyle \varphi (x)={\frac {e^{-x^{2}}}{\sqrt {\pi }}}\,}



Stigler[5] goes even further, defining the standard normal with variance 




σ

2


=


1

2
π





{\displaystyle \sigma ^{2}={\frac {1}{2\pi }}}

 :





φ
(
x
)
=

e

−
π

x

2






{\displaystyle \varphi (x)=e^{-\pi x^{2}}}



General normal distribution[edit]
Every normal distribution is a version of the standard normal distribution whose domain has been stretched by a factor σ (the standard deviation) and then translated by μ (the mean value):





f
(
x
∣
μ
,

σ

2


)
=


1
σ


φ

(



x
−
μ

σ


)

.


{\displaystyle f(x\mid \mu ,\sigma ^{2})={\frac {1}{\sigma }}\varphi \left({\frac {x-\mu }{\sigma }}\right).}



The probability density must be scaled by 



1

/

σ


{\displaystyle 1/\sigma }

 so that the integral is still 1.
If Z is a standard normal deviate, then X = Zσ + μ will have a normal distribution with expected value μ and standard deviation σ. Conversely, if X is a general normal deviate, then Z = (X − μ)/σ will have a standard normal distribution.
Every normal distribution is the exponential of a quadratic function:





f
(
x
)
=

e

a

x

2


+
b
x
+
c




{\displaystyle f(x)=e^{ax^{2}+bx+c}}



where a is negative and c is 




b

2



/

(
4
a
)
+
ln
⁡
(
−
a

/

π
)

/

2


{\displaystyle b^{2}/(4a)+\ln(-a/\pi )/2}

. In this form, the mean value μ is −b/(2a), and the variance σ2 is −1/(2a). For the standard normal distribution, a is −1/2, b is zero, and c is 



−
ln
⁡
(
2
π
)

/

2


{\displaystyle -\ln(2\pi )/2}

.
Notation[edit]
The standard Gaussian distribution (with zero mean and unit variance) is often denoted with the Greek letter ϕ (phi).[6] The alternative form of the Greek phi letter, φ, is also used quite often.
The normal distribution is also often denoted by N(μ, σ2).[7] Thus when a random variable X is distributed normally with mean μ and variance σ2, we write





X
 
∼
 


N


(
μ
,


σ

2


)
.


{\displaystyle X\ \sim \ {\mathcal {N}}(\mu ,\,\sigma ^{2}).}



Alternative parameterizations[edit]
Some authors advocate using the precision 



τ


{\displaystyle \tau }

 as the parameter defining the width of the distribution, instead of the deviation σ or the variance σ2. The precision is normally defined as the reciprocal of the variance, 1/σ2.[8] The formula for the distribution then becomes





f
(
x
)
=



τ

2
π






e

−
τ
(
x
−
μ

)

2



/

2


.


{\displaystyle f(x)={\sqrt {\frac {\tau }{2\pi }}}\,e^{-\tau (x-\mu )^{2}/2}.}



This choice is claimed to have advantages in numerical computations when σ is very close to zero and simplify formulas in some contexts, such as in the Bayesian inference of variables with multivariate normal distribution.
Also the reciprocal of the standard deviation 




τ

′


=
1

/

σ


{\displaystyle \tau ^{\prime }=1/\sigma }

 might be defined as the precision and the expression of the normal distribution becomes





f
(
x
)
=



τ

′



2
π





e

−
(

τ

′



)

2


(
x
−
μ

)

2



/

2


.


{\displaystyle f(x)={\frac {\tau ^{\prime }}{\sqrt {2\pi }}}\,e^{-(\tau ^{\prime })^{2}(x-\mu )^{2}/2}.}



According to Stigler, this formulation is advantageous because of a much simpler and easier-to-remember formula, the fact that the pdf has unit height at zero, and simple approximate formulas for the quantiles of the distribution.
Properties[edit]
The normal distribution is the only absolutely continuous distribution whose cumulants beyond the first two (i.e., other than the mean and variance) are zero. It is also the continuous distribution with the maximum entropy for a specified mean and variance.[9][10]
The normal distribution is a subclass of the elliptical distributions. The normal distribution is symmetric about its mean, and is non-zero over the entire real line. As such it may not be a suitable model for variables that are inherently positive or strongly skewed, such as the weight of a person or the price of a share. Such variables may be better described by other distributions, such as the log-normal distribution or the Pareto distribution.
The value of the normal distribution is practically zero when the value x lies more than a few standard deviations away from the mean. Therefore, it may not be an appropriate model when one expects a significant fraction of outliers—values that lie many standard deviations away from the mean—and least squares and other statistical inference methods that are optimal for normally distributed variables often become highly unreliable when applied to such data. In those cases, a more heavy-tailed distribution should be assumed and the appropriate robust statistical inference methods applied.
The Gaussian distribution belongs to the family of stable distributions which are the attractors of sums of independent, identically distributed distributions whether or not the mean or variance is finite. Except for the Gaussian which is a limiting case, all stable distributions have heavy tails and infinite variance. It is one of the few distributions that are stable and that have probability density functions that can be expressed analytically, the others being the Cauchy distribution and the Lévy distribution.
Symmetries and derivatives[edit]
The normal distribution f(x), with any mean μ and any positive deviation σ, has the following properties:

It is symmetric around the point x = μ, which is at the same time the mode, the median and the mean of the distribution and it divides the data in half.[11]
It is unimodal: its first derivative is positive for x < μ, negative for x > μ, and zero only at x = μ.
The area under the curve and over the x-axis is unity.
Its density has two inflection points (where the second derivative of f is zero and changes sign), located one standard deviation away from the mean, namely at x = μ − σ and x = μ + σ.[11]
Its density is log-concave.[11]
Its density is infinitely differentiable, indeed supersmooth of order 2.[12]
Its second derivative f′′(x) is equal to its derivative with respect to its variance σ2.

Furthermore, the density ϕ of the standard normal distribution (with μ = 0 and σ = 1) also has the following properties:

Its first derivative ϕ′(x) is −xϕ(x).
Its second derivative ϕ′′(x) is (x2 − 1)ϕ(x)
More generally, its n-th derivative ϕ(n)(x) is (−1)nHen(x)ϕ(x), where Hen is the 




n

t
h




{\displaystyle n^{th}}

 (probabilist) Hermite polynomial.[13]
If we want to calculate the probability of a normally distributed variable X with known 



μ


{\displaystyle \mu }

 and 



σ


{\displaystyle \sigma }

. Then we can calculate it by Standard Normal Distribution equivalent to 



Y
=



X
−
μ

σ




{\displaystyle Y={\frac {X-\mu }{\sigma }}}

 using probability table.

Differential equation[edit]

It satisfies the differential equation









σ

2



f
′

(
x
)
+
f
(
x
)
(
x
−
μ
)
=
0
,

f
(
0
)
=



e

−

μ

2



/

(
2

σ

2


)



2

σ

2


π





{\displaystyle \sigma ^{2}f'(x)+f(x)(x-\mu )=0,\qquad f(0)={\frac {e^{-\mu ^{2}/(2\sigma ^{2})}}{\sqrt {2\sigma ^{2}\pi }}}}






or









f
′

(
x
)
+
τ
f
(
x
)
(
x
−
μ
)
=
0
,

f
(
0
)
=





τ



e

−

μ

2


τ

/

2




2
π



.


{\displaystyle f'(x)+\tau f(x)(x-\mu )=0,\qquad f(0)={\frac {{\sqrt {\tau }}e^{-\mu ^{2}\tau /2}}{\sqrt {2\pi }}}.}





Moments[edit]
See also: List of integrals of Gaussian functions
The plain and absolute moments of a variable X are the expected values of Xp and |X|p,respectively. If the expected value μ of X is zero, these parameters are called central moments. Usually we are interested only in moments with integer order p.
If X has a normal distribution, these moments exist and are finite for any p whose real part is greater than −1. For any non-negative integer p, the plain central moments are [14]:





E
⁡

[

X

p


]

=


{



0



if 

p

 is odd,






σ

p



(
p
−
1
)
!
!



if 

p

 is even.









{\displaystyle \operatorname {E} \left[X^{p}\right]={\begin{cases}0&{\text{if }}p{\text{ is odd,}}\\\sigma ^{p}\,(p-1)!!&{\text{if }}p{\text{ is even.}}\end{cases}}}



Here n!! denotes the double factorial, that is, the product of all numbers from n to 1 that have the same parity as n.
The central absolute moments coincide with plain moments for all even orders, but are nonzero for odd orders. For any non-negative integer p,





E
⁡

[

|

X


|


p


]

=

σ

p



(
p
−
1
)
!
!
⋅




{






2
π






if 

p

 is odd





1



if 

p

 is even







}

=

σ

p


⋅




2

p

/

2


Γ

(



p
+
1

2


)



π





{\displaystyle \operatorname {E} \left[|X|^{p}\right]=\sigma ^{p}\,(p-1)!!\cdot \left.{\begin{cases}{\sqrt {\frac {2}{\pi }}}&{\text{if }}p{\text{ is odd}}\\1&{\text{if }}p{\text{ is even}}\end{cases}}\right\}=\sigma ^{p}\cdot {\frac {2^{p/2}\Gamma \left({\frac {p+1}{2}}\right)}{\sqrt {\pi }}}}



The last formula is valid also for any non-integer p > −1. When the mean μ is not zero, the plain and absolute moments can be expressed in terms of confluent hypergeometric functions 1F1 and U.[citation needed]





E
⁡

[

X

p


]

=

σ

p


⋅
(
−
i


2



)

p



U

(
−


1
2


p
,



1
2


,

−


1
2




(


μ
σ


)


2


)

,


{\displaystyle \operatorname {E} \left[X^{p}\right]=\sigma ^{p}\cdot (-i{\sqrt {2}})^{p}\;U\left(-{\frac {1}{2}}p,\,{\frac {1}{2}},\,-{\frac {1}{2}}\left({\frac {\mu }{\sigma }}\right)^{2}\right),}






E
⁡

[

|

X


|


p


]

=

σ

p


⋅

2

p

/

2





Γ

(



1
+
p

2


)



π






1



F

1



(
−


1
2


p
,



1
2


,

−


1
2




(


μ
σ


)


2


)

.


{\displaystyle \operatorname {E} \left[|X|^{p}\right]=\sigma ^{p}\cdot 2^{p/2}{\frac {\Gamma \left({\frac {1+p}{2}}\right)}{\sqrt {\pi }}}\;_{1}F_{1}\left(-{\frac {1}{2}}p,\,{\frac {1}{2}},\,-{\frac {1}{2}}\left({\frac {\mu }{\sigma }}\right)^{2}\right).}



These expressions remain valid even if p is not integer. See also generalized Hermite polynomials.



Order
Non-central moment
Central moment


1
μ
0


2
μ2 + σ2
σ 2


3
μ3 + 3μσ2
0


4
μ4 + 6μ2σ2 + 3σ4
3σ 4


5
μ5 + 10μ3σ2 + 15μσ4
0


6
μ6 + 15μ4σ2 + 45μ2σ4 + 15σ6
15σ 6


7
μ7 + 21μ5σ2 + 105μ3σ4 + 105μσ6
0


8
μ8 + 28μ6σ2 + 210μ4σ4 + 420μ2σ6 + 105σ8
105σ 8



The expectation of X conditioned on the event that X lies in an interval [a,b] is given by






E


[
X
∣
a
<
X
<
b
]

=
μ
−

σ

2





f
(
b
)
−
f
(
a
)


F
(
b
)
−
F
(
a
)





{\displaystyle \mathrm {E} \left[X\mid a<X<b\right]=\mu -\sigma ^{2}{\frac {f(b)-f(a)}{F(b)-F(a)}}}



where f(x) and F(x) respectively are the density and the cumulative distribution function of X. For b = ∞ this is known as the inverse Mills ratio. Note that above, density 



f


{\textstyle f}

 of X is used instead of standard normal density as in inverse Mills ratio, so here we have 




σ

2




{\textstyle \sigma ^{2}}

 instead of 



σ


{\textstyle \sigma }

.
Fourier transform and characteristic function[edit]
The Fourier transform of a normal distribution f with mean μ and deviation σ is[15]








φ
^



(
t
)
=

∫

−
∞


∞



f
(
x
)

e

−
i
t
x



d
x
=

e

i
μ
t



e

−


1
2


(
σ
t

)

2






{\displaystyle {\hat {\varphi }}(t)=\int _{-\infty }^{\infty }\!f(x)e^{-itx}\,dx=e^{i\mu t}e^{-{\frac {1}{2}}(\sigma t)^{2}}}



where i is the imaginary unit. If the mean μ is zero, the first factor is 1, and the Fourier transform is also a normal distribution on the frequency domain, with mean 0 and standard deviation 1/σ. In particular, the standard normal distribution ϕ (with μ = 0 and σ = 1) is an eigenfunction of the Fourier transform.
In probability theory, the Fourier transform of the probability distribution of a real-valued random variable X is called the characteristic function of that variable, and can be defined as the expected value of ei tX, as a function of the real variable t (the frequency parameter of the Fourier transform). This definition can be analytically extended to a complex-value parameter t.[16]
Moment and cumulant generating functions[edit]
The moment generating function of a real random variable X is the expected value of etX, as a function of the real parameter t. For a normal distribution with mean μ and deviation σ, the moment generating function exists and is equal to





M
(
t
)
=



φ
^



(
−
i
t
)
=

e

μ
t



e



1
2



σ

2



t

2






{\displaystyle M(t)={\hat {\varphi }}(-it)=e^{\mu t}e^{{\frac {1}{2}}\sigma ^{2}t^{2}}}



The cumulant generating function is the logarithm of the moment generating function, namely





g
(
t
)
=
ln
⁡
M
(
t
)
=
μ
t
+


1
2



σ

2



t

2




{\displaystyle g(t)=\ln M(t)=\mu t+{\frac {1}{2}}\sigma ^{2}t^{2}}



Since this is a quadratic polynomial in t, only the first two cumulants are nonzero, namely the mean μ and the variance σ2.
Cumulative distribution function[edit]
The cumulative distribution function (CDF) of the standard normal distribution, usually denoted with the capital Greek letter 



Φ


{\displaystyle \Phi }

 (phi), is the integral





Φ
(
x
)
=


1

2
π




∫

−
∞


x



e

−

t

2



/

2



d
t


{\displaystyle \Phi (x)={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{x}e^{-t^{2}/2}\,dt}



In statistics one often uses the related error function, or erf(x), defined as the probability of a random variable with normal distribution of mean 0 and variance 1/2 falling in the range 



[
−
x
,
x
]


{\displaystyle [-x,x]}

; that is





erf
⁡
(
x
)

=



1

π




∫

−
x


x



e

−

t

2





d
t


{\displaystyle \operatorname {erf} (x)\;=\;{\frac {1}{\sqrt {\pi }}}\int _{-x}^{x}e^{-t^{2}}\,dt}



These integrals cannot be expressed in terms of elementary functions, and are often said to be special functions. However, many numerical approximations are known; see below.
The two functions are closely related, namely





Φ
(
x
)
=


1
2



[
1
+
erf
⁡

(


x

2



)

]



{\displaystyle \Phi (x)={\frac {1}{2}}\left[1+\operatorname {erf} \left({\frac {x}{\sqrt {2}}}\right)\right]}



For a generic normal distribution f with mean μ and deviation σ, the cumulative distribution function is





F
(
x
)
=
Φ

(



x
−
μ

σ


)

=


1
2



[
1
+
erf
⁡

(



x
−
μ


σ


2





)

]



{\displaystyle F(x)=\Phi \left({\frac {x-\mu }{\sigma }}\right)={\frac {1}{2}}\left[1+\operatorname {erf} \left({\frac {x-\mu }{\sigma {\sqrt {2}}}}\right)\right]}



The complement of the standard normal CDF, 



Q
(
x
)
=
1
−
Φ
(
x
)


{\displaystyle Q(x)=1-\Phi (x)}

, is often called the Q-function, especially in engineering texts.[17][18] It gives the probability that the value of a standard normal random variable X will exceed x. Other definitions of the Q-function, all of which are simple transformations of 



Φ


{\displaystyle \Phi }

, are also used occasionally.[19]
The graph of the standard normal CDF 



Φ


{\displaystyle \Phi }

 has 2-fold rotational symmetry around the point (0,1/2); that is, 



Φ
(
−
x
)
=
1
−
Φ
(
x
)


{\displaystyle \Phi (-x)=1-\Phi (x)}

. Its antiderivative (indefinite integral) 



∫
Φ
(
x
)

d
x


{\displaystyle \int \Phi (x)\,dx}

 is 



∫
Φ
(
x
)

d
x
=
x
Φ
(
x
)
+
φ
(
x
)


{\displaystyle \int \Phi (x)\,dx=x\Phi (x)+\varphi (x)}

.

The cumulative distribution function (CDF) of the standard normal distribution can be expanded by Integration by parts into a series:








Φ
(
x
)

=

0.5
+


1

2
π



⋅

e

−

x

2



/

2



[
x
+



x

3


3


+



x

5



3
⋅
5



+
⋯
+



x

2
n
+
1



(
2
n
+
1
)
!
!



+
⋯
]



{\displaystyle \Phi (x)\;=\;0.5+{\frac {1}{\sqrt {2\pi }}}\cdot e^{-x^{2}/2}\left[x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{3\cdot 5}}+\cdots +{\frac {x^{2n+1}}{(2n+1)!!}}+\cdots \right]}





where 



!
!


{\displaystyle !!}

 denotes the double factorial. As an example, the following Pascal function approximates the CDF:


function CDF(x:extended):extended;
var value,sum:extended;
    i:integer;
begin
  sum:=x;
  value:=x;
  for i:=1 to 100 do
    begin
      value:=(value*x*x/(2*i+1));
      sum:=sum+value;
    end;
  result:=0.5+(sum/sqrt(2*pi))*exp(-(x*x)/2);
end;

Standard deviation and coverage[edit]
Further information: Interval estimation and Coverage probability




For the normal distribution, the values less than one standard deviation away from the mean account for 68.27% of the set; while two standard deviations from the mean account for 95.45%; and three standard deviations account for 99.73%.


About 68% of values drawn from a normal distribution are within one standard deviation σ away from the mean; about 95% of the values lie within two standard deviations; and about 99.7% are within three standard deviations. This fact is known as the 68-95-99.7 (empirical) rule, or the 3-sigma rule.
More precisely, the probability that a normal deviate lies in the range μ − nσ and μ + nσ is given by





F
(
μ
+
n
σ
)
−
F
(
μ
−
n
σ
)
=
Φ
(
n
)
−
Φ
(
−
n
)
=
erf
⁡

(


n

2



)

,


{\displaystyle F(\mu +n\sigma )-F(\mu -n\sigma )=\Phi (n)-\Phi (-n)=\operatorname {erf} \left({\frac {n}{\sqrt {2}}}\right),}



To 12 significant figures, the values for n = 1, 2, …, 6 are:[20]


n
F(μ+nσ) − F(μ − nσ)
i.e. 1 minus …
or 1 in …
OEIS


1
6999682689492137000♠0.682689492137
6999317310507863000♠0.317310507863



7000300000000000000♠3
.15148718753



 A178647


2
6999954499736104000♠0.954499736104
6998455002638960000♠0.045500263896



7001210000000000000♠21
.9778945080



 A110894


3
6999997300203937000♠0.997300203937
6997269979606300000♠0.002699796063



7002370000000000000♠370
.398347345



 A270712


4
6999999936657516000♠0.999936657516
6995633424840000000♠0.000063342484



7004157870000000000♠15787
.1927673





5
6999999999426697000♠0.999999426697
6993573303000000000♠0.000000573303



7006174427700000000♠1744277
.89362





6
6999999999998027000♠0.999999998027
6991197300000000000♠0.000000001973



7008506797345000000♠506797345
.897





Quantile function[edit]
Further information: Quantile function § Normal distribution
The quantile function of a distribution is the inverse of the cumulative distribution function. The quantile function of the standard normal distribution is called the probit function, and can be expressed in terms of the inverse error function:






Φ

−
1


(
p
)

=



2




erf

−
1


⁡
(
2
p
−
1
)
,

p
∈
(
0
,
1
)
.


{\displaystyle \Phi ^{-1}(p)\;=\;{\sqrt {2}}\;\operatorname {erf} ^{-1}(2p-1),\quad p\in (0,1).}



For a normal random variable with mean μ and variance σ2, the quantile function is






F

−
1


(
p
)
=
μ
+
σ

Φ

−
1


(
p
)
=
μ
+
σ


2




erf

−
1


⁡
(
2
p
−
1
)
,

p
∈
(
0
,
1
)
.


{\displaystyle F^{-1}(p)=\mu +\sigma \Phi ^{-1}(p)=\mu +\sigma {\sqrt {2}}\,\operatorname {erf} ^{-1}(2p-1),\quad p\in (0,1).}



The quantile 




Φ

−
1


(
p
)


{\displaystyle \Phi ^{-1}(p)}

 of the standard normal distribution is commonly denoted as zp. These values are used in hypothesis testing, construction of confidence intervals and Q-Q plots. A normal random variable X will exceed μ + σzp with probability 1 − p; and will lie outside the interval μ ± σzp with probability 2(1 − p). In particular, the quantile z0.975 is 1.96; therefore a normal random variable will lie outside the interval μ ± 1.96σ in only 5% of cases.
The following table gives the multiple n of σ such that X will lie in the range μ ± nσ with a specified probability p. These values are useful to determine tolerance interval for sample averages and other statistical estimators with normal (or asymptotically normal) distributions:[21][22]


F(μ + nσ) − F(μ − nσ)
n
 
F(μ + nσ) − F(μ − nσ)
n


0.80
7000128155156554500♠1.281551565545
0.999
7000329052673149200♠3.290526731492


0.90
7000164485362695100♠1.644853626951
0.9999
7000389059188641300♠3.890591886413


0.95
7000195996398454000♠1.959963984540
0.99999
7000441717341346900♠4.417173413469


0.98
7000232634787404100♠2.326347874041
0.999999
7000489163847569899♠4.891638475699


0.99
7000257582930354900♠2.575829303549
0.9999999
7000532672388638400♠5.326723886384


0.995
7000280703376834400♠2.807033768344
0.99999999
7000573072886823600♠5.730728868236


0.998
7000309023230616800♠3.090232306168
0.999999999
7000610941020486900♠6.109410204869


Zero-variance limit[edit]
In the limit when σ tends to zero, the probability density f(x) eventually tends to zero at any x ≠ μ, but grows without limit if x = μ, while its integral remains equal to 1. Therefore, the normal distribution cannot be defined as an ordinary function when σ = 0.
However, one can define the normal distribution with zero variance as a generalized function; specifically, as Dirac's "delta function" δ translated by the mean μ, that is f(x) = δ(x−μ). Its CDF is then the Heaviside step function translated by the mean μ, namely





F
(
x
)
=


{



0



if 

x
<
μ




1



if 

x
≥
μ








{\displaystyle F(x)={\begin{cases}0&{\text{if }}x<\mu \\1&{\text{if }}x\geq \mu \end{cases}}}



Central limit theorem[edit]




As the number of discrete events increases, the function begins to resemble a normal distribution






Comparison of probability density functions, p(k) for the sum of n fair 6-sided dice to show their convergence to a normal distribution with increasing n, in accordance to the central limit theorem. In the bottom-right graph, smoothed profiles of the previous graphs are rescaled, superimposed and compared with a normal distribution (black curve).


Main article: Central limit theorem
The central limit theorem states that under certain (fairly common) conditions, the sum of many random variables will have an approximately normal distribution. More specifically, where X1, …, Xn are independent and identically distributed random variables with the same arbitrary distribution, zero mean, and variance σ2; and Z is their mean scaled by 





n




{\displaystyle {\sqrt {n}}}







Z
=


n



(


1
n



∑

i
=
1


n



X

i


)



{\displaystyle Z={\sqrt {n}}\left({\frac {1}{n}}\sum _{i=1}^{n}X_{i}\right)}



Then, as n increases, the probability distribution of Z will tend to the normal distribution with zero mean and variance σ2.
The theorem can be extended to variables Xi that are not independent and/or not identically distributed if certain constraints are placed on the degree of dependence and the moments of the distributions.
Many test statistics, scores, and estimators encountered in practice contain sums of certain random variables in them, and even more estimators can be represented as sums of random variables through the use of influence functions. The central limit theorem implies that those statistical parameters will have asymptotically normal distributions.
The central limit theorem also implies that certain distributions can be approximated by the normal distribution, for example:

The binomial distribution B(n, p) is approximately normal with mean np and variance np(1 − p) for large n and for p not too close to zero or one.
The Poisson distribution with parameter λ is approximately normal with mean λ and variance λ, for large values of λ.[23]
The chi-squared distribution χ2(k) is approximately normal with mean k and variance 2k, for large k.
The Student's t-distribution t(ν) is approximately normal with mean 0 and variance 1 when ν is large.

Whether these approximations are sufficiently accurate depends on the purpose for which they are needed, and the rate of convergence to the normal distribution. It is typically the case that such approximations are less accurate in the tails of the distribution.
A general upper bound for the approximation error in the central limit theorem is given by the Berry–Esseen theorem, improvements of the approximation are given by the Edgeworth expansions.
Maximum entropy[edit]
Of all probability distributions over the reals with a specified mean μ and variance σ2, the normal distribution N(μ, σ2) is the one with maximum entropy.[24] If X is a continuous random variable with probability density f(x), then the entropy of X is defined as[25][26][27]





H
(
X
)
=
−

∫

−
∞


∞


f
(
x
)
log
⁡
f
(
x
)
d
x
=



1
2



(
1
+
log
⁡
(
2

σ

2


π
)
)


{\displaystyle H(X)=-\int _{-\infty }^{\infty }f(x)\log f(x)dx={\tfrac {1}{2}}(1+\log(2\sigma ^{2}\pi ))}



where f(x) log f(x) is understood to be zero whenever f(x) = 0. This functional can be maximized, subject to the constraints that the distribution is properly normalized and has a specified variance, by using variational calculus. A function with two Lagrange multipliers is defined:





L
=

∫

−
∞


∞


f
(
x
)
ln
⁡
(
f
(
x
)
)

d
x
−

λ

0



(
1
−

∫

−
∞


∞


f
(
x
)

d
x
)

−
λ

(

σ

2


−

∫

−
∞


∞


f
(
x
)
(
x
−
μ

)

2



d
x
)



{\displaystyle L=\int _{-\infty }^{\infty }f(x)\ln(f(x))\,dx-\lambda _{0}\left(1-\int _{-\infty }^{\infty }f(x)\,dx\right)-\lambda \left(\sigma ^{2}-\int _{-\infty }^{\infty }f(x)(x-\mu )^{2}\,dx\right)}



where f(x) is, for now, regarded as some function with mean μ and standard deviation 



σ


{\displaystyle \sigma }

. At maximum entropy, a small variation δf(x) about f(x) will produce a variation δL about L which is equal to zero:





0
=
δ
L
=

∫

−
∞


∞


δ
f
(
x
)

(
ln
⁡
(
f
(
x
)
)
+
1
+

λ

0


+
λ
(
x
−
μ

)

2


)


d
x


{\displaystyle 0=\delta L=\int _{-\infty }^{\infty }\delta f(x)\left(\ln(f(x))+1+\lambda _{0}+\lambda (x-\mu )^{2}\right)\,dx}



Since this must hold for any small δf(x), the term in brackets must be zero, and solving for f(x) yields:





f
(
x
)
=

e

−

λ

0


−
1
−
λ
(
x
−
μ

)

2






{\displaystyle f(x)=e^{-\lambda _{0}-1-\lambda (x-\mu )^{2}}}



Using the constraint equations to solve for λ0 and λ yields the normal distribution:





f
(
x
,
μ
,
σ
)
=


1

2

σ

2


π




e

−



(
x
−
μ

)

2




2

σ

2









{\displaystyle f(x,\mu ,\sigma )={\frac {1}{\sqrt {2\sigma ^{2}\pi }}}e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}}



Operations on normal deviates[edit]
The family of normal distributions is closed under linear transformations: if X is normally distributed with mean μ and standard deviation σ, then the variable Y = aX + b, for any real numbers a and b, is also normally distributed, with mean aμ + b and standard deviation |a|σ.
Also if X1 and X2 are two independent normal random variables, with means μ1, μ2 and standard deviations σ1, σ2, then their sum X1 + X2 will also be normally distributed,[proof] with mean μ1 + μ2 and variance 




σ

1


2


+

σ

2


2




{\displaystyle \sigma _{1}^{2}+\sigma _{2}^{2}}

.
In particular, if X and Y are independent normal deviates with zero mean and variance σ2, then X + Y and X − Y are also independent and normally distributed, with zero mean and variance 2σ2. This is a special case of the polarization identity.[28]
Also, if X1, X2 are two independent normal deviates with mean μ and deviation σ, and a, b are arbitrary real numbers, then the variable






X

3


=



a

X

1


+
b

X

2


−
(
a
+
b
)
μ



a

2


+

b

2





+
μ


{\displaystyle X_{3}={\frac {aX_{1}+bX_{2}-(a+b)\mu }{\sqrt {a^{2}+b^{2}}}}+\mu }



is also normally distributed with mean μ and deviation σ. It follows that the normal distribution is stable (with exponent α = 2).
More generally, any linear combination of independent normal deviates is a normal deviate.
Infinite divisibility and Cramér's theorem[edit]
For any positive integer n, any normal distribution with mean μ and variance σ2 is the distribution of the sum of n independent normal deviates, each with mean μ/n and variance σ2/n. This property is called infinite divisibility.[29]
Conversely, if X1 and X2 are independent random variables and their sum X1 + X2 has a normal distribution, then both X1 and X2 must be normal deviates.[30]
This result is known as Cramér's decomposition theorem, and is equivalent to saying that the convolution of two distributions is normal if and only if both are normal. Cramér's theorem implies that a linear combination of independent non-Gaussian variables will never have an exactly normal distribution, although it may approach it arbitrarily closely.[31]
Bernstein's theorem[edit]
Bernstein's theorem states that if X and Y are independent and X + Y and X − Y are also independent, then both X and Y must necessarily have normal distributions.[32][33]
More generally, if X1, …, Xn are independent random variables, then two distinct linear combinations ∑akXk and ∑bkXk will be independent if and only if all Xk's are normal and ∑akbkσ 2
k  = 0, where σ 2
k  denotes the variance of Xk.[32]
Other properties[edit]


If the characteristic function φX of some random variable X is of the form φX(t) = eQ(t), where Q(t) is a polynomial, then the Marcinkiewicz theorem (named after Józef Marcinkiewicz) asserts that Q can be at most a quadratic polynomial, and therefore X a normal random variable.[31] The consequence of this result is that the normal distribution is the only distribution with a finite number (two) of non-zero cumulants.
If X and Y are jointly normal and uncorrelated, then they are independent. The requirement that X and Y should be jointly normal is essential, without it the property does not hold.[34][35][proof] For non-normal random variables uncorrelatedness does not imply independence.
The Kullback–Leibler divergence of one normal distribution X1 ∼ N(μ1, σ21 )from another X2 ∼ N(μ2, σ22 )is given by:[36]






D


K
L



(

X

1



∥


X

2


)
=



(

μ

1


−

μ

2



)

2




2

σ

2


2






+



1
2



(




σ

1


2



σ

2


2




−
1
−
ln
⁡



σ

1


2



σ

2


2





)

 
.


{\displaystyle D_{\mathrm {KL} }(X_{1}\,\|\,X_{2})={\frac {(\mu _{1}-\mu _{2})^{2}}{2\sigma _{2}^{2}}}\,+\,{\frac {1}{2}}\left(\,{\frac {\sigma _{1}^{2}}{\sigma _{2}^{2}}}-1-\ln {\frac {\sigma _{1}^{2}}{\sigma _{2}^{2}}}\,\right)\ .}



The Hellinger distance between the same distributions is equal to






H

2


(

X

1


,

X

2


)
=
1

−





2

σ

1



σ

2





σ

1


2


+

σ

2


2








e

−


1
4





(

μ

1


−

μ

2



)

2





σ

1


2


+

σ

2


2







 
.


{\displaystyle H^{2}(X_{1},X_{2})=1\,-\,{\sqrt {\frac {2\sigma _{1}\sigma _{2}}{\sigma _{1}^{2}+\sigma _{2}^{2}}}}\;e^{-{\frac {1}{4}}{\frac {(\mu _{1}-\mu _{2})^{2}}{\sigma _{1}^{2}+\sigma _{2}^{2}}}}\ .}




The Fisher information matrix for a normal distribution is diagonal and takes the form







I


=


(





1

σ

2






0




0




1

2

σ

4








)




{\displaystyle {\mathcal {I}}={\begin{pmatrix}{\frac {1}{\sigma ^{2}}}&0\\0&{\frac {1}{2\sigma ^{4}}}\end{pmatrix}}}




Normal distributions belongs to an exponential family with natural parameters 





θ

1


=


μ

σ

2







{\displaystyle \scriptstyle \theta _{1}={\frac {\mu }{\sigma ^{2}}}}

 and 





θ

2


=



−
1


2

σ

2








{\displaystyle \scriptstyle \theta _{2}={\frac {-1}{2\sigma ^{2}}}}

, and natural statistics x and x2. The dual, expectation parameters for normal distribution are η1 = μ and η2 = μ2 + σ2.
The conjugate prior of the mean of a normal distribution is another normal distribution.[37] Specifically, if x1, …, xn are iid N(μ, σ2) and the prior is μ ~ N(μ0, σ2
0), then the posterior distribution for the estimator of μ will be





μ
∣

x

1


,
…
,

x

n


 
∼
 


N



(






σ

2


n



μ

0


+

σ

0


2





x
¯








σ

2


n


+

σ

0


2





,
 


(


n

σ

2




+


1

σ

0


2




)



−
1


)



{\displaystyle \mu \mid x_{1},\ldots ,x_{n}\ \sim \ {\mathcal {N}}\left({\frac {{\frac {\sigma ^{2}}{n}}\mu _{0}+\sigma _{0}^{2}{\bar {x}}}{{\frac {\sigma ^{2}}{n}}+\sigma _{0}^{2}}},\ \left({\frac {n}{\sigma ^{2}}}+{\frac {1}{\sigma _{0}^{2}}}\right)^{\!-1}\right)}




The family of normal distributions forms a manifold with constant curvature −1. The same family is flat with respect to the (±1)-connections ∇(e) and ∇(m).[38]


Related distributions[edit]
Operations on a single random variable[edit]
If X is distributed normally with mean μ and variance σ2, then

The exponential of X is distributed log-normally: eX ~ ln(N (μ, σ2)).
The absolute value of X has folded normal distribution: |X| ~ Nf (μ, σ2). If μ = 0 this is known as the half-normal distribution.
The absolute value of normalized residuals, |X - μ|/σ, has chi distribution with one degree of freedom: |X - μ|/σ ~ χ1(|X - μ|/σ).
The square of X/σ has the noncentral chi-squared distribution with one degree of freedom: X2/σ2 ~ χ21(μ2/σ2). If μ = 0, the distribution is called simply chi-squared.
The distribution of the variable X restricted to an interval [a, b] is called the truncated normal distribution.
(X − μ)−2 has a Lévy distribution with location 0 and scale σ−2.

Combination of two independent random variables[edit]
If X1 and X2 are two independent standard normal random variables with mean 0 and variance 1, then

Their sum and difference is distributed normally with mean zero and variance two: X1 ± X2 ∼ N(0, 2).
Their product Z = X1·X2 follows the "product-normal" distribution[39] with density function fZ(z) = π−1K0(|z|), where K0 is the modified Bessel function of the second kind. This distribution is symmetric around zero, unbounded at z = 0, and has the characteristic function φZ(t) = (1 + t 2)−1/2.
Their ratio follows the standard Cauchy distribution: X1 ÷ X2 ∼ Cauchy(0, 1).
Their Euclidean norm 







X

1


2



+


X

2


2







{\displaystyle \scriptstyle {\sqrt {X_{1}^{2}\,+\,X_{2}^{2}}}}

 has the Rayleigh distribution.

Combination of two or more independent random variables[edit]

If X1, X2, …, Xn are independent standard normal random variables, then the sum of their squares has the chi-squared distribution with n degrees of freedom









X

1


2


+
⋯
+

X

n


2


 
∼
 

χ

n


2


.


{\displaystyle X_{1}^{2}+\cdots +X_{n}^{2}\ \sim \ \chi _{n}^{2}.}

.




If X1, X2, …, Xn are independent normally distributed random variables with means μ and variances σ2, then their sample mean is independent from the sample standard deviation,[40] which can be demonstrated using Basu's theorem or Cochran's theorem.[41] The ratio of these two quantities will have the Student's t-distribution with n − 1 degrees of freedom:








t
=





X
¯


−
μ


S

/



n





=





1
n


(

X

1


+
⋯
+

X

n


)
−
μ




1

n
(
n
−
1
)




[
(

X

1


−


X
¯



)

2


+
⋯
+
(

X

n


−


X
¯



)

2


]




 
∼
 

t

n
−
1


.


{\displaystyle t={\frac {{\overline {X}}-\mu }{S/{\sqrt {n}}}}={\frac {{\frac {1}{n}}(X_{1}+\cdots +X_{n})-\mu }{\sqrt {{\frac {1}{n(n-1)}}\left[(X_{1}-{\overline {X}})^{2}+\cdots +(X_{n}-{\overline {X}})^{2}\right]}}}\ \sim \ t_{n-1}.}






' If X1, …, Xn, Y1, …, Ym are independent standard normal random variables, then the ratio of their normalized sums of squares will have the F-distribution with (n, m) degrees of freedom:[42]








F
=




(

X

1


2


+

X

2


2


+
⋯
+

X

n


2


)


/

n



(

Y

1


2


+

Y

2


2


+
⋯
+

Y

m


2


)


/

m



 
∼
 

F

n
,

m


.


{\displaystyle F={\frac {\left(X_{1}^{2}+X_{2}^{2}+\cdots +X_{n}^{2}\right)/n}{\left(Y_{1}^{2}+Y_{2}^{2}+\cdots +Y_{m}^{2}\right)/m}}\ \sim \ F_{n,\,m}.}





Operations on the density function[edit]
The split normal distribution is most directly defined in terms of joining scaled sections of the density functions of different normal distributions and rescaling the density to integrate to one. The truncated normal distribution results from rescaling a section of a single density function.
Extensions[edit]
The notion of normal distribution, being one of the most important distributions in probability theory, has been extended far beyond the standard framework of the univariate (that is one-dimensional) case (Case 1). All these extensions are also called normal or Gaussian laws, so a certain ambiguity in names exists.

The multivariate normal distribution describes the Gaussian law in the k-dimensional Euclidean space. A vector X ∈ Rk is multivariate-normally distributed if any linear combination of its components ∑k
j=1aj Xj has a (univariate) normal distribution. The variance of X is a k×k symmetric positive-definite matrix V. The multivariate normal distribution is a special case of the elliptical distributions. As such, its iso-density loci in the k = 2 case are ellipses and in the case of arbitrary k are ellipsoids.
Rectified Gaussian distribution a rectified version of normal distribution with all the negative elements reset to 0
Complex normal distribution deals with the complex normal vectors. A complex vector X ∈ Ck is said to be normal if both its real and imaginary components jointly possess a 2k-dimensional multivariate normal distribution. The variance-covariance structure of X is described by two matrices: the variance matrix Γ, and the relation matrix C.
Matrix normal distribution describes the case of normally distributed matrices.
Gaussian processes are the normally distributed stochastic processes. These can be viewed as elements of some infinite-dimensional Hilbert space H, and thus are the analogues of multivariate normal vectors for the case k = ∞. A random element h ∈ H is said to be normal if for any constant a ∈ H the scalar product (a, h) has a (univariate) normal distribution. The variance structure of such Gaussian random element can be described in terms of the linear covariance operator K: H → H. Several Gaussian processes became popular enough to have their own names:

Brownian motion,
Brownian bridge,
Ornstein–Uhlenbeck process.


Gaussian q-distribution is an abstract mathematical construction that represents a "q-analogue" of the normal distribution.
the q-Gaussian is an analogue of the Gaussian distribution, in the sense that it maximises the Tsallis entropy, and is one type of Tsallis distribution. Note that this distribution is different from the Gaussian q-distribution above.

A random variable X has a two-piece normal distribution if it has a distribution






f

X


(
x
)
=
N
(
μ
,

σ

1


2


)

 if 

x
≤
μ


{\displaystyle f_{X}(x)=N(\mu ,\sigma _{1}^{2}){\text{ if }}x\leq \mu }







f

X


(
x
)
=
N
(
μ
,

σ

2


2


)

 if 

x
≥
μ


{\displaystyle f_{X}(x)=N(\mu ,\sigma _{2}^{2}){\text{ if }}x\geq \mu }



where μ is the mean and σ1 and σ2 are the standard deviations of the distribution to the left and right of the mean respectively.
The mean, variance and third central moment of this distribution have been determined[43]





E
⁡
(
X
)
=
μ
+



2
π



(

σ

2


−

σ

1


)


{\displaystyle \operatorname {E} (X)=\mu +{\sqrt {\frac {2}{\pi }}}(\sigma _{2}-\sigma _{1})}






V
⁡
(
X
)
=

(
1
−


2
π


)

(

σ

2


−

σ

1



)

2


+

σ

1



σ

2




{\displaystyle \operatorname {V} (X)=\left(1-{\frac {2}{\pi }}\right)(\sigma _{2}-\sigma _{1})^{2}+\sigma _{1}\sigma _{2}}






T
⁡
(
X
)
=



2
π



(

σ

2


−

σ

1


)

[

(


4
π


−
1
)

(

σ

2


−

σ

1



)

2


+

σ

1



σ

2


]



{\displaystyle \operatorname {T} (X)={\sqrt {\frac {2}{\pi }}}(\sigma _{2}-\sigma _{1})\left[\left({\frac {4}{\pi }}-1\right)(\sigma _{2}-\sigma _{1})^{2}+\sigma _{1}\sigma _{2}\right]}



where E(X), V(X) and T(X) are the mean, variance, and third central moment respectively.
One of the main practical uses of the Gaussian law is to model the empirical distributions of many different random variables encountered in practice. In such case a possible extension would be a richer family of distributions, having more than two parameters and therefore being able to fit the empirical distribution more accurately. The examples of such extensions are:

Pearson distribution — a four-parameter family of probability distributions that extend the normal law to include different skewness and kurtosis values.
The generalized normal distribution, also known as the exponential power distribution, allows for distribution tails with thicker or thinner asymptotic behaviors.

Normality tests[edit]
Main article: Normality tests
Normality tests assess the likelihood that the given data set {x1, …, xn} comes from a normal distribution. Typically the null hypothesis H0 is that the observations are distributed normally with unspecified mean μ and variance σ2, versus the alternative Ha that the distribution is arbitrary. Many tests (over 40) have been devised for this problem, the more prominent of them are outlined below:

"Visual" tests are more intuitively appealing but subjective at the same time, as they rely on informal human judgement to accept or reject the null hypothesis.

Q-Q plot— is a plot of the sorted values from the data set against the expected values of the corresponding quantiles from the standard normal distribution. That is, it's a plot of point of the form (Φ−1(pk), x(k)), where plotting points pk are equal to pk = (k − α)/(n + 1 − 2α) and α is an adjustment constant, which can be anything between 0 and 1. If the null hypothesis is true, the plotted points should approximately lie on a straight line.
P-P plot— similar to the Q-Q plot, but used much less frequently. This method consists of plotting the points (Φ(z(k)), pk), where 





z

(
k
)


=
(

x

(
k
)


−



μ
^



)

/




σ
^






{\displaystyle \scriptstyle z_{(k)}=(x_{(k)}-{\hat {\mu }})/{\hat {\sigma }}}

. For normally distributed data this plot should lie on a 45° line between (0, 0) and (1, 1).
Shapiro-Wilk test employs the fact that the line in the Q-Q plot has the slope of σ. The test compares the least squares estimate of that slope with the value of the sample variance, and rejects the null hypothesis if these two quantities differ significantly.
Normal probability plot (rankit plot)


Moment tests:

D'Agostino's K-squared test
Jarque–Bera test


Empirical distribution function tests:

Lilliefors test (an adaptation of the Kolmogorov–Smirnov test)
Anderson–Darling test



Estimation of parameters[edit]
See also: Maximum likelihood § Continuous distribution, continuous parameter space
It is often the case that we don't know the parameters of the normal distribution, but instead want to estimate them. That is, having a sample (x1, …, xn) from a normal N(μ, σ2) population we would like to learn the approximate values of parameters μ and σ2. The standard approach to this problem is the maximum likelihood method, which requires maximization of the log-likelihood function:





ln
⁡


L


(
μ
,

σ

2


)
=

∑

i
=
1


n


ln
⁡
f
(

x

i


;

μ
,

σ

2


)
=
−


n
2


ln
⁡
(
2
π
)
−


n
2


ln
⁡

σ

2


−


1

2

σ

2






∑

i
=
1


n


(

x

i


−
μ

)

2


.


{\displaystyle \ln {\mathcal {L}}(\mu ,\sigma ^{2})=\sum _{i=1}^{n}\ln f(x_{i};\,\mu ,\sigma ^{2})=-{\frac {n}{2}}\ln(2\pi )-{\frac {n}{2}}\ln \sigma ^{2}-{\frac {1}{2\sigma ^{2}}}\sum _{i=1}^{n}(x_{i}-\mu )^{2}.}



Taking derivatives with respect to μ and σ2 and solving the resulting system of first order conditions yields the maximum likelihood estimates:








μ
^



=


x
¯


≡


1
n



∑

i
=
1


n



x

i


,





σ
^




2


=


1
n



∑

i
=
1


n


(

x

i


−


x
¯



)

2


.


{\displaystyle {\hat {\mu }}={\overline {x}}\equiv {\frac {1}{n}}\sum _{i=1}^{n}x_{i},\qquad {\hat {\sigma }}^{2}={\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-{\overline {x}})^{2}.}



Sample mean[edit]
See also: Standard error of the mean
Estimator 







μ
^






{\displaystyle \scriptstyle {\hat {\mu }}}

 is called the sample mean, since it is the arithmetic mean of all observations. The statistic 






x
¯





{\displaystyle \scriptstyle {\overline {x}}}

 is complete and sufficient for μ, and therefore by the Lehmann–Scheffé theorem, 







μ
^






{\displaystyle \scriptstyle {\hat {\mu }}}

 is the uniformly minimum variance unbiased (UMVU) estimator.[44] In finite samples it is distributed normally:








μ
^



 
∼
 


N


(
μ
,



σ

2






/

n
)
.


{\displaystyle {\hat {\mu }}\ \sim \ {\mathcal {N}}(\mu ,\,\,\sigma ^{2}\!\!\;/n).}



The variance of this estimator is equal to the μμ-element of the inverse Fisher information matrix 







I



−
1





{\displaystyle \scriptstyle {\mathcal {I}}^{-1}}

. This implies that the estimator is finite-sample efficient. Of practical importance is the fact that the standard error of 







μ
^






{\displaystyle \scriptstyle {\hat {\mu }}}

 is proportional to 




1

/



n





{\displaystyle \scriptstyle 1/{\sqrt {n}}}

, that is, if one wishes to decrease the standard error by a factor of 10, one must increase the number of points in the sample by a factor of 100. This fact is widely used in determining sample sizes for opinion polls and the number of trials in Monte Carlo simulations.
From the standpoint of the asymptotic theory, 







μ
^






{\displaystyle \scriptstyle {\hat {\mu }}}

 is consistent, that is, it converges in probability to μ as n → ∞. The estimator is also asymptotically normal, which is a simple corollary of the fact that it is normal in finite samples:







n


(



μ
^



−
μ
)
 


→

d



 


N


(
0
,


σ

2


)
.


{\displaystyle {\sqrt {n}}({\hat {\mu }}-\mu )\ {\xrightarrow {d}}\ {\mathcal {N}}(0,\,\sigma ^{2}).}



Sample variance[edit]
See also: Standard deviation § Estimation, and Variance § Estimation
The estimator 








σ
^




2





{\displaystyle \scriptstyle {\hat {\sigma }}^{2}}

 is called the sample variance, since it is the variance of the sample (x1, …, xn). In practice, another estimator is often used instead of the 








σ
^




2





{\displaystyle \scriptstyle {\hat {\sigma }}^{2}}

. This other estimator is denoted s2, and is also called the sample variance, which represents a certain ambiguity in terminology; its square root s is called the sample standard deviation. The estimator s2 differs from 








σ
^




2





{\displaystyle \scriptstyle {\hat {\sigma }}^{2}}

 by having (n − 1) instead of n in the denominator (the so-called Bessel's correction):






s

2


=


n

n
−
1








σ
^




2


=


1

n
−
1




∑

i
=
1


n


(

x

i


−


x
¯



)

2


.


{\displaystyle s^{2}={\frac {n}{n-1}}\,{\hat {\sigma }}^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}(x_{i}-{\overline {x}})^{2}.}



The difference between s2 and 








σ
^




2





{\displaystyle \scriptstyle {\hat {\sigma }}^{2}}

 becomes negligibly small for large n's. In finite samples however, the motivation behind the use of s2 is that it is an unbiased estimator of the underlying parameter σ2, whereas 








σ
^




2





{\displaystyle \scriptstyle {\hat {\sigma }}^{2}}

 is biased. Also, by the Lehmann–Scheffé theorem the estimator s2 is uniformly minimum variance unbiased (UMVU),[44] which makes it the "best" estimator among all unbiased ones. However it can be shown that the biased estimator 








σ
^




2





{\displaystyle \scriptstyle {\hat {\sigma }}^{2}}

 is "better" than the s2 in terms of the mean squared error (MSE) criterion. In finite samples both s2 and 








σ
^




2





{\displaystyle \scriptstyle {\hat {\sigma }}^{2}}

 have scaled chi-squared distribution with (n − 1) degrees of freedom:






s

2


 
∼
 



σ

2



n
−
1



⋅

χ

n
−
1


2


,





σ
^




2


 
∼
 



σ

2


n


⋅

χ

n
−
1


2


 
.


{\displaystyle s^{2}\ \sim \ {\frac {\sigma ^{2}}{n-1}}\cdot \chi _{n-1}^{2},\qquad {\hat {\sigma }}^{2}\ \sim \ {\frac {\sigma ^{2}}{n}}\cdot \chi _{n-1}^{2}\ .}



The first of these expressions shows that the variance of s2 is equal to 2σ4/(n−1), which is slightly greater than the σσ-element of the inverse Fisher information matrix 







I



−
1





{\displaystyle \scriptstyle {\mathcal {I}}^{-1}}

. Thus, s2 is not an efficient estimator for σ2, and moreover, since s2 is UMVU, we can conclude that the finite-sample efficient estimator for σ2 does not exist.
Applying the asymptotic theory, both estimators s2 and 








σ
^




2





{\displaystyle \scriptstyle {\hat {\sigma }}^{2}}

 are consistent, that is they converge in probability to σ2 as the sample size n → ∞. The two estimators are also both asymptotically normal:







n


(




σ
^




2


−

σ

2


)
≃


n


(

s

2


−

σ

2


)
 


→

d



 


N


(
0
,

2

σ

4


)
.


{\displaystyle {\sqrt {n}}({\hat {\sigma }}^{2}-\sigma ^{2})\simeq {\sqrt {n}}(s^{2}-\sigma ^{2})\ {\xrightarrow {d}}\ {\mathcal {N}}(0,\,2\sigma ^{4}).}



In particular, both estimators are asymptotically efficient for σ2.
Confidence intervals[edit]
See also: Studentization
By Cochran's theorem, for normal distributions the sample mean 







μ
^






{\displaystyle \scriptstyle {\hat {\mu }}}

 and the sample variance s2 are independent, which means there can be no gain in considering their joint distribution. There is also a converse theorem: if in a sample the sample mean and sample variance are independent, then the sample must have come from the normal distribution. The independence between 







μ
^






{\displaystyle \scriptstyle {\hat {\mu }}}

 and s can be employed to construct the so-called t-statistic:





t
=






μ
^



−
μ


s

/



n





=





x
¯


−
μ




1

n
(
n
−
1
)



∑
(

x

i


−


x
¯



)

2





 
∼
 

t

n
−
1




{\displaystyle t={\frac {{\hat {\mu }}-\mu }{s/{\sqrt {n}}}}={\frac {{\overline {x}}-\mu }{\sqrt {{\frac {1}{n(n-1)}}\sum (x_{i}-{\overline {x}})^{2}}}}\ \sim \ t_{n-1}}



This quantity t has the Student's t-distribution with (n − 1) degrees of freedom, and it is an ancillary statistic (independent of the value of the parameters). Inverting the distribution of this t-statistics will allow us to construct the confidence interval for μ;[45] similarly, inverting the χ2 distribution of the statistic s2 will give us the confidence interval for σ2:[46]










μ
∈

[




μ
^



−

t

n
−
1
,
1
−
α

/

2





1

n



s
,
 
 



μ
^



+

t

n
−
1
,
1
−
α

/

2





1

n



s

]

≈

[




μ
^



−

|


z

α

/

2



|



1

n



s
,
 
 



μ
^



+

|


z

α

/

2



|



1

n



s

]

,






σ

2


∈

[




(
n
−
1
)

s

2




χ

n
−
1
,
1
−
α

/

2


2




,
 
 



(
n
−
1
)

s

2




χ

n
−
1
,
α

/

2


2





]

≈

[


s

2


−

|


z

α

/

2



|




2


n




s

2


,
 
 

s

2


+

|


z

α

/

2



|




2


n




s

2



]

,






{\displaystyle {\begin{aligned}&\mu \in \left[\,{\hat {\mu }}-t_{n-1,1-\alpha /2}\,{\frac {1}{\sqrt {n}}}s,\ \ {\hat {\mu }}+t_{n-1,1-\alpha /2}\,{\frac {1}{\sqrt {n}}}s\,\right]\approx \left[\,{\hat {\mu }}-|z_{\alpha /2}|{\frac {1}{\sqrt {n}}}s,\ \ {\hat {\mu }}+|z_{\alpha /2}|{\frac {1}{\sqrt {n}}}s\,\right],\\&\sigma ^{2}\in \left[\,{\frac {(n-1)s^{2}}{\chi _{n-1,1-\alpha /2}^{2}}},\ \ {\frac {(n-1)s^{2}}{\chi _{n-1,\alpha /2}^{2}}}\,\right]\approx \left[\,s^{2}-|z_{\alpha /2}|{\frac {\sqrt {2}}{\sqrt {n}}}s^{2},\ \ s^{2}+|z_{\alpha /2}|{\frac {\sqrt {2}}{\sqrt {n}}}s^{2}\,\right],\end{aligned}}}



where tk,p and χ 2
k,p  are the pth quantiles of the t- and χ2-distributions respectively. These confidence intervals are of the confidence level 1 − α, meaning that the true values μ and σ2 fall outside of these intervals with probability (or significance level) α. In practice people usually take α = 5%, resulting in the 95% confidence intervals. The approximate formulas in the display above were derived from the asymptotic distributions of 







μ
^






{\displaystyle \scriptstyle {\hat {\mu }}}

 and s2. The approximate formulas become valid for large values of n, and are more convenient for the manual calculation since the standard normal quantiles zα/2 do not depend on n. In particular, the most popular value of α = 5%, results in |z0.025| = 1.96.
Bayesian analysis of the normal distribution[edit]
Bayesian analysis of normally distributed data is complicated by the many different possibilities that may be considered:

Either the mean, or the variance, or neither, may be considered a fixed quantity.
When the variance is unknown, analysis may be done directly in terms of the variance, or in terms of the precision, the reciprocal of the variance. The reason for expressing the formulas in terms of precision is that the analysis of most cases is simplified.
Both univariate and multivariate cases need to be considered.
Either conjugate or improper prior distributions may be placed on the unknown variables.
An additional set of cases occurs in Bayesian linear regression, where in the basic model the data is assumed to be normally distributed, and normal priors are placed on the regression coefficients. The resulting analysis is similar to the basic cases of independent identically distributed data, but more complex.

The formulas for the non-linear-regression cases are summarized in the conjugate prior article.
Sum of two quadratics[edit]
Scalar form[edit]
The following auxiliary formula is useful for simplifying the posterior update equations, which otherwise become fairly tedious.





a
(
x
−
y

)

2


+
b
(
x
−
z

)

2


=
(
a
+
b
)


(
x
−



a
y
+
b
z


a
+
b



)


2


+



a
b


a
+
b



(
y
−
z

)

2




{\displaystyle a(x-y)^{2}+b(x-z)^{2}=(a+b)\left(x-{\frac {ay+bz}{a+b}}\right)^{2}+{\frac {ab}{a+b}}(y-z)^{2}}



This equation rewrites the sum of two quadratics in x by expanding the squares, grouping the terms in x, and completing the square. Note the following about the complex constant factors attached to some of the terms:

The factor 






a
y
+
b
z


a
+
b





{\displaystyle {\frac {ay+bz}{a+b}}}

 has the form of a weighted average of y and z.







a
b


a
+
b



=


1



1
a


+


1
b





=
(

a

−
1


+

b

−
1



)

−
1


.


{\displaystyle {\frac {ab}{a+b}}={\frac {1}{{\frac {1}{a}}+{\frac {1}{b}}}}=(a^{-1}+b^{-1})^{-1}.}

 This shows that this factor can be thought of as resulting from a situation where the reciprocals of quantities a and b add directly, so to combine a and b themselves, it's necessary to reciprocate, add, and reciprocate the result again to get back into the original units. This is exactly the sort of operation performed by the harmonic mean, so it is not surprising that 






a
b


a
+
b





{\displaystyle {\frac {ab}{a+b}}}

 is one-half the harmonic mean of a and b.

Vector form[edit]
A similar formula can be written for the sum of two vector quadratics: If x, y, z are vectors of length k, and A and B are symmetric, invertible matrices of size 



k
×
k


{\displaystyle k\times k}

, then











(

y

−

x


)
′


A

(

y

−

x

)
+
(

x

−

z


)
′


B

(

x

−

z

)




=





(

x

−

c


)
′

(

A

+

B

)
(

x

−

c

)
+
(

y

−

z


)
′

(


A


−
1


+


B


−
1



)

−
1


(

y

−

z

)






{\displaystyle {\begin{aligned}&(\mathbf {y} -\mathbf {x} )'\mathbf {A} (\mathbf {y} -\mathbf {x} )+(\mathbf {x} -\mathbf {z} )'\mathbf {B} (\mathbf {x} -\mathbf {z} )\\={}&(\mathbf {x} -\mathbf {c} )'(\mathbf {A} +\mathbf {B} )(\mathbf {x} -\mathbf {c} )+(\mathbf {y} -\mathbf {z} )'(\mathbf {A} ^{-1}+\mathbf {B} ^{-1})^{-1}(\mathbf {y} -\mathbf {z} )\end{aligned}}}



where






c

=
(

A

+

B


)

−
1


(

A


y

+

B


z

)


{\displaystyle \mathbf {c} =(\mathbf {A} +\mathbf {B} )^{-1}(\mathbf {A} \mathbf {y} +\mathbf {B} \mathbf {z} )}



Note that the form x′ A x is called a quadratic form and is a scalar:







x

′


A


x

=

∑

i
,
j



a

i
j



x

i



x

j




{\displaystyle \mathbf {x} '\mathbf {A} \mathbf {x} =\sum _{i,j}a_{ij}x_{i}x_{j}}



In other words, it sums up all possible combinations of products of pairs of elements from x, with a separate coefficient for each. In addition, since 




x

i



x

j


=

x

j



x

i




{\displaystyle x_{i}x_{j}=x_{j}x_{i}}

, only the sum 




a

i
j


+

a

j
i




{\displaystyle a_{ij}+a_{ji}}

 matters for any off-diagonal elements of A, and there is no loss of generality in assuming that A is symmetric. Furthermore, if A is symmetric, then the form 





x

′


A


y

=


y

′


A


x

.


{\displaystyle \mathbf {x} '\mathbf {A} \mathbf {y} =\mathbf {y} '\mathbf {A} \mathbf {x} .}


Sum of differences from the mean[edit]
Another useful formula is as follows:






∑

i
=
1


n


(

x

i


−
μ

)

2


=

∑

i
=
1


n


(

x

i


−



x
¯




)

2


+
n
(



x
¯



−
μ

)

2




{\displaystyle \sum _{i=1}^{n}(x_{i}-\mu )^{2}=\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}+n({\bar {x}}-\mu )^{2}}



where 






x
¯



=


1
n



∑

i
=
1


n



x

i


.


{\displaystyle {\bar {x}}={\frac {1}{n}}\sum _{i=1}^{n}x_{i}.}


With known variance[edit]
For a set of i.i.d. normally distributed data points X of size n where each individual point x follows 



x
∼


N


(
μ
,

σ

2


)


{\displaystyle x\sim {\mathcal {N}}(\mu ,\sigma ^{2})}

 with known variance σ2, the conjugate prior distribution is also normally distributed.
This can be shown more easily by rewriting the variance as the precision, i.e. using τ = 1/σ2. Then if 



x
∼


N


(
μ
,
1

/

τ
)


{\displaystyle x\sim {\mathcal {N}}(\mu ,1/\tau )}

 and 



μ
∼


N


(

μ

0


,
1

/


τ

0


)
,


{\displaystyle \mu \sim {\mathcal {N}}(\mu _{0},1/\tau _{0}),}

 we proceed as follows.
First, the likelihood function is (using the formula above for the sum of differences from the mean):









p
(

X

∣
μ
,
τ
)



=

∏

i
=
1


n





τ

2
π




exp
⁡

(
−


1
2


τ
(

x

i


−
μ

)

2


)







=


(


τ

2
π



)



n
2



exp
⁡

(
−


1
2


τ

∑

i
=
1


n


(

x

i


−
μ

)

2


)







=


(


τ

2
π



)



n
2



exp
⁡

[
−


1
2


τ

(

∑

i
=
1


n


(

x

i


−



x
¯




)

2


+
n
(



x
¯



−
μ

)

2


)

]

.






{\displaystyle {\begin{aligned}p(\mathbf {X} \mid \mu ,\tau )&=\prod _{i=1}^{n}{\sqrt {\frac {\tau }{2\pi }}}\exp \left(-{\frac {1}{2}}\tau (x_{i}-\mu )^{2}\right)\\&=\left({\frac {\tau }{2\pi }}\right)^{\frac {n}{2}}\exp \left(-{\frac {1}{2}}\tau \sum _{i=1}^{n}(x_{i}-\mu )^{2}\right)\\&=\left({\frac {\tau }{2\pi }}\right)^{\frac {n}{2}}\exp \left[-{\frac {1}{2}}\tau \left(\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}+n({\bar {x}}-\mu )^{2}\right)\right].\end{aligned}}}



Then, we proceed as follows:









p
(
μ
∣

X

)



∝
p
(

X

∣
μ
)
p
(
μ
)






=


(


τ

2
π



)



n
2



exp
⁡

[
−


1
2


τ

(

∑

i
=
1


n


(

x

i


−



x
¯




)

2


+
n
(



x
¯



−
μ

)

2


)

]





τ

0



2
π




exp
⁡

(
−


1
2



τ

0


(
μ
−

μ

0



)

2


)







∝
exp
⁡

(
−


1
2



(
τ

(

∑

i
=
1


n


(

x

i


−



x
¯




)

2


+
n
(



x
¯



−
μ

)

2


)

+

τ

0


(
μ
−

μ

0



)

2


)

)







∝
exp
⁡

(
−


1
2



(
n
τ
(



x
¯



−
μ

)

2


+

τ

0


(
μ
−

μ

0



)

2


)

)







=
exp
⁡

(
−


1
2


(
n
τ
+

τ

0


)


(
μ
−




n
τ



x
¯



+

τ

0



μ

0




n
τ
+

τ

0






)


2


+



n
τ

τ

0




n
τ
+

τ

0





(



x
¯



−

μ

0



)

2


)







∝
exp
⁡

(
−


1
2


(
n
τ
+

τ

0


)


(
μ
−




n
τ



x
¯



+

τ

0



μ

0




n
τ
+

τ

0






)


2


)







{\displaystyle {\begin{aligned}p(\mu \mid \mathbf {X} )&\propto p(\mathbf {X} \mid \mu )p(\mu )\\&=\left({\frac {\tau }{2\pi }}\right)^{\frac {n}{2}}\exp \left[-{\frac {1}{2}}\tau \left(\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}+n({\bar {x}}-\mu )^{2}\right)\right]{\sqrt {\frac {\tau _{0}}{2\pi }}}\exp \left(-{\frac {1}{2}}\tau _{0}(\mu -\mu _{0})^{2}\right)\\&\propto \exp \left(-{\frac {1}{2}}\left(\tau \left(\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}+n({\bar {x}}-\mu )^{2}\right)+\tau _{0}(\mu -\mu _{0})^{2}\right)\right)\\&\propto \exp \left(-{\frac {1}{2}}\left(n\tau ({\bar {x}}-\mu )^{2}+\tau _{0}(\mu -\mu _{0})^{2}\right)\right)\\&=\exp \left(-{\frac {1}{2}}(n\tau +\tau _{0})\left(\mu -{\dfrac {n\tau {\bar {x}}+\tau _{0}\mu _{0}}{n\tau +\tau _{0}}}\right)^{2}+{\frac {n\tau \tau _{0}}{n\tau +\tau _{0}}}({\bar {x}}-\mu _{0})^{2}\right)\\&\propto \exp \left(-{\frac {1}{2}}(n\tau +\tau _{0})\left(\mu -{\dfrac {n\tau {\bar {x}}+\tau _{0}\mu _{0}}{n\tau +\tau _{0}}}\right)^{2}\right)\end{aligned}}}



In the above derivation, we used the formula above for the sum of two quadratics and eliminated all constant factors not involving μ. The result is the kernel of a normal distribution, with mean 






n
τ



x
¯



+

τ

0



μ

0




n
τ
+

τ

0







{\displaystyle {\frac {n\tau {\bar {x}}+\tau _{0}\mu _{0}}{n\tau +\tau _{0}}}}

 and precision 



n
τ
+

τ

0




{\displaystyle n\tau +\tau _{0}}

, i.e.





p
(
μ
∣

X

)
∼


N



(



n
τ



x
¯



+

τ

0



μ

0




n
τ
+

τ

0





,


1

n
τ
+

τ

0





)



{\displaystyle p(\mu \mid \mathbf {X} )\sim {\mathcal {N}}\left({\frac {n\tau {\bar {x}}+\tau _{0}\mu _{0}}{n\tau +\tau _{0}}},{\frac {1}{n\tau +\tau _{0}}}\right)}



This can be written as a set of Bayesian update equations for the posterior parameters in terms of the prior parameters:










τ

0

′




=

τ

0


+
n
τ





μ

0

′




=



n
τ



x
¯



+

τ

0



μ

0




n
τ
+

τ

0












x
¯






=


1
n



∑

i
=
1


n



x

i








{\displaystyle {\begin{aligned}\tau _{0}'&=\tau _{0}+n\tau \\\mu _{0}'&={\frac {n\tau {\bar {x}}+\tau _{0}\mu _{0}}{n\tau +\tau _{0}}}\\{\bar {x}}&={\frac {1}{n}}\sum _{i=1}^{n}x_{i}\end{aligned}}}



That is, to combine n data points with total precision of nτ (or equivalently, total variance of n/σ2) and mean of values 






x
¯





{\displaystyle {\bar {x}}}

, derive a new total precision simply by adding the total precision of the data to the prior total precision, and form a new mean through a precision-weighted average, i.e. a weighted average of the data mean and the prior mean, each weighted by the associated total precision. This makes logical sense if the precision is thought of as indicating the certainty of the observations: In the distribution of the posterior mean, each of the input components is weighted by its certainty, and the certainty of this distribution is the sum of the individual certainties. (For the intuition of this, compare the expression "the whole is (or is not) greater than the sum of its parts". In addition, consider that the knowledge of the posterior comes from a combination of the knowledge of the prior and likelihood, so it makes sense that we are more certain of it than of either of its components.)
The above formula reveals why it is more convenient to do Bayesian analysis of conjugate priors for the normal distribution in terms of the precision. The posterior precision is simply the sum of the prior and likelihood precisions, and the posterior mean is computed through a precision-weighted average, as described above. The same formulas can be written in terms of variance by reciprocating all the precisions, yielding the more ugly formulas












σ

0


2



′




=


1



n

σ

2




+


1

σ

0


2












μ

0

′




=






n



x
¯





σ

2




+



μ

0



σ

0


2








n

σ

2




+


1

σ

0


2














x
¯






=


1
n



∑

i
=
1


n



x

i








{\displaystyle {\begin{aligned}{\sigma _{0}^{2}}'&={\frac {1}{{\frac {n}{\sigma ^{2}}}+{\frac {1}{\sigma _{0}^{2}}}}}\\\mu _{0}'&={\frac {{\frac {n{\bar {x}}}{\sigma ^{2}}}+{\frac {\mu _{0}}{\sigma _{0}^{2}}}}{{\frac {n}{\sigma ^{2}}}+{\frac {1}{\sigma _{0}^{2}}}}}\\{\bar {x}}&={\frac {1}{n}}\sum _{i=1}^{n}x_{i}\end{aligned}}}



With known mean[edit]
For a set of i.i.d. normally distributed data points X of size n where each individual point x follows 



x
∼


N


(
μ
,

σ

2


)


{\displaystyle x\sim {\mathcal {N}}(\mu ,\sigma ^{2})}

 with known mean μ, the conjugate prior of the variance has an inverse gamma distribution or a scaled inverse chi-squared distribution. The two are equivalent except for having different parameterizations. Although the inverse gamma is more commonly used, we use the scaled inverse chi-squared for the sake of convenience. The prior for σ2 is as follows:





p
(

σ

2


∣

ν

0


,

σ

0


2


)
=



(

σ

0


2





ν

0


2



)



ν

0


2





Γ

(



ν

0


2


)




 



exp
⁡

[



−

ν

0



σ

0


2




2

σ

2





]



(

σ

2



)

1
+



ν

0


2







∝



exp
⁡

[



−

ν

0



σ

0


2




2

σ

2





]



(

σ

2



)

1
+



ν

0


2









{\displaystyle p(\sigma ^{2}\mid \nu _{0},\sigma _{0}^{2})={\frac {(\sigma _{0}^{2}{\frac {\nu _{0}}{2}})^{\frac {\nu _{0}}{2}}}{\Gamma \left({\frac {\nu _{0}}{2}}\right)}}~{\frac {\exp \left[{\frac {-\nu _{0}\sigma _{0}^{2}}{2\sigma ^{2}}}\right]}{(\sigma ^{2})^{1+{\frac {\nu _{0}}{2}}}}}\propto {\frac {\exp \left[{\frac {-\nu _{0}\sigma _{0}^{2}}{2\sigma ^{2}}}\right]}{(\sigma ^{2})^{1+{\frac {\nu _{0}}{2}}}}}}



The likelihood function from above, written in terms of the variance, is:









p
(

X

∣
μ
,

σ

2


)



=


(


1

2
π

σ

2





)



n
2



exp
⁡

[
−


1

2

σ

2






∑

i
=
1


n


(

x

i


−
μ

)

2


]







=


(


1

2
π

σ

2





)



n
2



exp
⁡

[
−


S

2

σ

2





]







{\displaystyle {\begin{aligned}p(\mathbf {X} \mid \mu ,\sigma ^{2})&=\left({\frac {1}{2\pi \sigma ^{2}}}\right)^{\frac {n}{2}}\exp \left[-{\frac {1}{2\sigma ^{2}}}\sum _{i=1}^{n}(x_{i}-\mu )^{2}\right]\\&=\left({\frac {1}{2\pi \sigma ^{2}}}\right)^{\frac {n}{2}}\exp \left[-{\frac {S}{2\sigma ^{2}}}\right]\end{aligned}}}



where





S
=

∑

i
=
1


n


(

x

i


−
μ

)

2


.


{\displaystyle S=\sum _{i=1}^{n}(x_{i}-\mu )^{2}.}



Then:









p
(

σ

2


∣

X

)



∝
p
(

X

∣

σ

2


)
p
(

σ

2


)






=


(


1

2
π

σ

2





)



n
2



exp
⁡

[
−


S

2

σ

2





]




(

σ

0


2





ν

0


2



)



ν

0


2





Γ

(



ν

0


2


)




 



exp
⁡

[



−

ν

0



σ

0


2




2

σ

2





]



(

σ

2



)

1
+



ν

0


2













∝


(


1

σ

2




)



n
2





1

(

σ

2



)

1
+



ν

0


2







exp
⁡

[
−


S

2

σ

2





+



−

ν

0



σ

0


2




2

σ

2





]







=


1

(

σ

2



)

1
+




ν

0


+
n

2







exp
⁡

[
−




ν

0



σ

0


2


+
S


2

σ

2





]







{\displaystyle {\begin{aligned}p(\sigma ^{2}\mid \mathbf {X} )&\propto p(\mathbf {X} \mid \sigma ^{2})p(\sigma ^{2})\\&=\left({\frac {1}{2\pi \sigma ^{2}}}\right)^{\frac {n}{2}}\exp \left[-{\frac {S}{2\sigma ^{2}}}\right]{\frac {(\sigma _{0}^{2}{\frac {\nu _{0}}{2}})^{\frac {\nu _{0}}{2}}}{\Gamma \left({\frac {\nu _{0}}{2}}\right)}}~{\frac {\exp \left[{\frac {-\nu _{0}\sigma _{0}^{2}}{2\sigma ^{2}}}\right]}{(\sigma ^{2})^{1+{\frac {\nu _{0}}{2}}}}}\\&\propto \left({\frac {1}{\sigma ^{2}}}\right)^{\frac {n}{2}}{\frac {1}{(\sigma ^{2})^{1+{\frac {\nu _{0}}{2}}}}}\exp \left[-{\frac {S}{2\sigma ^{2}}}+{\frac {-\nu _{0}\sigma _{0}^{2}}{2\sigma ^{2}}}\right]\\&={\frac {1}{(\sigma ^{2})^{1+{\frac {\nu _{0}+n}{2}}}}}\exp \left[-{\frac {\nu _{0}\sigma _{0}^{2}+S}{2\sigma ^{2}}}\right]\end{aligned}}}



The above is also a scaled inverse chi-squared distribution where










ν

0

′




=

ν

0


+
n





ν

0

′




σ

0


2



′




=

ν

0



σ

0


2


+

∑

i
=
1


n


(

x

i


−
μ

)

2








{\displaystyle {\begin{aligned}\nu _{0}'&=\nu _{0}+n\\\nu _{0}'{\sigma _{0}^{2}}'&=\nu _{0}\sigma _{0}^{2}+\sum _{i=1}^{n}(x_{i}-\mu )^{2}\end{aligned}}}



or equivalently










ν

0

′




=

ν

0


+
n







σ

0


2



′




=




ν

0



σ

0


2


+

∑

i
=
1


n


(

x

i


−
μ

)

2





ν

0


+
n









{\displaystyle {\begin{aligned}\nu _{0}'&=\nu _{0}+n\\{\sigma _{0}^{2}}'&={\frac {\nu _{0}\sigma _{0}^{2}+\sum _{i=1}^{n}(x_{i}-\mu )^{2}}{\nu _{0}+n}}\end{aligned}}}



Reparameterizing in terms of an inverse gamma distribution, the result is:










α
′




=
α
+


n
2







β
′




=
β
+




∑

i
=
1


n


(

x

i


−
μ

)

2



2








{\displaystyle {\begin{aligned}\alpha '&=\alpha +{\frac {n}{2}}\\\beta '&=\beta +{\frac {\sum _{i=1}^{n}(x_{i}-\mu )^{2}}{2}}\end{aligned}}}



With unknown mean and unknown variance[edit]
For a set of i.i.d. normally distributed data points X of size n where each individual point x follows 



x
∼


N


(
μ
,

σ

2


)


{\displaystyle x\sim {\mathcal {N}}(\mu ,\sigma ^{2})}

 with unknown mean μ and unknown variance σ2, a combined (multivariate) conjugate prior is placed over the mean and variance, consisting of a normal-inverse-gamma distribution. Logically, this originates as follows:

From the analysis of the case with unknown mean but known variance, we see that the update equations involve sufficient statistics computed from the data consisting of the mean of the data points and the total variance of the data points, computed in turn from the known variance divided by the number of data points.
From the analysis of the case with unknown variance but known mean, we see that the update equations involve sufficient statistics over the data consisting of the number of data points and sum of squared deviations.
Keep in mind that the posterior update values serve as the prior distribution when further data is handled. Thus, we should logically think of our priors in terms of the sufficient statistics just described, with the same semantics kept in mind as much as possible.
To handle the case where both mean and variance are unknown, we could place independent priors over the mean and variance, with fixed estimates of the average mean, total variance, number of data points used to compute the variance prior, and sum of squared deviations. Note however that in reality, the total variance of the mean depends on the unknown variance, and the sum of squared deviations that goes into the variance prior (appears to) depend on the unknown mean. In practice, the latter dependence is relatively unimportant: Shifting the actual mean shifts the generated points by an equal amount, and on average the squared deviations will remain the same. This is not the case, however, with the total variance of the mean: As the unknown variance increases, the total variance of the mean will increase proportionately, and we would like to capture this dependence.
This suggests that we create a conditional prior of the mean on the unknown variance, with a hyperparameter specifying the mean of the pseudo-observations associated with the prior, and another parameter specifying the number of pseudo-observations. This number serves as a scaling parameter on the variance, making it possible to control the overall variance of the mean relative to the actual variance parameter. The prior for the variance also has two hyperparameters, one specifying the sum of squared deviations of the pseudo-observations associated with the prior, and another specifying once again the number of pseudo-observations. Note that each of the priors has a hyperparameter specifying the number of pseudo-observations, and in each case this controls the relative variance of that prior. These are given as two separate hyperparameters so that the variance (aka the confidence) of the two priors can be controlled separately.
This leads immediately to the normal-inverse-gamma distribution, which is the product of the two distributions just defined, with conjugate priors used (an inverse gamma distribution over the variance, and a normal distribution over the mean, conditional on the variance) and with the same four parameters just defined.

The priors are normally defined as follows:









p
(
μ
∣

σ

2


;

μ

0


,

n

0


)



∼


N


(

μ

0


,

σ

2



/


n

0


)




p
(

σ

2


;

ν

0


,

σ

0


2


)



∼
I

χ

2


(

ν

0


,

σ

0


2


)
=
I
G
(

ν

0



/

2
,

ν

0



σ

0


2



/

2
)






{\displaystyle {\begin{aligned}p(\mu \mid \sigma ^{2};\mu _{0},n_{0})&\sim {\mathcal {N}}(\mu _{0},\sigma ^{2}/n_{0})\\p(\sigma ^{2};\nu _{0},\sigma _{0}^{2})&\sim I\chi ^{2}(\nu _{0},\sigma _{0}^{2})=IG(\nu _{0}/2,\nu _{0}\sigma _{0}^{2}/2)\end{aligned}}}



The update equations can be derived, and look as follows:












x
¯






=


1
n



∑

i
=
1


n



x

i







μ

0

′




=




n

0



μ

0


+
n



x
¯






n

0


+
n








n

0

′




=

n

0


+
n





ν

0

′




=

ν

0


+
n





ν

0

′




σ

0


2



′




=

ν

0



σ

0


2


+

∑

i
=
1


n


(

x

i


−



x
¯




)

2


+




n

0


n



n

0


+
n



(

μ

0


−



x
¯




)

2








{\displaystyle {\begin{aligned}{\bar {x}}&={\frac {1}{n}}\sum _{i=1}^{n}x_{i}\\\mu _{0}'&={\frac {n_{0}\mu _{0}+n{\bar {x}}}{n_{0}+n}}\\n_{0}'&=n_{0}+n\\\nu _{0}'&=\nu _{0}+n\\\nu _{0}'{\sigma _{0}^{2}}'&=\nu _{0}\sigma _{0}^{2}+\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}+{\frac {n_{0}n}{n_{0}+n}}(\mu _{0}-{\bar {x}})^{2}\end{aligned}}}



The respective numbers of pseudo-observations add the number of actual observations to them. The new mean hyperparameter is once again a weighted average, this time weighted by the relative numbers of observations. Finally, the update for 




ν

0

′




σ

0


2



′



{\displaystyle \nu _{0}'{\sigma _{0}^{2}}'}

 is similar to the case with known mean, but in this case the sum of squared deviations is taken with respect to the observed data mean rather than the true mean, and as a result a new "interaction term" needs to be added to take care of the additional error source stemming from the deviation between prior and data mean.


  [Proof]

The prior distributions are









p
(
μ
∣

σ

2


;

μ

0


,

n

0


)



∼


N


(

μ

0


,

σ

2



/


n

0


)
=


1

2
π



σ

2



n

0







exp
⁡

(
−



n

0



2

σ

2





(
μ
−

μ

0



)

2


)







∝
(

σ

2



)

−
1

/

2


exp
⁡

(
−



n

0



2

σ

2





(
μ
−

μ

0



)

2


)





p
(

σ

2


;

ν

0


,

σ

0


2


)



∼
I

χ

2


(

ν

0


,

σ

0


2


)
=
I
G
(

ν

0



/

2
,

ν

0



σ

0


2



/

2
)






=



(

σ

0


2



ν

0



/

2

)


ν

0



/

2




Γ
(

ν

0



/

2
)



 



exp
⁡

[



−

ν

0



σ

0


2




2

σ

2





]



(

σ

2



)

1
+

ν

0



/

2











∝

(

σ

2



)

−
(
1
+

ν

0



/

2
)



exp
⁡

[



−

ν

0



σ

0


2




2

σ

2





]

.






{\displaystyle {\begin{aligned}p(\mu \mid \sigma ^{2};\mu _{0},n_{0})&\sim {\mathcal {N}}(\mu _{0},\sigma ^{2}/n_{0})={\frac {1}{\sqrt {2\pi {\frac {\sigma ^{2}}{n_{0}}}}}}\exp \left(-{\frac {n_{0}}{2\sigma ^{2}}}(\mu -\mu _{0})^{2}\right)\\&\propto (\sigma ^{2})^{-1/2}\exp \left(-{\frac {n_{0}}{2\sigma ^{2}}}(\mu -\mu _{0})^{2}\right)\\p(\sigma ^{2};\nu _{0},\sigma _{0}^{2})&\sim I\chi ^{2}(\nu _{0},\sigma _{0}^{2})=IG(\nu _{0}/2,\nu _{0}\sigma _{0}^{2}/2)\\&={\frac {(\sigma _{0}^{2}\nu _{0}/2)^{\nu _{0}/2}}{\Gamma (\nu _{0}/2)}}~{\frac {\exp \left[{\frac {-\nu _{0}\sigma _{0}^{2}}{2\sigma ^{2}}}\right]}{(\sigma ^{2})^{1+\nu _{0}/2}}}\\&\propto {(\sigma ^{2})^{-(1+\nu _{0}/2)}}\exp \left[{\frac {-\nu _{0}\sigma _{0}^{2}}{2\sigma ^{2}}}\right].\end{aligned}}}



Therefore, the joint prior is









p
(
μ
,

σ

2


;

μ

0


,

n

0


,

ν

0


,

σ

0


2


)



=
p
(
μ
∣

σ

2


;

μ

0


,

n

0


)

p
(

σ

2


;

ν

0


,

σ

0


2


)






∝
(

σ

2



)

−
(

ν

0


+
3
)

/

2


exp
⁡

[
−


1

2

σ

2






(

ν

0



σ

0


2


+

n

0


(
μ
−

μ

0



)

2


)

]

.






{\displaystyle {\begin{aligned}p(\mu ,\sigma ^{2};\mu _{0},n_{0},\nu _{0},\sigma _{0}^{2})&=p(\mu \mid \sigma ^{2};\mu _{0},n_{0})\,p(\sigma ^{2};\nu _{0},\sigma _{0}^{2})\\&\propto (\sigma ^{2})^{-(\nu _{0}+3)/2}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(\nu _{0}\sigma _{0}^{2}+n_{0}(\mu -\mu _{0})^{2}\right)\right].\end{aligned}}}



The likelihood function from the section above with known variance is:









p
(

X

∣
μ
,

σ

2


)



=


(


1

2
π

σ

2





)


n

/

2


exp
⁡

[
−


1

2

σ

2






(

∑

i
=
1


n


(

x

i


−
μ

)

2


)

]







{\displaystyle {\begin{aligned}p(\mathbf {X} \mid \mu ,\sigma ^{2})&=\left({\frac {1}{2\pi \sigma ^{2}}}\right)^{n/2}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(\sum _{i=1}^{n}(x_{i}-\mu )^{2}\right)\right]\end{aligned}}}



Writing it in terms of variance rather than precision, we get:









p
(

X

∣
μ
,

σ

2


)



=


(


1

2
π

σ

2





)


n

/

2


exp
⁡

[
−


1

2

σ

2






(

∑

i
=
1


n


(

x

i


−



x
¯




)

2


+
n
(



x
¯



−
μ

)

2


)

]







∝



σ

2




−
n

/

2


exp
⁡

[
−


1

2

σ

2






(
S
+
n
(



x
¯



−
μ

)

2


)

]







{\displaystyle {\begin{aligned}p(\mathbf {X} \mid \mu ,\sigma ^{2})&=\left({\frac {1}{2\pi \sigma ^{2}}}\right)^{n/2}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}+n({\bar {x}}-\mu )^{2}\right)\right]\\&\propto {\sigma ^{2}}^{-n/2}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(S+n({\bar {x}}-\mu )^{2}\right)\right]\end{aligned}}}



where 



S
=

∑

i
=
1


n


(

x

i


−



x
¯




)

2


.


{\displaystyle S=\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}.}


Therefore, the posterior is (dropping the hyperparameters as conditioning factors):









p
(
μ
,

σ

2


∣

X

)



∝
p
(
μ
,

σ

2


)

p
(

X

∣
μ
,

σ

2


)






∝
(

σ

2



)

−
(

ν

0


+
3
)

/

2


exp
⁡

[
−


1

2

σ

2






(

ν

0



σ

0


2


+

n

0


(
μ
−

μ

0



)

2


)

]




σ

2




−
n

/

2


exp
⁡

[
−


1

2

σ

2






(
S
+
n
(



x
¯



−
μ

)

2


)

]







=
(

σ

2



)

−
(

ν

0


+
n
+
3
)

/

2


exp
⁡

[
−


1

2

σ

2






(

ν

0



σ

0


2


+
S
+

n

0


(
μ
−

μ

0



)

2


+
n
(



x
¯



−
μ

)

2


)

]







=
(

σ

2



)

−
(

ν

0


+
n
+
3
)

/

2


exp
⁡

[
−


1

2

σ

2






(

ν

0



σ

0


2


+
S
+




n

0


n



n

0


+
n



(

μ

0


−



x
¯




)

2


+
(

n

0


+
n
)


(
μ
−




n

0



μ

0


+
n



x
¯






n

0


+
n



)


2


)

]







∝
(

σ

2



)

−
1

/

2


exp
⁡

[
−




n

0


+
n


2

σ

2







(
μ
−




n

0



μ

0


+
n



x
¯






n

0


+
n



)


2


]








×
(

σ

2



)

−
(

ν

0



/

2
+
n

/

2
+
1
)


exp
⁡

[
−


1

2

σ

2






(

ν

0



σ

0


2


+
S
+




n

0


n



n

0


+
n



(

μ

0


−



x
¯




)

2


)

]







=



N



μ
∣

σ

2





(




n

0



μ

0


+
n



x
¯






n

0


+
n



,



σ

2




n

0


+
n



)

⋅



I
G




σ

2





(


1
2


(

ν

0


+
n
)
,


1
2



(

ν

0



σ

0


2


+
S
+




n

0


n



n

0


+
n



(

μ

0


−



x
¯




)

2


)

)

.






{\displaystyle {\begin{aligned}p(\mu ,\sigma ^{2}\mid \mathbf {X} )&\propto p(\mu ,\sigma ^{2})\,p(\mathbf {X} \mid \mu ,\sigma ^{2})\\&\propto (\sigma ^{2})^{-(\nu _{0}+3)/2}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(\nu _{0}\sigma _{0}^{2}+n_{0}(\mu -\mu _{0})^{2}\right)\right]{\sigma ^{2}}^{-n/2}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(S+n({\bar {x}}-\mu )^{2}\right)\right]\\&=(\sigma ^{2})^{-(\nu _{0}+n+3)/2}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(\nu _{0}\sigma _{0}^{2}+S+n_{0}(\mu -\mu _{0})^{2}+n({\bar {x}}-\mu )^{2}\right)\right]\\&=(\sigma ^{2})^{-(\nu _{0}+n+3)/2}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(\nu _{0}\sigma _{0}^{2}+S+{\frac {n_{0}n}{n_{0}+n}}(\mu _{0}-{\bar {x}})^{2}+(n_{0}+n)\left(\mu -{\frac {n_{0}\mu _{0}+n{\bar {x}}}{n_{0}+n}}\right)^{2}\right)\right]\\&\propto (\sigma ^{2})^{-1/2}\exp \left[-{\frac {n_{0}+n}{2\sigma ^{2}}}\left(\mu -{\frac {n_{0}\mu _{0}+n{\bar {x}}}{n_{0}+n}}\right)^{2}\right]\\&\quad \times (\sigma ^{2})^{-(\nu _{0}/2+n/2+1)}\exp \left[-{\frac {1}{2\sigma ^{2}}}\left(\nu _{0}\sigma _{0}^{2}+S+{\frac {n_{0}n}{n_{0}+n}}(\mu _{0}-{\bar {x}})^{2}\right)\right]\\&={\mathcal {N}}_{\mu \mid \sigma ^{2}}\left({\frac {n_{0}\mu _{0}+n{\bar {x}}}{n_{0}+n}},{\frac {\sigma ^{2}}{n_{0}+n}}\right)\cdot {\rm {IG}}_{\sigma ^{2}}\left({\frac {1}{2}}(\nu _{0}+n),{\frac {1}{2}}\left(\nu _{0}\sigma _{0}^{2}+S+{\frac {n_{0}n}{n_{0}+n}}(\mu _{0}-{\bar {x}})^{2}\right)\right).\end{aligned}}}



In other words, the posterior distribution has the form of a product of a normal distribution over p(μ | σ2) times an inverse gamma distribution over p(σ2), with parameters that are the same as the update equations above.


Occurrence and applications[edit]
The occurrence of normal distribution in practical problems can be loosely classified into four categories:

Exactly normal distributions;
Approximately normal laws, for example when such approximation is justified by the central limit theorem; and
Distributions modeled as normal – the normal distribution being the distribution with maximum entropy for a given mean and variance.
Regression problems – the normal distribution being found after systematic effects have been modeled sufficiently well.

Exact normality[edit]




The ground state of a quantum harmonic oscillator has the Gaussian distribution.


Certain quantities in physics are distributed normally, as was first demonstrated by James Clerk Maxwell. Examples of such quantities are:

Probability density function of a ground state in a quantum harmonic oscillator.
The position of a particle that experiences diffusion. If initially the particle is located at a specific point (that is its probability distribution is the dirac delta function), then after time t its location is described by a normal distribution with variance t, which satisfies the diffusion equation Template:Sfrac2 f(x,t) = Template:Sfrac2 Template:Sfrac2 f(x,t). If the initial location is given by a certain density function g(x), then the density at time t is the convolution of g and the normal PDF.

Approximate normality[edit]
Approximately normal distributions occur in many situations, as explained by the central limit theorem. When the outcome is produced by many small effects acting additively and independently, its distribution will be close to normal. The normal approximation will not be valid if the effects act multiplicatively (instead of additively), or if there is a single external influence that has a considerably larger magnitude than the rest of the effects.

In counting problems, where the central limit theorem includes a discrete-to-continuum approximation and where infinitely divisible and decomposable distributions are involved, such as

Binomial random variables, associated with binary response variables;
Poisson random variables, associated with rare events;


Thermal radiation has a Bose–Einstein distribution on very short time scales, and a normal distribution on longer timescales due to the central limit theorem.

Assumed normality[edit]




Histogram of sepal widths for Iris versicolor from Fisher's Iris flower data set, with superimposed best-fitting normal distribution.



I can only recognize the occurrence of the normal curve – the Laplacian curve of errors – as a very abnormal phenomenon. It is roughly approximated to in certain distributions; for this reason, and on account for its beautiful simplicity, we may, perhaps, use it as a first approximation, particularly in theoretical investigations.
— Pearson (1901)

There are statistical methods to empirically test that assumption, see the above Normality tests section.

In biology, the logarithm of various variables tend to have a normal distribution, that is, they tend to have a log-normal distribution (after separation on male/female subpopulations), with examples including:

Measures of size of living tissue (length, height, skin area, weight);[47]
The length of inert appendages (hair, claws, nails, teeth) of biological specimens, in the direction of growth; presumably the thickness of tree bark also falls under this category;
Certain physiological measurements, such as blood pressure of adult humans.


In finance, in particular the Black–Scholes model, changes in the logarithm of exchange rates, price indices, and stock market indices are assumed normal (these variables behave like compound interest, not like simple interest, and so are multiplicative). Some mathematicians such as Benoit Mandelbrot have argued that log-Levy distributions, which possesses heavy tails would be a more appropriate model, in particular for the analysis for stock market crashes. The use of the assumption of normal distribution occurring in financial models has also been criticized by Nassim Nicholas Taleb in his works.
Measurement errors in physical experiments are often modeled by a normal distribution. This use of a normal distribution does not imply that one is assuming the measurement errors are normally distributed, rather using the normal distribution produces the most conservative predictions possible given only knowledge about the mean and variance of the errors.[48]





Fitted cumulative normal distribution to October rainfalls, see distribution fitting



In standardized testing, results can be made to have a normal distribution by either selecting the number and difficulty of questions (as in the IQ test) or transforming the raw test scores into "output" scores by fitting them to the normal distribution. For example, the SAT's traditional range of 200–800 is based on a normal distribution with a mean of 500 and a standard deviation of 100.
Many scores are derived from the normal distribution, including percentile ranks ("percentiles" or "quantiles"), normal curve equivalents, stanines, z-scores, and T-scores. Additionally, some behavioral statistical procedures assume that scores are normally distributed; for example, t-tests and ANOVAs. Bell curve grading assigns relative grades based on a normal distribution of scores.
In hydrology the distribution of long duration river discharge or rainfall, e.g. monthly and yearly totals, is often thought to be practically normal according to the central limit theorem.[49] The blue picture illustrates an example of fitting the normal distribution to ranked October rainfalls showing the 90% confidence belt based on the binomial distribution. The rainfall data are represented by plotting positions as part of the cumulative frequency analysis.

Produced normality[edit]
In regression analysis, lack of normality in residuals simply indicates that the model postulated is inadequate in accounting for the tendency in the data and needs to be augmented; in other words, normality in residuals can always be achieved given a properly constructed model.
Generating values from normal distribution[edit]




The bean machine, a device invented by Francis Galton, can be called the first generator of normal random variables. This machine consists of a vertical board with interleaved rows of pins. Small balls are dropped from the top and then bounce randomly left or right as they hit the pins. The balls are collected into bins at the bottom and settle down into a pattern resembling the Gaussian curve.


In computer simulations, especially in applications of the Monte-Carlo method, it is often desirable to generate values that are normally distributed. The algorithms listed below all generate the standard normal deviates, since a N(μ, σ2) can be generated as X = μ + σZ, where Z is standard normal. All these algorithms rely on the availability of a random number generator U capable of producing uniform random variates.

The most straightforward method is based on the probability integral transform property: if U is distributed uniformly on (0,1), then Φ−1(U) will have the standard normal distribution. The drawback of this method is that it relies on calculation of the probit function Φ−1, which cannot be done analytically. Some approximate methods are described in Hart (1968) and in the erf article. Wichura gives a fast algorithm for computing this function to 16 decimal places,[50] which is used by R to compute random variates of the normal distribution.
An easy to program approximate approach, that relies on the central limit theorem, is as follows: generate 12 uniform U(0,1) deviates, add them all up, and subtract 6 – the resulting random variable will have approximately standard normal distribution. In truth, the distribution will be Irwin–Hall, which is a 12-section eleventh-order polynomial approximation to the normal distribution. This random deviate will have a limited range of (−6, 6).[51]
The Box–Muller method uses two independent random numbers U and V distributed uniformly on (0,1). Then the two random variables X and Y








X
=


−
2
ln
⁡
U



cos
⁡
(
2
π
V
)
,

Y
=


−
2
ln
⁡
U



sin
⁡
(
2
π
V
)
.


{\displaystyle X={\sqrt {-2\ln U}}\,\cos(2\pi V),\qquad Y={\sqrt {-2\ln U}}\,\sin(2\pi V).}




will both have the standard normal distribution, and will be independent. This formulation arises because for a bivariate normal random vector (X, Y) the squared norm X2 + Y2 will have the chi-squared distribution with two degrees of freedom, which is an easily generated exponential random variable corresponding to the quantity −2ln(U) in these equations; and the angle is distributed uniformly around the circle, chosen by the random variable V.


Marsaglia polar method is a modification of the Box–Muller method algorithm, which does not require computation of functions sin() and cos(). In this method U and V are drawn from the uniform (−1,1) distribution, and then S = U2 + V2 is computed. If S is greater or equal to one then the method starts over, otherwise two quantities








X
=
U




−
2
ln
⁡
S

S



,

Y
=
V




−
2
ln
⁡
S

S





{\displaystyle X=U{\sqrt {\frac {-2\ln S}{S}}},\qquad Y=V{\sqrt {\frac {-2\ln S}{S}}}}




are returned. Again, X and Y will be independent and standard normally distributed.


The Ratio method[52] is a rejection method. The algorithm proceeds as follows:

Generate two independent uniform deviates U and V;
Compute X = √8/e (V − 0.5)/U;
Optional: if X2 ≤ 5 − 4e1/4U then accept X and terminate algorithm;
Optional: if X2 ≥ 4e−1.35/U + 1.4 then reject X and start over from step 1;
If X2 ≤ −4 lnU then accept X, otherwise start over the algorithm.




The two optional steps allow the evaluation of the logarithm in the last step to be avoided in most cases. These steps can be greatly improved[53] so that the logarithm is rarely evaluated.


The ziggurat algorithm[54] is faster than the Box–Muller transform and still exact. In about 97% of all cases it uses only two random numbers, one random integer and one random uniform, one multiplication and an if-test. Only in 3% of the cases, where the combination of those two falls outside the "core of the ziggurat" (a kind of rejection sampling using logarithms), do exponentials and more uniform random numbers have to be employed.
Integer arithmetic can be used to sample from the standard normal distribution.[55] This method is exact in the sense that it satisfies the conditions of ideal approximation;[56] i.e., it is equivalent to sampling a real number from the standard normal distribution and rounding this to the nearest representable floating point number.
There is also some investigation[57] into the connection between the fast Hadamard transform and the normal distribution, since the transform employs just addition and subtraction and by the central limit theorem random numbers from almost any distribution will be transformed into the normal distribution. In this regard a series of Hadamard transforms can be combined with random permutations to turn arbitrary data sets into a normally distributed data.

Numerical approximations for the normal CDF[edit]
The standard normal CDF is widely used in scientific and statistical computing.
The values Φ(x) may be approximated very accurately by a variety of methods, such as numerical integration, Taylor series, asymptotic series and continued fractions. Different approximations are used depending on the desired level of accuracy.


A very simple and practical approximation is given by Bell [58] with a maximum absolute error of 0.003:





Φ
(
x
)
≈


1
2



{
1
+
sign
⁡
(
x
)


[
1
−

e

−
2

x

2



/

π


]


1

/

2


}



{\displaystyle \Phi (x)\approx {\frac {1}{2}}\left\{1+\operatorname {sign} (x)\left[1-e^{-2x^{2}/\pi }\right]^{1/2}\right\}}



The inverse is also easily obtained. The inverse normal formula is...






Φ

−
1


(
x
)
≈


{



(
−
1.57079632679
ln
⁡
(
1
−
(
2
x
−
1

)

2


)

)

1

/

2


,


x
≥
0.5
,




(
−
1.57079632679
ln
⁡
(
1
−
(
2
x

)

2


)

)

1

/

2


,



otherwise

.








{\displaystyle \Phi ^{-1}(x)\approx {\begin{cases}(-1.57079632679\ln(1-(2x-1)^{2}))^{1/2},&x\geq 0.5,\\(-1.57079632679\ln(1-(2x)^{2}))^{1/2},&{\text{otherwise}}.\end{cases}}}



Applying the following correction to the distribution's tails...

 if abs(.5-CDF)>.321 then do;
   if CDF>.5 then x= 1.0032*( x)^1.0362;
   else           x=-1.0032*(-x)^1.0362;
 end;

guarantees an absolute relative error < 1.42% in x, for CDF in [.00001,.99999]. All of this is easily confirmed for a range of x of nearly 9 sigma.
Zelen & Severo (1964) give the approximation for Φ(x) for x > 0 with the absolute error | ε(x) | < 7.5·10−8 (algorithm 26.2.17):





Φ
(
x
)
=
1
−
φ
(
x
)

(

b

1


t
+

b

2



t

2


+

b

3



t

3


+

b

4



t

4


+

b

5



t

5


)

+
ε
(
x
)
,

t
=


1

1
+

b

0


x



,


{\displaystyle \Phi (x)=1-\varphi (x)\left(b_{1}t+b_{2}t^{2}+b_{3}t^{3}+b_{4}t^{4}+b_{5}t^{5}\right)+\varepsilon (x),\qquad t={\frac {1}{1+b_{0}x}},}



where ϕ(x) is the standard normal PDF, and b0 = 0.2316419, b1 = 0.319381530, b2 = −0.356563782, b3 = 1.781477937, b4 = −1.821255978, b5 = 1.330274429.
Hart (1968) lists some dozens of approximations - by means of rational functions, with or without exponentials - for the erfc() function. His algorithms vary in the degree of complexity and the resulting precision, with maximum absolute precision of 24 digits. An algorithm by West (2009) combines Hart's algorithm 5666 with a continued fraction approximation in the tail to provide a fast computation algorithm with a 16-digit precision.
Cody (1969) after recalling Hart68 solution is not suited for erf, gives a solution for both erf and erfc, with maximal relative error bound, via Rational Chebyshev Approximation.
Marsaglia (2004) suggested a simple algorithm[nb 1] based on the Taylor series expansion





Φ
(
x
)
=


1
2


+
φ
(
x
)

(
x
+



x

3


3


+



x

5



3
⋅
5



+



x

7



3
⋅
5
⋅
7



+



x

9



3
⋅
5
⋅
7
⋅
9



+
⋯
)



{\displaystyle \Phi (x)={\frac {1}{2}}+\varphi (x)\left(x+{\frac {x^{3}}{3}}+{\frac {x^{5}}{3\cdot 5}}+{\frac {x^{7}}{3\cdot 5\cdot 7}}+{\frac {x^{9}}{3\cdot 5\cdot 7\cdot 9}}+\cdots \right)}



for calculating Φ(x) with arbitrary precision. The drawback of this algorithm is comparatively slow calculation time (for example it takes over 300 iterations to calculate the function with 16 digits of precision when x = 10).
The GNU Scientific Library calculates values of the standard normal CDF using Hart's algorithms and approximations with Chebyshev polynomials.


Some more approximations can be found at: Error function#Approximation with elementary functions.
History[edit]
Development[edit]
Some authors[59][60] attribute the credit for the discovery of the normal distribution to de Moivre, who in 1738[nb 2] published in the second edition of his "The Doctrine of Chances" the study of the coefficients in the binomial expansion of (a + b)n. De Moivre proved that the middle term in this expansion has the approximate magnitude of 



2

/



2
π
n




{\displaystyle 2/{\sqrt {2\pi n}}}

, and that "If m or ½n be a Quantity infinitely great, then the Logarithm of the Ratio, which a Term distant from the middle by the Interval ℓ, has to the middle Term, is 



−



2
ℓ
ℓ

n




{\displaystyle -{\frac {2\ell \ell }{n}}}

."[61] Although this theorem can be interpreted as the first obscure expression for the normal probability law, Stigler points out that de Moivre himself did not interpret his results as anything more than the approximate rule for the binomial coefficients, and in particular de Moivre lacked the concept of the probability density function.[62]




Carl Friedrich Gauss discovered the normal distribution in 1809 as a way to rationalize the method of least squares.


In 1809 Gauss published his monograph "Theoria motus corporum coelestium in sectionibus conicis solem ambientium" where among other things he introduces several important statistical concepts, such as the method of least squares, the method of maximum likelihood, and the normal distribution. Gauss used M, M′, M′′, … to denote the measurements of some unknown quantity V, and sought the "most probable" estimator: the one that maximizes the probability φ(M−V) · φ(M′−V) · φ(M′′−V) · … of obtaining the observed experimental results. In his notation φΔ is the probability law of the measurement errors of magnitude Δ. Not knowing what the function φ is, Gauss requires that his method should reduce to the well-known answer: the arithmetic mean of the measured values.[nb 3] Starting from these principles, Gauss demonstrates that the only law that rationalizes the choice of arithmetic mean as an estimator of the location parameter, is the normal law of errors:[63]





φ


Δ


=


h

√
π





e

−

h
h

Δ
Δ


,


{\displaystyle \varphi {\mathit {\Delta }}={\frac {h}{\surd \pi }}\,e^{-\mathrm {hh} \Delta \Delta },}



where h is "the measure of the precision of the observations". Using this normal law as a generic model for errors in the experiments, Gauss formulates what is now known as the non-linear weighted least squares (NWLS) method.[64]




Marquis de Laplace proved the central limit theorem in 1810, consolidating the importance of the normal distribution in statistics.


Although Gauss was the first to suggest the normal distribution law, Laplace made significant contributions.[nb 4] It was Laplace who first posed the problem of aggregating several observations in 1774,[65] although his own solution led to the Laplacian distribution. It was Laplace who first calculated the value of the integral ∫ e−t ²dt = √π in 1782, providing the normalization constant for the normal distribution.[66] Finally, it was Laplace who in 1810 proved and presented to the Academy the fundamental central limit theorem, which emphasized the theoretical importance of the normal distribution.[67]
It is of interest to note that in 1809 an American mathematician Adrain published two derivations of the normal probability law, simultaneously and independently from Gauss.[68] His works remained largely unnoticed by the scientific community, until in 1871 they were "rediscovered" by Abbe.[69]
In the middle of the 19th century Maxwell demonstrated that the normal distribution is not just a convenient mathematical tool, but may also occur in natural phenomena:[70] "The number of particles whose velocity, resolved in a certain direction, lies between x and x + dx is






N




1

α



π







e

−



x

2



α

2






d
x


{\displaystyle \mathrm {N} \;{\frac {1}{\alpha \;{\sqrt {\pi }}}}\;e^{-{\frac {x^{2}}{\alpha ^{2}}}}dx}



Naming[edit]
Since its introduction, the normal distribution has been known by many different names: the law of error, the law of facility of errors, Laplace's second law, Gaussian law, etc. Gauss himself apparently coined the term with reference to the "normal equations" involved in its applications, with normal having its technical meaning of orthogonal rather than "usual".[71] However, by the end of the 19th century some authors[nb 5] had started using the name normal distribution, where the word "normal" was used as an adjective – the term now being seen as a reflection of the fact that this distribution was seen as typical, common – and thus "normal". Peirce (one of those authors) once defined "normal" thus: "...the 'normal' is not the average (or any other kind of mean) of what actually occurs, but of what would, in the long run, occur under certain circumstances."[72] Around the turn of the 20th century Pearson popularized the term normal as a designation for this distribution.[73]

Many years ago I called the Laplace–Gaussian curve the normal curve, which name, while it avoids an international question of priority, has the disadvantage of leading people to believe that all other distributions of frequency are in one sense or another 'abnormal'.
— Pearson (1920)

Also, it was Pearson who first wrote the distribution in terms of the standard deviation σ as in modern notation. Soon after this, in year 1915, Fisher added the location parameter to the formula for normal distribution, expressing it in the way it is written nowadays:





d
f
=


1

2

σ

2


π




e

−
(
x
−
m

)

2



/

(
2

σ

2


)



d
x


{\displaystyle df={\frac {1}{\sqrt {2\sigma ^{2}\pi }}}e^{-(x-m)^{2}/(2\sigma ^{2})}\,dx}



The term "standard normal", which denotes the normal distribution with zero mean and unit variance came into general use around the 1950s, appearing in the popular textbooks by P.G. Hoel (1947) "Introduction to mathematical statistics" and A.M. Mood (1950) "Introduction to the theory of statistics".[74]
When the name is used, the "Gaussian distribution" was named after Carl Friedrich Gauss, who introduced the distribution in 1809 as a way of rationalizing the method of least squares as outlined above. Among English speakers, both "normal distribution" and "Gaussian distribution" are in common use, with different terms preferred by different communities.
See also[edit]


Statistics portal



Behrens–Fisher problem — the long-standing problem of testing whether two normal samples with different variances have same means;
Bhattacharyya distance – method used to separate mixtures of normal distributions
Erdős–Kac theorem—on the occurrence of the normal distribution in number theory
Gaussian blur—convolution, which uses the normal distribution as a kernel
Sum of normally distributed random variables
Normally distributed and uncorrelated does not imply independent
Tweedie distribution — The normal distribution is a member of the family of Tweedie exponential dispersion models
Z-test— using the normal distribution
Rayleigh distribution
Multivariate normal distribution — a generalization of the normal distribution in multiple dimensions
Sub-Gaussian distribution

Notes[edit]


^ For example, this algorithm is given in the article Bc programming language.
^ De Moivre first published his findings in 1733, in a pamphlet "Approximatio ad Summam Terminorum Binomii (a + b)n in Seriem Expansi" that was designated for private circulation only. But it was not until the year 1738 that he made his results publicly available. The original pamphlet was reprinted several times, see for example Walker (1985).
^ "It has been customary certainly to regard as an axiom the hypothesis that if any quantity has been determined by several direct observations, made under the same circumstances and with equal care, the arithmetical mean of the observed values affords the most probable value, if not rigorously, yet very nearly at least, so that it is always most safe to adhere to it." — Gauss (1809, section 177)
^ "My custom of terming the curve the Gauss–Laplacian or normal curve saves us from proportioning the merit of discovery between the two great astronomer mathematicians." quote from Pearson (1905, p. 189)
^ Besides those specifically referenced here, such use is encountered in the works of Peirce, Galton (Galton (1889, chapter V)) and Lexis (Lexis (1878), Rohrbasser & Véron (2003)) c. 1875.[citation needed]


Citations[edit]


^ Normal Distribution, Gale Encyclopedia of Psychology
^ Casella & Berger (2001, p. 102)
^ Lyon, A. (2014). Why are Normal Distributions Normal?, The British Journal for the Philosophy of Science.
^ For the proof see Gaussian integral
^ Stigler (1982)
^ Halperin, Hartley & Hoel (1965, item 7)
^ McPherson (1990, p. 110)
^ Bernardo & Smith (2000), p. 121
^ Cover, Thomas M.; Thomas, Joy A. (2006). Elements of Information Theory. John Wiley and Sons. p. 254. 
^ Park, Sung Y.; Bera, Anil K. (2009). "Maximum Entropy Autoregressive Conditional Heteroskedasticity Model" (PDF). Journal of Econometrics. Elsevier. 150 (2): 219–230. doi:10.1016/j.jeconom.2008.12.014. Retrieved 2011-06-02. 
^ a b c Patel & Read (1996, [2.1.4])
^ Fan (1991, p. 1258)
^ Patel & Read (1996, [2.1.8])
^ Papoulis, Athanasios. Probability, Random Variables and Stochastic Processes. p. 148. 
^ Bryc (1995, p. 23)
^ Bryc (1995, p. 24)
^ Scott, Clayton; Nowak, Robert (August 7, 2003). "The Q-function". Connexions. 
^ Barak, Ohad (April 6, 2006). "Q Function and Error Function" (PDF). Tel Aviv University. 
^ Weisstein, Eric W. "Normal Distribution Function". MathWorld. 
^ "Wolfram|Alpha: Computational Knowledge Engine". Wolframalpha.com. Retrieved 2017-03-03. 
^ http://www.wolframalpha.com/input/?i=Table%5BSqrt%282%29*InverseErf%28x%29%2C+{x%2C+N%28{8%2F10%2C+9%2F10%2C+19%2F20%2C+49%2F50%2C+99%2F100%2C+995%2F1000%2C+998%2F1000}%2C+13%29}%5D
^ "Wolfram|Alpha: Computational Knowledge Engine". Wolframalpha.com. Retrieved 2017-03-03. 
^ "Normal Approximation to Poisson Distribution". Stat.ucla.edu. Retrieved 2017-03-03. 
^ Cover & Thomas (2006, p. 254)
^ Williams, D. (2001) Weighing the Odds Cambridge UP ISBN 0-521-00618-X (pages 197-199)
^ Bernardo, J.M., Smith, A.F.M. (2000) Bayesian Theory'.' Wiley. ISBN 0-471-49464-X (pages 209, 366)
^ O'Hagan, A. (1994) Kendall's Advanced Theory of statistics, Vol 2B, Bayesian Inference, Edward Arnold. ISBN 0-340-52922-9 (Section 5.40)
^ Bryc (1995, p. 27)
^ Patel & Read (1996, [2.3.6])
^ Galambos & Simonelli (2004, Theorem 3.5)
^ a b Bryc (1995, p. 35)
^ a b Lukacs & King (1954)
^ Quine, M.P. (1993). "On three characterisations of the normal distribution". Probability and Mathematical Statistics. 14 (2): 257–263. 
^ UIUC, Lecture 21. The Multivariate Normal Distribution, 21.6:"Individually Gaussian Versus Jointly Gaussian".
^ Edward L. Melnick and Aaron Tenenbein, "Misspecifications of the Normal Distribution", The American Statistician, volume 36, number 4 November 1982, pages 372–373
^ "Kullback Leibler (KL) Distance of Two Normal (Gaussian) Probability Distributions". Allisons.org. 2007-12-05. Retrieved 2017-03-03. 
^ Jordan, Michael I. (February 8, 2010). "Stat260: Bayesian Modeling and Inference: The Conjugate Prior for the Normal Distribution" (PDF). 
^ Amari & Nagaoka (2000)
^ Normal Product Distribution, Mathworld.wolfram.com
^ Lukacs, Eugene (1942). "A Characterization of the Normal Distribution". The Annals of Mathematical Statistics. Institute of Mathematical Statistics. 13 (1): 91–3. doi:10.1214/aoms/1177731647. ISSN 0003-4851. JSTOR 2236166 – via JSTOR. (registration required (help)). 
^ Basu, D.; Laha, R. G. (1954). "On Some Characterizations of the Normal Distribution". Sankhyā. Indian Statistical Institute. 13 (4): 359–62. ISSN 0036-4452. JSTOR 25048183 – via JSTOR. (registration required (help)). 
^ Lehmann, E. L. (1997). Testing Statistical Hypotheses (2nd ed.). Springer. p. 199. ISBN 0-387-94919-4. 
^ John, S (1982). "The three parameter two-piece normal family of distributions and its fitting". Communications in statistics - Theory and Methods. 11 (8): 879–885. doi:10.1080/03610928208828279. 
^ a b Krishnamoorthy (2006, p. 127)
^ Krishnamoorthy (2006, p. 130)
^ Krishnamoorthy (2006, p. 133)
^ Huxley (1932)
^ Jaynes, Edwin T. (2003). Probability Theory: The Logic of Science. Cambridge University Press. pp. 592–593. 
^ Oosterbaan, Roland J. (1994). "Chapter 6: Frequency and Regression Analysis of Hydrologic Data". In Ritzema, Henk P. Drainage Principles and Applications, Publication 16 (PDF) (second revised ed.). Wageningen, The Netherlands: International Institute for Land Reclamation and Improvement (ILRI). pp. 175–224. ISBN 90-70754-33-9. 
^ Wichura, Michael J. (1988). "Algorithm AS241: The Percentage Points of the Normal Distribution". Applied Statistics. Blackwell Publishing. 37 (3): 477–84. doi:10.2307/2347330. JSTOR 2347330 – via JSTOR. (registration required (help)). 
^ Johnson, Kotz & Balakrishnan (1995, Equation (26.48))
^ Kinderman & Monahan (1977)
^ Leva (1992)
^ Marsaglia & Tsang (2000)
^ Karney (2016)
^ Monahan (1985, section 2)
^ Wallace (1996)
^ Bell, Jeff (2015). "A Simple and Pragmatic Approximation to the Normal Cumulative Probability Distribution". SSRN Electronic Journal. doi:10.2139/ssrn.2579686. 
^ Johnson, Kotz & Balakrishnan (1994, p. 85)
^ Le Cam & Lo Yang (2000, p. 74)
^ De Moivre, Abraham (1733), Corollary I – see Walker (1985, p. 77)
^ Stigler (1986, p. 76)
^ Gauss (1809, section 177)
^ Gauss (1809, section 179)
^ Laplace (1774, Problem III)
^ Pearson (1905, p. 189)
^ Stigler (1986, p. 144)
^ Stigler (1978, p. 243)
^ Stigler (1978, p. 244)
^ Maxwell (1860, p. 23)
^ Jaynes, Edwin J.; Probability Theory: The Logic of Science, Ch 7
^ Peirce, Charles S. (c. 1909 MS), Collected Papers v. 6, paragraph 327
^ Kruskal & Stigler (1997)
^ "Earliest uses… (entry STANDARD NORMAL CURVE)". 


References[edit]


Aldrich, John; Miller, Jeff. "Earliest Uses of Symbols in Probability and Statistics". 
Aldrich, John; Miller, Jeff. "Earliest Known Uses of Some of the Words of Mathematics".  In particular, the entries for "bell-shaped and bell curve", "normal (distribution)", "Gaussian", and "Error, law of error, theory of errors, etc.".
Amari, Shun-ichi; Nagaoka, Hiroshi (2000). Methods of Information Geometry. Oxford University Press. ISBN 0-8218-0531-2. 
Bernardo, José M.; Smith, Adrian F. M. (2000). Bayesian Theory. Wiley. ISBN 0-471-49464-X. 
Bryc, Wlodzimierz (1995). The Normal Distribution: Characterizations with Applications. Springer-Verlag. ISBN 0-387-97990-5. 
Casella, George; Berger, Roger L. (2001). Statistical Inference (2nd ed.). Duxbury. ISBN 0-534-24312-6. 
Cody, William J. (1969). "Rational Chebyshev Approximations for the Error Function". Mathematics of Computation. 23 (107): 631–638. doi:10.1090/S0025-5718-1969-0247736-4. 
Cover, Thomas M.; Thomas, Joy A. (2006). Elements of Information Theory. John Wiley and Sons. 
de Moivre, Abraham (1738). The Doctrine of Chances. ISBN 0-8218-2103-2. 
Fan, Jianqing (1991). "On the optimal rates of convergence for nonparametric deconvolution problems". The Annals of Statistics. 19 (3): 1257–1272. doi:10.1214/aos/1176348248. JSTOR 2241949. 
Galton, Francis (1889). Natural Inheritance (PDF). London, UK: Richard Clay and Sons. 
Galambos, Janos; Simonelli, Italo (2004). Products of Random Variables: Applications to Problems of Physics and to Arithmetical Functions. Marcel Dekker, Inc. ISBN 0-8247-5402-6. 
Gauss, Carolo Friderico (1809). Theoria motvs corporvm coelestivm in sectionibvs conicis Solem ambientivm [Theory of the Motion of the Heavenly Bodies Moving about the Sun in Conic Sections] (in Latin). English translation. 
Gould, Stephen Jay (1981). The Mismeasure of Man (first ed.). W. W. Norton. ISBN 0-393-01489-4. 
Halperin, Max; Hartley, Herman O.; Hoel, Paul G. (1965). "Recommended Standards for Statistical Symbols and Notation. COPSS Committee on Symbols and Notation". The American Statistician. 19 (3): 12–14. doi:10.2307/2681417. JSTOR 2681417. 
Hart, John F.; et al. (1968). Computer Approximations. New York, NY: John Wiley & Sons, Inc. ISBN 0-88275-642-7. 
Hazewinkel, Michiel, ed. (2001), "Normal Distribution", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Herrnstein, Richard J.; Murray, Charles (1994). The Bell Curve: Intelligence and Class Structure in American Life. Free Press. ISBN 0-02-914673-9. 
Huxley, Julian S. (1932). Problems of Relative Growth. London. ISBN 0-486-61114-0. OCLC 476909537. 
Johnson, Norman L.; Kotz, Samuel; Balakrishnan, Narayanaswamy (1994). Continuous Univariate Distributions, Volume 1. Wiley. ISBN 0-471-58495-9. 
Johnson, Norman L.; Kotz, Samuel; Balakrishnan, Narayanaswamy (1995). Continuous Univariate Distributions, Volume 2. Wiley. ISBN 0-471-58494-0. 
Karney, C. f. f. (2016). "Sampling exactly from the normal distribution". ACM Transactions on Mathematical Software. 42 (1): 3:1–14. arXiv:1303.6257. doi:10.1145/2710016. 
Kinderman, Albert J.; Monahan, John F. (1977). "Computer Generation of Random Variables Using the Ratio of Uniform Deviates". ACM Transactions on Mathematical Software. 3 (3): 257–260. doi:10.1145/355744.355750. 
Krishnamoorthy, Kalimuthu (2006). Handbook of Statistical Distributions with Applications. Chapman & Hall/CRC. ISBN 1-58488-635-8. 
Kruskal, William H.; Stigler, Stephen M. (1997). Spencer, Bruce D., ed. Normative Terminology: 'Normal' in Statistics and Elsewhere. Statistics and Public Policy. Oxford University Press. ISBN 0-19-852341-6. 
Laplace, Pierre-Simon de (1774). "Mémoire sur la probabilité des causes par les événements". Mémoires de l'Académie royale des Sciences de Paris (Savants étrangers), tome 6: 621–656.  Translated by Stephen M. Stigler in Statistical Science 1 (3), 1986: JSTOR 2245476.
Laplace, Pierre-Simon (1812). Théorie analytique des probabilités [Analytical theory of probabilities]. 
Le Cam, Lucien; Lo Yang, Grace (2000). Asymptotics in Statistics: Some Basic Concepts (second ed.). Springer. ISBN 0-387-95036-2. 
Leva, Joseph L. (1992). "A fast normal random number generator". ACM Transactions on Mathematical Software. 18 (4): 449–453. doi:10.1145/138351.138364. 
Lexis, Wilhelm (1878). "Sur la durée normale de la vie humaine et sur la théorie de la stabilité des rapports statistiques". Annales de démographie internationale. Paris. II: 447–462. 
Lukacs, Eugene; King, Edgar P. (1954). "A Property of Normal Distribution". The Annals of Mathematical Statistics. 25 (2): 389–394. doi:10.1214/aoms/1177728796. JSTOR 2236741. 
McPherson, Glen (1990). Statistics in Scientific Investigation: Its Basis, Application and Interpretation. Springer-Verlag. ISBN 0-387-97137-8. 
Marsaglia, George; Tsang, Wai Wan (2000). "The Ziggurat Method for Generating Random Variables". Journal of Statistical Software. 5 (8). doi:10.18637/jss.v005.i08. 
Marsaglia, George (2004). "Evaluating the Normal Distribution". Journal of Statistical Software. 11 (4). doi:10.18637/jss.v011.i04. 
Maxwell, James Clerk (1860). "V. Illustrations of the dynamical theory of gases. — Part I: On the motions and collisions of perfectly elastic spheres". Philosophical Magazine, series 4. 19 (124): 19–32. doi:10.1080/14786446008642818. 
Monahan, J. F. (1985). "Accuracy in random number generation". Mathematics of Computation. 45 (172): 559–568. doi:10.1090/S0025-5718-1985-0804945-X. 
Patel, Jagdish K.; Read, Campbell B. (1996). Handbook of the Normal Distribution (2nd ed.). CRC Press. ISBN 0-8247-9342-0. 
Pearson, Karl (1901). "On Lines and Planes of Closest Fit to Systems of Points in Space" (PDF). Philosophical Magazine. 6. 2: 559–572. 
Pearson, Karl (1905). "'Das Fehlergesetz und seine Verallgemeinerungen durch Fechner und Pearson'. A rejoinder". Biometrika. 4 (1): 169–212. doi:10.2307/2331536. JSTOR 2331536. 
Pearson, Karl (1920). "Notes on the History of Correlation". Biometrika. 13 (1): 25–45. doi:10.1093/biomet/13.1.25. JSTOR 2331722. 
Rohrbasser, Jean-Marc; Véron, Jacques (2003). "Wilhelm Lexis: The Normal Length of Life as an Expression of the "Nature of Things"". Population. 58 (3): 303–322. doi:10.3917/pope.303.0303. 
Stigler, Stephen M. (1978). "Mathematical Statistics in the Early States". The Annals of Statistics. 6 (2): 239–265. doi:10.1214/aos/1176344123. JSTOR 2958876. 
Stigler, Stephen M. (1982). "A Modest Proposal: A New Standard for the Normal". The American Statistician. 36 (2): 137–138. doi:10.2307/2684031. JSTOR 2684031. 
Stigler, Stephen M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Harvard University Press. ISBN 0-674-40340-1. 
Stigler, Stephen M. (1999). Statistics on the Table. Harvard University Press. ISBN 0-674-83601-4. 
Walker, Helen M. (1985). "De Moivre on the Law of Normal Probability" (PDF). In Smith, David Eugene. A Source Book in Mathematics. Dover. ISBN 0-486-64690-4. 
Wallace, C. S. (1996). "Fast pseudo-random generators for normal and exponential variates". ACM Transactions on Mathematical Software. 22 (1): 119–127. doi:10.1145/225545.225554. 
Weisstein, Eric W. "Normal Distribution". MathWorld. 
West, Graeme (2009). "Better Approximations to Cumulative Normal Functions" (PDF). Wilmott Magazine: 70–76. 
Zelen, Marvin; Severo, Norman C. (1964). Probability Functions (chapter 26). Handbook of mathematical functions with formulas, graphs, and mathematical tables, by Abramowitz, M.; and Stegun, I. A.: National Bureau of Standards. New York, NY: Dover. ISBN 0-486-61272-4. 


External links[edit]



Wikimedia Commons has media related to Normal distribution.



Hazewinkel, Michiel, ed. (2001), "Normal distribution", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Normal Distribution Video Tutorial Part 1-2 on YouTube
Normal distribution calculator
An 8-foot-tall (2.4 m) Probability Machine (named Sir Francis) comparing stock market returns to the randomness of the beans dropping through the quincunx pattern. on YouTube Link originating from Index Fund Advisors







v
t
e


Some common univariate probability distributions






Continuous



beta
Cauchy
chi-squared
exponential
F
gamma
Laplace
log-normal
normal
Pareto
Student's t
uniform
Weibull








Discrete



Bernoulli
binomial
discrete uniform
geometric
hypergeometric
negative binomial
Poisson









List of probability distributions










v
t
e


Probability distributions







List






Discrete univariate
with finite support



Benford
Bernoulli
beta-binomial
binomial
categorical
hypergeometric
Poisson binomial
Rademacher
discrete uniform
Zipf
Zipf–Mandelbrot








Discrete univariate
with infinite support



beta negative binomial
Borel
Conway–Maxwell–Poisson
discrete phase-type
Delaporte
extended negative binomial
Gauss–Kuzmin
geometric
logarithmic
negative binomial
parabolic fractal
Poisson
Skellam
Yule–Simon
zeta








Continuous univariate
supported on a bounded interval



arcsine
ARGUS
Balding–Nichols
Bates
beta
beta rectangular
Irwin–Hall
Kumaraswamy
logit-normal
noncentral beta
raised cosine
reciprocal
triangular
U-quadratic
uniform
Wigner semicircle








Continuous univariate
supported on a semi-infinite interval



Benini
Benktander 1st kind
Benktander 2nd kind
beta prime
Burr
chi-squared
chi
Dagum
Davis
exponential-logarithmic
Erlang
exponential
F
folded normal
Flory–Schulz
Fréchet
gamma
gamma/Gompertz
generalized inverse Gaussian
Gompertz
half-logistic
half-normal
Hotelling's T-squared
hyper-Erlang
hyperexponential
hypoexponential
inverse chi-squared

scaled inverse chi-squared


inverse Gaussian
inverse gamma
Kolmogorov
Lévy
log-Cauchy
log-Laplace
log-logistic
log-normal
Lomax
matrix-exponential
Maxwell–Boltzmann
Maxwell–Jüttner
Mittag-Leffler
Nakagami
noncentral chi-squared
Pareto
phase-type
poly-Weibull
Rayleigh
relativistic Breit–Wigner
Rice
shifted Gompertz
truncated normal
type-2 Gumbel
Weibull

Discrete Weibull


Wilks's lambda








Continuous univariate
supported on the whole real line



Cauchy
exponential power
Fisher's z
Gaussian q
generalized normal
generalized hyperbolic
geometric stable
Gumbel
Holtsmark
hyperbolic secant
Johnson's SU
Landau
Laplace
asymmetric Laplace
logistic
noncentral t
normal (Gaussian)
normal-inverse Gaussian
skew normal
slash
stable
Student's t
type-1 Gumbel
Tracy–Widom
variance-gamma
Voigt








Continuous univariate
with support whose type varies



generalized extreme value
generalized Pareto
Marchenko–Pastur
q-exponential
q-Gaussian
q-Weibull
shifted log-logistic
Tukey lambda








Mixed continuous-discrete univariate



rectified Gaussian








Multivariate (joint)



Discrete
Ewens
multinomial
Dirichlet-multinomial
negative multinomial
Continuous
Dirichlet
generalized Dirichlet
multivariate normal
multivariate stable
multivariate t
normal-inverse-gamma
normal-gamma
Matrix-valued
inverse matrix gamma
inverse-Wishart
matrix normal
matrix t
matrix gamma
normal-inverse-Wishart
normal-Wishart
Wishart








Directional



Univariate (circular) directional
Circular uniform
univariate von Mises
wrapped normal
wrapped Cauchy
wrapped exponential
wrapped asymmetric Laplace
wrapped Lévy
Bivariate (spherical)
Kent
Bivariate (toroidal)
bivariate von Mises
Multivariate
von Mises–Fisher
Bingham








Degenerate and singular



Degenerate
Dirac delta function
Singular
Cantor








Families



Circular
compound Poisson
elliptical
exponential
natural exponential
location–scale
maximum entropy
mixture
Pearson
Tweedie
wrapped









 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Normal_distribution&oldid=774515637"					
Categories: Continuous distributionsConjugate prior distributionsNormal distributionExponential family distributionsStable distributionsLocation-scale family probability distributionsHidden categories: All articles with unsourced statementsArticles with unsourced statements from June 2011Pages with login required references or sourcesPages using ISBN magic linksUse mdy dates from August 2012Pages using deprecated image syntaxArticles with unsourced statements from June 2010CS1 Latin-language sources (la)Articles with example Pascal code 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


AlemannischالعربيةAzərbaycancaتۆرکجهBân-lâm-gúБеларускаяБългарскиBosanskiCatalàČeštinaCymraegDanskDeutschEestiΕλληνικάEspañolEsperantoEuskaraفارسیFrançaisGalego한국어हिन्दीHrvatskiBahasa IndonesiaÍslenskaItalianoעבריתქართულიҚазақшаLatinaLatviešuLietuviųMagyarमराठीNederlands日本語Norsk bokmålNorsk nynorskPiemontèisPolskiPortuguêsRomânăРусскийScotsSimple EnglishSlovenčinaSlovenščinaСрпски / srpskiSrpskohrvatski / српскохрватскиBasa SundaSuomiSvenskaTagalogதமிழ்Татарча/tatarçaไทยTürkçeУкраїнськаاردوTiếng Việtייִדיש中文 
Edit links 





 This page was last modified on 9 April 2017, at 00:22.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 



(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"1.176","walltime":"1.738","ppvisitednodes":{"value":10547,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":236363,"limit":2097152},"templateargumentsize":{"value":9585,"limit":2097152},"expansiondepth":{"value":12,"limit":40},"expensivefunctioncount":{"value":4,"limit":500},"entityaccesscount":{"value":1,"limit":400},"timingprofile":["100.00%  770.310      1 -total"," 29.34%  226.042      2 Template:Reflist"," 18.17%  139.974     30 Template:Cite_book"," 13.79%  106.244     28 Template:Cite_journal","  9.46%   72.865     55 Template:Harvtxt","  8.53%   65.686     55 Template:Harvard_citation/core","  7.86%   60.548     32 Template:Val","  5.25%   40.415     12 Template:Cite_web","  4.99%   38.418      2 Template:Citation_needed","  4.58%   35.293      2 Template:Fix"]},"scribunto":{"limitreport-timeusage":{"value":"0.410","limit":"10.000"},"limitreport-memusage":{"value":10162286,"limit":52428800}},"cachereport":{"origin":"mw1306","timestamp":"20170409002224","ttl":2592000,"transientcontent":false}}});});(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":69,"wgHostname":"mw1241"});});








Gamma distribution - Wikipedia
document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );
(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Gamma_distribution","wgTitle":"Gamma distribution","wgCurRevisionId":774709059,"wgRevisionId":774709059,"wgArticleId":207079,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Gamma_distribution","wgRelevantArticleId":207079,"wgRequestId":"WOsbQApAMFoAAG8WamMAAAAK","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":false,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgMFMode":"stable","wgMFLazyLoadImages":true,"wgMFLazyLoadReferences":false,"wgPreferredVariant":"en","wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesBetaFeatureEnabled":false,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q117806","wgCentralAuthMobileDomain":true,"wgVisualEditorToolbarScrollOffset":0,"wgEditSubmitButtonLabelPublish":false,"wgMinervaMenuData":{"groups":[[{"name":"home","components":[{"text":"Home","href":"/wiki/Main_Page","class":"mw-ui-icon mw-ui-icon-before mw-ui-icon-mf-home ","data-event-name":"home"}]},{"name":"random","components":[{"text":"Random","href":"/wiki/Special:Random/#/random","class":"mw-ui-icon mw-ui-icon-before mw-ui-icon-mf-random ","id":"randomButton","data-event-name":"random"}]},{"name":"nearby","components":[{"text":"Nearby","href":"/wiki/Special:Nearby","class":"mw-ui-icon mw-ui-icon-before mw-ui-icon-mf-nearby nearby","data-event-name":"nearby"}],"class":"jsonly"}],[{"name":"auth","components":[{"text":"Log in","href":"/w/index.php?title=Special:UserLogin\u0026returnto=Gamma+distribution\u0026returntoquery=welcome%3Dyes","class":"mw-ui-icon mw-ui-icon-before mw-ui-icon-mf-anonymous ","data-event-name":"login"}],"class":"jsonly"}],[{"name":"settings","components":[{"text":"Settings","href":"/w/index.php?title=Special:MobileOptions\u0026returnto=Gamma+distribution","class":"mw-ui-icon mw-ui-icon-before mw-ui-icon-mf-settings ","data-event-name":"settings"}]}]],"sitelinks":[{"name":"about","components":[{"text":"About Wikipedia","href":"/wiki/Wikipedia:About","class":""}]},{"name":"disclaimers","components":[{"text":"Disclaimers","href":"/wiki/Wikipedia:General_disclaimer","class":""}]}]},"wgMinervaTocEnabled":true,"wgMFDescription":null});mw.loader.state({"mobile.usermodule.styles":"ready","user.styles":"ready","user":"ready","user.options":"loading","user.tokens":"loading","ext.math.styles":"ready","ext.cite.styles":"ready","skins.minerva.base.reset":"ready","skins.minerva.base.styles":"ready","skins.minerva.content.styles":"ready","skins.minerva.tablet.styles":"ready","mediawiki.ui.icon":"ready","mediawiki.ui.button":"ready","skins.minerva.icons.images":"ready","mobile.usermodule":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["mediawiki.toc","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","skins.minerva.scripts.top","skins.minerva.scripts","skins.minerva.watchstar","skins.minerva.editor","skins.minerva.toggling","mobile.site","ext.gadget.switcher","ext.centralauth.centralautologin","ext.visualEditor.targetLoader","ext.relatedArticles.readMore.bootstrap","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"]);});































Open main menu



β









Edit this pageRead in another languageGamma distributionfunction mfTempOpenSection(id){var block=document.getElementById("mf-section-"+id);block.className+=" open-block";block.previousSibling.className+=" open-block";}Not to be confused with Gamma function.



This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (September 2012) (Learn how and when to remove this template message)
Gamma
Probability density function
Cumulative distribution function
Parameters

k > 0 shape
θ > 0 scale


α > 0 shape
β > 0 rate

Support





x

∈

(
0
,

∞
)



{\displaystyle \scriptstyle x\;\in \;(0,\,\infty )}






x

∈

(
0
,

∞
)



{\displaystyle \scriptstyle x\;\in \;(0,\,\infty )}

PDF






1

Γ
(
k
)

θ

k






x

k

−

1



e

−


x
θ






{\displaystyle {\frac {1}{\Gamma (k)\theta ^{k}}}x^{k\,-\,1}e^{-{\frac {x}{\theta }}}}








β

α



Γ
(
α
)




x

α

−

1



e

−
β
x




{\displaystyle {\frac {\beta ^{\alpha }}{\Gamma (\alpha )}}x^{\alpha \,-\,1}e^{-\beta x}}
[1]
CDF






1

Γ
(
k
)



γ

(
k
,



x
θ


)



{\displaystyle {\frac {1}{\Gamma (k)}}\gamma \left(k,\,{\frac {x}{\theta }}\right)}







1

Γ
(
α
)



γ
(
α
,

β
x
)


{\displaystyle {\frac {1}{\Gamma (\alpha )}}\gamma (\alpha ,\,\beta x)}

Mean






E

[
X
]
=
k
θ



{\displaystyle \scriptstyle \mathbf {E} [X]=k\theta }






E

[
ln
⁡
X
]
=
ψ
(
k
)
+
ln
⁡
(
θ
)



{\displaystyle \scriptstyle \mathbf {E} [\ln X]=\psi (k)+\ln(\theta )}

(see digamma function)






E

[
X
]
=


α
β





{\displaystyle \scriptstyle \mathbf {E} [X]={\frac {\alpha }{\beta }}}






E

[
ln
⁡
X
]
=
ψ
(
α
)
−
ln
⁡
(
β
)



{\displaystyle \scriptstyle \mathbf {E} [\ln X]=\psi (\alpha )-\ln(\beta )}

(see digamma function)
Median
No simple closed form
No simple closed form
Mode





(
k

−

1
)
θ

 for 

k


≥


1



{\displaystyle \scriptstyle (k\,-\,1)\theta {\text{ for }}k\;{\geq }\;1}









α

−

1

β



 for 

α


≥


1



{\displaystyle \scriptstyle {\frac {\alpha \,-\,1}{\beta }}{\text{ for }}\alpha \;{\geq }\;1}

Variance





Var
⁡
[
X
]
=
k

θ

2





{\displaystyle \scriptstyle \operatorname {Var} [X]=k\theta ^{2}}





Var
⁡
[
ln
⁡
X
]
=

ψ

1


(
k
)



{\displaystyle \scriptstyle \operatorname {Var} [\ln X]=\psi _{1}(k)}

(see trigamma function)





Var
⁡
[
X
]
=


α

β

2







{\displaystyle \scriptstyle \operatorname {Var} [X]={\frac {\alpha }{\beta ^{2}}}}





Var
⁡
[
ln
⁡
X
]
=

ψ

1


(
α
)



{\displaystyle \scriptstyle \operatorname {Var} [\ln X]=\psi _{1}(\alpha )}

(see trigamma function)
Skewness







2

k






{\displaystyle \scriptstyle {\frac {2}{\sqrt {k}}}}








2

α






{\displaystyle \scriptstyle {\frac {2}{\sqrt {\alpha }}}}

Excess kurtosis







6
k





{\displaystyle \scriptstyle {\frac {6}{k}}}








6
α





{\displaystyle \scriptstyle {\frac {6}{\alpha }}}

Entropy










k





+

ln
⁡
θ

+

ln
⁡
[
Γ
(
k
)
]










+

(
1

−

k
)
ψ
(
k
)








{\displaystyle \scriptstyle {\begin{aligned}\scriptstyle k&\scriptstyle \,+\,\ln \theta \,+\,\ln[\Gamma (k)]\\\scriptstyle &\scriptstyle \,+\,(1\,-\,k)\psi (k)\end{aligned}}}











α





−

ln
⁡
β

+

ln
⁡
[
Γ
(
α
)
]










+

(
1

−

α
)
ψ
(
α
)








{\displaystyle \scriptstyle {\begin{aligned}\scriptstyle \alpha &\scriptstyle \,-\,\ln \beta \,+\,\ln[\Gamma (\alpha )]\\\scriptstyle &\scriptstyle \,+\,(1\,-\,\alpha )\psi (\alpha )\end{aligned}}}

MGF





(
1

−

θ
t

)

−
k



 for 

t

<



1
θ





{\displaystyle \scriptstyle (1\,-\,\theta t)^{-k}{\text{ for }}t\;<\;{\frac {1}{\theta }}}








(
1

−



t
β


)


−
α



 for 

t

<

β



{\displaystyle \scriptstyle \left(1\,-\,{\frac {t}{\beta }}\right)^{-\alpha }{\text{ for }}t\;<\;\beta }

CF





(
1

−

θ
i

t

)

−
k





{\displaystyle \scriptstyle (1\,-\,\theta i\,t)^{-k}}








(
1

−




i

t

β


)


−
α





{\displaystyle \scriptstyle \left(1\,-\,{\frac {i\,t}{\beta }}\right)^{-\alpha }}

In probability theory and statistics, the gamma distribution is a two-parameter family of continuous probability distributions. The common exponential distribution and chi-squared distribution are special cases of the gamma distribution. There are three different parametrizations in common use:
With a shape parameter k and a scale parameter θ.
With a shape parameter α = k and an inverse scale parameter β = 1/θ, called a rate parameter.
With a shape parameter k and a mean parameter μ = k/β.
In each of these three forms, both parameters are positive real numbers.
The gamma distribution is the maximum entropy probability distribution for a random variable X for which E[X] = kθ = α/β is fixed and greater than zero, and E[ln(X)] = ψ(k) + ln(θ) = ψ(α) − ln(β) is fixed (ψ is the digamma function).[2]

ContentsParameterizationsEdit
The parameterization with k and θ appears to be more common in econometrics and certain other applied fields, where e.g. the gamma distribution is frequently used to model waiting times. For instance, in life testing, the waiting time until death is a random variable that is frequently modeled with a gamma distribution.[3]
The parameterization with α and β is more common in Bayesian statistics, where the gamma distribution is used as a conjugate prior distribution for various types of inverse scale (aka rate) parameters, such as the λ of an exponential distribution or a Poisson distribution[4] – or for that matter, the β of the gamma distribution itself. (The closely related inverse gamma distribution is used as a conjugate prior for scale parameters, such as the variance of a normal distribution.)
If k is a positive integer, then the distribution represents an Erlang distribution; i.e., the sum of k independent exponentially distributed random variables, each of which has a mean of θ.
Characterization using shape α and rate βEdit
The gamma distribution can be parameterized in terms of a shape parameter α = k and an inverse scale parameter β = 1/θ, called a rate parameter. A random variable X that is gamma-distributed with shape α and rate β is denoted




X
∼
Γ
(
α
,
β
)
≡


Gamma


(
α
,
β
)


{\displaystyle X\sim \Gamma (\alpha ,\beta )\equiv {\textrm {Gamma}}(\alpha ,\beta )}
 
The corresponding probability density function in the shape-rate parametrization is




f
(
x
;
α
,
β
)
=




β

α



x

α
−
1



e

−
β
x




Γ
(
α
)





 for 

x
>
0

 and 

α
,
β
>
0


{\displaystyle f(x;\alpha ,\beta )={\frac {\beta ^{\alpha }x^{\alpha -1}e^{-\beta x}}{\Gamma (\alpha )}}\quad {\text{ for }}x>0{\text{ and }}\alpha ,\beta >0}
 ,
where 




Γ
(
α
)



{\displaystyle {\Gamma (\alpha )}}
  is a complete gamma function.

Both parametrizations are common because either can be more convenient depending on the situation.
The cumulative distribution function is the regularized gamma function:




F
(
x
;
α
,
β
)
=

∫

0


x


f
(
u
;
α
,
β
)

d
u
=



γ
(
α
,
β
x
)


Γ
(
α
)





{\displaystyle F(x;\alpha ,\beta )=\int _{0}^{x}f(u;\alpha ,\beta )\,du={\frac {\gamma (\alpha ,\beta x)}{\Gamma (\alpha )}}}
 
where 



γ
(
α
,
β
x
)


{\displaystyle \gamma (\alpha ,\beta x)}
  is the lower incomplete gamma function.
If α is a positive integer (i.e., the distribution is an Erlang distribution), the cumulative distribution function has the following series expansion:[5]




F
(
x
;
α
,
β
)
=
1
−

∑

i
=
0


α
−
1





(
β
x

)

i




i
!




e

−
β
x


=

e

−
β
x



∑

i
=
α


∞





(
β
x

)

i




i
!





{\displaystyle F(x;\alpha ,\beta )=1-\sum _{i=0}^{\alpha -1}{\frac {(\beta x)^{i}}{i!}}e^{-\beta x}=e^{-\beta x}\sum _{i=\alpha }^{\infty }{\frac {(\beta x)^{i}}{i!}}}
 
Characterization using shape k and scale θEdit
A random variable X that is gamma-distributed with shape k and scale θ is denoted by




X
∼
Γ
(
k
,
θ
)
≡


Gamma


(
k
,
θ
)


{\displaystyle X\sim \Gamma (k,\theta )\equiv {\textrm {Gamma}}(k,\theta )}
 

 


Illustration of the gamma PDF for parameter values over k and x with θ set to 1, 2, 3, 4, 5 and 6. One can see each θ layer by itself here [2] as well as by k [3] and x. [4].


The probability density function using the shape-scale parametrization is




f
(
x
;
k
,
θ
)
=




x

k
−
1



e

−


x
θ







θ

k


Γ
(
k
)





 for 

x
>
0

 and 

k
,
θ
>
0.


{\displaystyle f(x;k,\theta )={\frac {x^{k-1}e^{-{\frac {x}{\theta }}}}{\theta ^{k}\Gamma (k)}}\quad {\text{ for }}x>0{\text{ and }}k,\theta >0.}
 
Here Γ(k) is the gamma function evaluated at k.
The cumulative distribution function is the regularized gamma function:




F
(
x
;
k
,
θ
)
=

∫

0


x


f
(
u
;
k
,
θ
)

d
u
=



γ

(
k
,


x
θ


)



Γ
(
k
)





{\displaystyle F(x;k,\theta )=\int _{0}^{x}f(u;k,\theta )\,du={\frac {\gamma \left(k,{\frac {x}{\theta }}\right)}{\Gamma (k)}}}
 
where 



γ

(
k
,


x
θ


)



{\displaystyle \gamma \left(k,{\frac {x}{\theta }}\right)}
  is the lower incomplete gamma function.
It can also be expressed as follows, if k is a positive integer (i.e., the distribution is an Erlang distribution):[5]




F
(
x
;
k
,
θ
)
=
1
−

∑

i
=
0


k
−
1




1

i
!





(


x
θ


)


i



e

−
x

/

θ


=

e

−
x

/

θ



∑

i
=
k


∞




1

i
!





(


x
θ


)


i




{\displaystyle F(x;k,\theta )=1-\sum _{i=0}^{k-1}{\frac {1}{i!}}\left({\frac {x}{\theta }}\right)^{i}e^{-x/\theta }=e^{-x/\theta }\sum _{i=k}^{\infty }{\frac {1}{i!}}\left({\frac {x}{\theta }}\right)^{i}}
 
PropertiesEdit
SkewnessEdit
The skewness is equal to 



2

/



k




{\displaystyle 2/{\sqrt {k}}}
 , it depends only on the shape parameter (k) and approaches a normal distribution when k is large (approximately when k > 10).
Median calculationEdit
Unlike the mode and the mean which have readily calculable formulas based on the parameters, the median does not have an easy closed form equation. The median for this distribution is defined as the value ν such that






1

Γ
(
k
)

θ

k






∫

0


ν



x

k
−
1



e

−
x

/

θ


d
x
=



1
2



.


{\displaystyle {\frac {1}{\Gamma (k)\theta ^{k}}}\int _{0}^{\nu }x^{k-1}e^{-x/\theta }dx={\tfrac {1}{2}}.}
 
A formula for approximating the median for any gamma distribution, when the mean is known, has been derived based on the fact that the ratio μ/(μ − ν) is approximately a linear function of k when k ≥ 1.[6] The approximation formula is




ν
≈
μ



3
k
−
0.8


3
k
+
0.2



,


{\displaystyle \nu \approx \mu {\frac {3k-0.8}{3k+0.2}},}
 
where 



μ
(
=
k
θ
)


{\displaystyle \mu (=k\theta )}
  is the mean.
A rigorous treatment of the problem of determining an asymptotic expansion and bounds for the median of the Gamma Distribution was handled first by Chen and Rubin, who proved




m
−


1
3


<
λ
(
m
)
<
m
,


{\displaystyle m-{\frac {1}{3}}<\lambda (m)<m,}
 
where 



λ
(
m
)


{\displaystyle \lambda (m)}
  denotes the median of the 




Gamma

(
m
,
1
)


{\displaystyle {\text{Gamma}}(m,1)}
  distribution.[7]
K. P. Choi later showed that the first five terms in the asymptotic expansion of the median are




λ
(
m
)
=
m
−


1
3


+


8

405
m



+


184

25515

m

2





+


2248

3444525

m

3





−
⋯


{\displaystyle \lambda (m)=m-{\frac {1}{3}}+{\frac {8}{405m}}+{\frac {184}{25515m^{2}}}+{\frac {2248}{3444525m^{3}}}-\cdots }
 
by comparing the median to Ramanujan's 



θ


{\displaystyle \theta }
  function.[8]
Later, it was shown that 



λ
(
m
)


{\displaystyle \lambda (m)}
  is a convex function of 



m


{\displaystyle m}
 .[9]
SummationEdit
If Xi has a Gamma(ki, θ) distribution for i = 1, 2, ..., N (i.e., all distributions have the same scale parameter θ), then





∑

i
=
1


N



X

i


∼

G
a
m
m
a


(

∑

i
=
1


N



k

i


,
θ
)



{\displaystyle \sum _{i=1}^{N}X_{i}\sim \mathrm {Gamma} \left(\sum _{i=1}^{N}k_{i},\theta \right)}
 
provided all Xi are independent.
For the cases where the Xi are independent but have different scale parameters see Mathai (1982) and Moschopoulos (1984).
The gamma distribution exhibits infinite divisibility.
ScalingEdit
If




X
∼

G
a
m
m
a

(
k
,
θ
)
,


{\displaystyle X\sim \mathrm {Gamma} (k,\theta ),}
 
then, for any c > 0,




c
X
∼

G
a
m
m
a

(
k
,
c

θ
)
,


{\displaystyle cX\sim \mathrm {Gamma} (k,c\,\theta ),}
  by moment generating functions,
or equivalently




c
X
∼

G
a
m
m
a


(
k
,


β
c


)

,


{\displaystyle cX\sim \mathrm {Gamma} \left(k,{\frac {\beta }{c}}\right),}
 
Indeed, we know that if X is an exponential r.v. with rate λ then cX is an exponential r.v. with rate λ/c; the same thing is valid with Gamma variates (and this can be checked using the moment-generating function, see, e.g.,these notes, 10.4-(ii)): multiplication by a positive constant c divides the rate (or, equivalently, multiplies the scale).
Exponential familyEdit
The gamma distribution is a two-parameter exponential family with natural parameters k − 1 and −1/θ (equivalently, α − 1 and −β), and natural statistics X and ln(X).
If the shape parameter k is held fixed, the resulting one-parameter family of distributions is a natural exponential family.
Logarithmic expectationEdit
One can show that




E
⁡
[
ln
⁡
(
X
)
]
=
ψ
(
α
)
−
ln
⁡
(
β
)


{\displaystyle \operatorname {E} [\ln(X)]=\psi (\alpha )-\ln(\beta )}
 
or equivalently,




E
⁡
[
ln
⁡
(
X
)
]
=
ψ
(
k
)
+
ln
⁡
(
θ
)


{\displaystyle \operatorname {E} [\ln(X)]=\psi (k)+\ln(\theta )}
 
where ψ is the digamma function.
This can be derived using the exponential family formula for the moment generating function of the sufficient statistic, because one of the sufficient statistics of the gamma distribution is ln(x).
Information entropyEdit
The information entropy is








H
⁡
(
X
)



=
E
⁡
[
−
ln
⁡
(
p
(
X
)
)
]






=
E
⁡
[
−
α
ln
⁡
(
β
)
+
ln
⁡
(
Γ
(
α
)
)
−
(
α
−
1
)
ln
⁡
(
X
)
+
β
X
]






=
α
−
ln
⁡
(
β
)
+
ln
⁡
(
Γ
(
α
)
)
+
(
1
−
α
)
ψ
(
α
)
.






{\displaystyle {\begin{aligned}\operatorname {H} (X)&=\operatorname {E} [-\ln(p(X))]\\&=\operatorname {E} [-\alpha \ln(\beta )+\ln(\Gamma (\alpha ))-(\alpha -1)\ln(X)+\beta X]\\&=\alpha -\ln(\beta )+\ln(\Gamma (\alpha ))+(1-\alpha )\psi (\alpha ).\end{aligned}}}
 
In the k, θ parameterization, the information entropy is given by




H
⁡
(
X
)
=
k
+
ln
⁡
(
θ
)
+
ln
⁡
(
Γ
(
k
)
)
+
(
1
−
k
)
ψ
(
k
)
.


{\displaystyle \operatorname {H} (X)=k+\ln(\theta )+\ln(\Gamma (k))+(1-k)\psi (k).}
 
Kullback–Leibler divergenceEdit

 


Illustration of the Kullback–Leibler (KL) divergence for two gamma PDFs. Here β = β0 + 1 which are set to 1, 2, 3, 4, 5 and 6. The typical asymmetry for the KL divergence is clearly visible.


The Kullback–Leibler divergence (KL-divergence), of Gamma(αp, βp) ("true" distribution) from Gamma(αq, βq) ("approximating" distribution) is given by[10]









D


K
L



(

α

p


,

β

p


;

α

q


,

β

q


)
=





(

α

p


−

α

q


)
ψ
(

α

p


)
−
log
⁡
Γ
(

α

p


)
+
log
⁡
Γ
(

α

q


)








+

α

q


(
log
⁡

β

p


−
log
⁡

β

q


)
+

α

p






β

q


−

β

p




β

p




.






{\displaystyle {\begin{aligned}D_{\mathrm {KL} }(\alpha _{p},\beta _{p};\alpha _{q},\beta _{q})={}&(\alpha _{p}-\alpha _{q})\psi (\alpha _{p})-\log \Gamma (\alpha _{p})+\log \Gamma (\alpha _{q})\\&{}+\alpha _{q}(\log \beta _{p}-\log \beta _{q})+\alpha _{p}{\frac {\beta _{q}-\beta _{p}}{\beta _{p}}}.\end{aligned}}}
 
Written using the k, θ parameterization, the KL-divergence of Gamma(kp, θp) from Gamma(kq, θq) is given by









D


K
L



(

k

p


,

θ

p


;

k

q


,

θ

q


)
=





(

k

p


−

k

q


)
ψ
(

k

p


)
−
log
⁡
Γ
(

k

p


)
+
log
⁡
Γ
(

k

q


)








+

k

q


(
log
⁡

θ

q


−
log
⁡

θ

p


)
+

k

p






θ

p


−

θ

q




θ

q




.






{\displaystyle {\begin{aligned}D_{\mathrm {KL} }(k_{p},\theta _{p};k_{q},\theta _{q})={}&(k_{p}-k_{q})\psi (k_{p})-\log \Gamma (k_{p})+\log \Gamma (k_{q})\\&{}+k_{q}(\log \theta _{q}-\log \theta _{p})+k_{p}{\frac {\theta _{p}-\theta _{q}}{\theta _{q}}}.\end{aligned}}}
 
Laplace transformEdit
The Laplace transform of the gamma PDF is




F
(
s
)
=
(
1
+
θ
s

)

−
k


=



β

α



(
s
+
β

)

α





.


{\displaystyle F(s)=(1+\theta s)^{-k}={\frac {\beta ^{\alpha }}{(s+\beta )^{\alpha }}}.}
 
Differential equationEdit





{
x

f
′

(
x
)
+
f
(
x
)
(
−
α
+
1
+
β
x
)
=
0
;
f
(
1
)
=




e

−
1

/

β



β

−
α




Γ
(
α
)



}



{\displaystyle \left\{xf'(x)+f(x)(-\alpha +1+\beta x)=0;f(1)={\frac {e^{-1/\beta }\beta ^{-\alpha }}{\Gamma (\alpha )}}\right\}}
 





{
x

f
′

(
x
)
+
f
(
x
)
(
−
k
+
1
+
x

/

θ
)
=
0
;
f
(
1
)
=




e

−
θ




(


1
θ


)


−
k




Γ
(
k
)



}



{\displaystyle \left\{xf'(x)+f(x)(-k+1+x/\theta )=0;f(1)={\frac {e^{-\theta }\left({\frac {1}{\theta }}\right)^{-k}}{\Gamma (k)}}\right\}}
 
Parameter estimationEdit
Maximum likelihood estimationEdit
The likelihood function for N iid observations (x1, ..., xN) is




L
(
k
,
θ
)
=

∏

i
=
1


N


f
(

x

i


;
k
,
θ
)


{\displaystyle L(k,\theta )=\prod _{i=1}^{N}f(x_{i};k,\theta )}
 
from which we calculate the log-likelihood function




ℓ
(
k
,
θ
)
=
(
k
−
1
)

∑

i
=
1


N


ln
⁡

(

x

i


)

−

∑

i
=
1


N





x

i


θ


−
N
k
ln
⁡
(
θ
)
−
N
ln
⁡
(
Γ
(
k
)
)


{\displaystyle \ell (k,\theta )=(k-1)\sum _{i=1}^{N}\ln {(x_{i})}-\sum _{i=1}^{N}{\frac {x_{i}}{\theta }}-Nk\ln(\theta )-N\ln(\Gamma (k))}
 
Finding the maximum with respect to θ by taking the derivative and setting it equal to zero yields the maximum likelihood estimator of the θ parameter:







θ
^



=


1

k
N




∑

i
=
1


N



x

i




{\displaystyle {\hat {\theta }}={\frac {1}{kN}}\sum _{i=1}^{N}x_{i}}
 
Substituting this into the log-likelihood function gives




ℓ
=
(
k
−
1
)

∑

i
=
1


N


ln
⁡

(

x

i


)

−
N
k
−
N
k
ln
⁡


(



∑

x

i




k
N



)


−
N
ln
⁡
(
Γ
(
k
)
)


{\displaystyle \ell =(k-1)\sum _{i=1}^{N}\ln {(x_{i})}-Nk-Nk\ln {\left({\frac {\sum x_{i}}{kN}}\right)}-N\ln(\Gamma (k))}
 
Finding the maximum with respect to k by taking the derivative and setting it equal to zero yields




ln
⁡
(
k
)
−
ψ
(
k
)
=
ln
⁡

(


1
N



∑

i
=
1


N



x

i


)

−


1
N



∑

i
=
1


N


ln
⁡
(

x

i


)


{\displaystyle \ln(k)-\psi (k)=\ln \left({\frac {1}{N}}\sum _{i=1}^{N}x_{i}\right)-{\frac {1}{N}}\sum _{i=1}^{N}\ln(x_{i})}
 
There is no closed-form solution for k. The function is numerically very well behaved, so if a numerical solution is desired, it can be found using, for example, Newton's method. An initial value of k can be found either using the method of moments, or using the approximation




ln
⁡
(
k
)
−
ψ
(
k
)
≈


1

2
k




(
1
+


1

6
k
+
1



)



{\displaystyle \ln(k)-\psi (k)\approx {\frac {1}{2k}}\left(1+{\frac {1}{6k+1}}\right)}
 
If we let




s
=
ln
⁡

(


1
N



∑

i
=
1


N



x

i


)

−


1
N



∑

i
=
1


N


ln
⁡
(

x

i


)


{\displaystyle s=\ln \left({\frac {1}{N}}\sum _{i=1}^{N}x_{i}\right)-{\frac {1}{N}}\sum _{i=1}^{N}\ln(x_{i})}
 
then k is approximately




k
≈



3
−
s
+


(
s
−
3

)

2


+
24
s




12
s





{\displaystyle k\approx {\frac {3-s+{\sqrt {(s-3)^{2}+24s}}}{12s}}}
 
which is within 1.5% of the correct value.[11] An explicit form for the Newton–Raphson update of this initial guess is:[12]




k
←
k
−



ln
⁡
(
k
)
−
ψ
(
k
)
−
s




1
k


−

ψ

′


(
k
)



.


{\displaystyle k\leftarrow k-{\frac {\ln(k)-\psi (k)-s}{{\frac {1}{k}}-\psi ^{\prime }(k)}}.}
 
Bayesian minimum mean squared errorEdit
With known k and unknown θ, the posterior density function for theta (using the standard scale-invariant prior for θ) is




P
(
θ
∣
k
,

x

1


,
…
,

x

N


)
∝


1
θ



∏

i
=
1


N


f
(

x

i


;
k
,
θ
)


{\displaystyle P(\theta \mid k,x_{1},\dots ,x_{N})\propto {\frac {1}{\theta }}\prod _{i=1}^{N}f(x_{i};k,\theta )}
 
Denoting




y
≡

∑

i
=
1


N



x

i


,

P
(
θ
∣
k
,

x

1


,
…
,

x

N


)
=
C
(

x

i


)

θ

−
N
k
−
1



e

−
y

/

θ




{\displaystyle y\equiv \sum _{i=1}^{N}x_{i},\qquad P(\theta \mid k,x_{1},\dots ,x_{N})=C(x_{i})\theta ^{-Nk-1}e^{-y/\theta }}
 
Integration with respect to θ can be carried out using a change of variables, revealing that 1/θ is gamma-distributed with parameters α = Nk, β = y.





∫

0


∞



θ

−
N
k
−
1
+
m



e

−
y

/

θ



d
θ
=

∫

0


∞



x

N
k
−
1
−
m



e

−
x
y



d
x
=

y

−
(
N
k
−
m
)


Γ
(
N
k
−
m
)



{\displaystyle \int _{0}^{\infty }\theta ^{-Nk-1+m}e^{-y/\theta }\,d\theta =\int _{0}^{\infty }x^{Nk-1-m}e^{-xy}\,dx=y^{-(Nk-m)}\Gamma (Nk-m)\!}
 
The moments can be computed by taking the ratio (m by m = 0)




E
⁡
[

x

m


]
=



Γ
(
N
k
−
m
)


Γ
(
N
k
)




y

m




{\displaystyle \operatorname {E} [x^{m}]={\frac {\Gamma (Nk-m)}{\Gamma (Nk)}}y^{m}}
 
which shows that the mean ± standard deviation estimate of the posterior distribution for θ is






y

N
k
−
1



±



y

2



(
N
k
−
1

)

2


(
N
k
−
2
)



.


{\displaystyle {\frac {y}{Nk-1}}\pm {\frac {y^{2}}{(Nk-1)^{2}(Nk-2)}}.}
 
Generating gamma-distributed random variablesEdit
Given the scaling property above, it is enough to generate gamma variables with θ = 1 as we can later convert to any value of β with simple division.
Suppose we wish to generate random variables from Gamma(n + δ, 1), where n is a non-negative integer and 0 < δ < 1. Using the fact that a Gamma(1, 1) distribution is the same as an Exp(1) distribution, and noting the method of generating exponential variables, we conclude that if U is uniformly distributed on (0, 1], then −ln(U) is distributed Gamma(1, 1). Now, using the "α-addition" property of gamma distribution, we expand this result:




−

∑

k
=
1


n


ln
⁡

U

k


∼
Γ
(
n
,
1
)


{\displaystyle -\sum _{k=1}^{n}\ln U_{k}\sim \Gamma (n,1)}
 
where Uk are all uniformly distributed on (0, 1] and independent. All that is left now is to generate a variable distributed as Gamma(δ, 1) for 0 < δ < 1 and apply the "α-addition" property once more. This is the most difficult part.
Random generation of gamma variates is discussed in detail by Devroye,[13]:401–428 noting that none are uniformly fast for all shape parameters. For small values of the shape parameter, the algorithms are often not valid.[13]:406 For arbitrary values of the shape parameter, one can apply the Ahrens and Dieter[14] modified acceptance–rejection method Algorithm GD (shape k ≥ 1), or transformation method[15] when 0 < k < 1. Also see Cheng and Feast Algorithm GKM 3[16] or Marsaglia's squeeze method.[17]
The following is a version of the Ahrens-Dieter acceptance–rejection method:[14]
Generate U, V and W as iid uniform (0, 1] variates.
If 



U
≤


e

e
+
δ





{\displaystyle U\leq {\frac {e}{e+\delta }}}
  then 



ξ
=

V

1

/

δ




{\displaystyle \xi =V^{1/\delta }}
  and 



η
=
W

ξ

δ
−
1




{\displaystyle \eta =W\xi ^{\delta -1}}
 . Otherwise, 



ξ
=
1
−
ln
⁡
V


{\displaystyle \xi =1-\ln V}
  and 



η
=
W

e

−
ξ




{\displaystyle \eta =We^{-\xi }}
 .
If 



η
>

ξ

δ
−
1



e

−
ξ




{\displaystyle \eta >\xi ^{\delta -1}e^{-\xi }}
  then go to step 1.
ξ is distributed as Γ(δ, 1).
A summary of this is




θ

(
ξ
−

∑

i
=
1


⌊
k
⌋


ln
⁡
(

U

i


)
)

∼
Γ
(
k
,
θ
)


{\displaystyle \theta \left(\xi -\sum _{i=1}^{\lfloor k\rfloor }\ln(U_{i})\right)\sim \Gamma (k,\theta )}
 
where 




⌊
k
⌋



{\displaystyle \scriptstyle \lfloor k\rfloor }
  is the integral part of k, ξ is generated via the algorithm above with δ = {k} (the fractional part of k) and the Uk are all independent.
While the above approach is technically correct, Devroye notes that it is linear in the value of k and in general is not a good choice. Instead he recommends using either rejection-based or table-based methods, depending on context.[13]:401–428
For example, Marsaglia's simple transformation-rejection method relying on a one normal and one uniform random number:[18]
Setup: d = a - 1/3, c = 1/sqrt(9d).
Generate: v=(1+c*x)ˆ3, with x standard normal.
if v > 0 and log(UNI) < 0.5 · xˆ2 + d − dv + d log(v) return dv.
go back to step 2.
With 



1
≤
a
=
α
=
k


{\displaystyle 1\leq a=\alpha =k}
  generates a gamma distributed random number in time that is approximately constant with k. The acceptance rate does depend on k, with an acceptance rate of 0.95, 0.98, and 0.99 for k=1, 2, and 4. For k < 1, one can use 




γ

α


=

γ

1
+
α



U

1

/

α




{\displaystyle \gamma _{\alpha }=\gamma _{1+\alpha }U^{1/\alpha }}
  to boost k to be usable with this method.
ApplicationsEdit
 
This section needs expansion. You can help by adding to it. (March 2009)
The gamma distribution has been used to model the size of insurance claims[19] and rainfalls.[20] This means that aggregate insurance claims and the amount of rainfall accumulated in a reservoir are modelled by a gamma process. The gamma distribution is also used to model errors in multi-level Poisson regression models, because the combination of the Poisson distribution and a gamma distribution is a negative binomial distribution.
In wireless communication, the gamma distribution is used to model the multi-path fading of signal power.
In neuroscience, the gamma distribution is often used to describe the distribution of inter-spike intervals.[21][22]
In bacterial gene expression, the copy number of a constitutively expressed protein often follows the gamma distribution, where the scale and shape parameter are, respectively, the mean number of bursts per cell cycle and the mean number of protein molecules produced by a single mRNA during its lifetime.[23]
In genomics, the gamma distribution was applied in peak calling step (i.e. in recognition of signal) in ChIP-chip[24] and ChIP-seq[25] data analysis.
The gamma distribution is widely used as a conjugate prior in Bayesian statistics. It is the conjugate prior for the precision (i.e. inverse of the variance) of a normal distribution. It is also the conjugate prior for the exponential distribution.
Related distributionsEdit
Special casesEdit
Conjugate priorEdit
In Bayesian inference, the gamma distribution is the conjugate prior to many likelihood distributions: the Poisson, exponential, normal (with known mean), Pareto, gamma with known shape σ, inverse gamma with known shape parameter, and Gompertz with known scale parameter.
The gamma distribution's conjugate prior is:[26]




p
(
k
,
θ
∣
p
,
q
,
r
,
s
)
=


1
Z






p

k
−
1



e

−

θ

−
1


q




Γ
(
k

)

r



θ

k
s





,


{\displaystyle p(k,\theta \mid p,q,r,s)={\frac {1}{Z}}{\frac {p^{k-1}e^{-\theta ^{-1}q}}{\Gamma (k)^{r}\theta ^{ks}}},}
 
where Z is the normalizing constant, which has no closed-form solution. The posterior distribution can be found by updating the parameters as follows:









p
′




=
p

∏

i



x

i


,





q
′




=
q
+

∑

i



x

i


,





r
′




=
r
+
n
,





s
′




=
s
+
n
,






{\displaystyle {\begin{aligned}p'&=p\prod \nolimits _{i}x_{i},\\q'&=q+\sum \nolimits _{i}x_{i},\\r'&=r+n,\\s'&=s+n,\end{aligned}}}
 
where n is the number of observations, and xi is the ith observation.
Compound gammaEdit
If the shape parameter of the gamma distribution is known, but the inverse-scale parameter is unknown, then a gamma distribution for the inverse-scale forms a conjugate prior. The compound distribution, which results from integrating out the inverse-scale, has a closed form solution, known as the compound gamma distribution.[27]
If instead the shape parameter is known but the mean is unknown, with the prior of the mean being given by another gamma distribution, then it results in K-distribution.
Related distributions and propertiesEdit
If X ~ Gamma(1, 1/λ) (shape -scale parametrization), then X has an exponential distribution with rate parameter λ.
If X ~ Gamma(ν/2, 2)(shape -scale parametrization), then X is identical to χ2(ν), the chi-squared distribution with ν degrees of freedom. Conversely, if Q ~ χ2(ν) and c is a positive constant, then cQ ~ Gamma(ν/2, 2c).
If k is an integer, the gamma distribution is an Erlang distribution and is the probability distribution of the waiting time until the kth "arrival" in a one-dimensional Poisson process with intensity 1/θ. If





X
∼
Γ
(
k
∈

Z

,
θ
)
,

Y
∼

P
o
i
s


(


x
θ


)

,


{\displaystyle X\sim \Gamma (k\in \mathbf {Z} ,\theta ),\qquad Y\sim \mathrm {Pois} \left({\frac {x}{\theta }}\right),}
 

then




P
(
X
>
x
)
=
P
(
Y
<
k
)
.


{\displaystyle P(X>x)=P(Y<k).}
 

If X has a Maxwell–Boltzmann distribution with parameter a, then






X

2


∼
Γ

(



3
2



,
2

a

2


)



{\displaystyle X^{2}\sim \Gamma \left({\tfrac {3}{2}},2a^{2}\right)}
 .

If X ~ Gamma(k, θ), then 





X




{\displaystyle {\sqrt {X}}}
  follows a generalized gamma distribution with parameters p = 2, d = 2k, and 



a
=


θ




{\displaystyle a={\sqrt {\theta }}}
 [citation needed] .
More generally, if X ~ Gamma(k,θ), then 




X

q




{\displaystyle X^{q}}
  for 



q
>
0


{\displaystyle q>0}
  follows a generalized gamma distribution with parameters p = 1/q, d = k/q, and 



a
=

θ

q




{\displaystyle a=\theta ^{q}}
 .
If X ~ Gamma(k, θ), then 1/X ~ Inv-Gamma(k, θ−1) (see Inverse-gamma distribution for derivation).
Parametrization 1: If 




X

k


∼
Γ
(

α

k


,

θ

k


)



{\displaystyle X_{k}\sim \Gamma (\alpha _{k},\theta _{k})\,}
  are independent, then 







α

2



θ

2



X

1





α

1



θ

1



X

2





∼

F

(
2

α

1


,
2

α

2


)


{\displaystyle {\frac {\alpha _{2}\theta _{2}X_{1}}{\alpha _{1}\theta _{1}X_{2}}}\sim \mathrm {F} (2\alpha _{1},2\alpha _{2})}
 , or equivalently, 






X

1



X

2




∼

β



′



(

α

1


,

α

2


,
1
,



θ

1



θ

2




)


{\displaystyle {\frac {X_{1}}{X_{2}}}\sim \beta ^{'}(\alpha _{1},\alpha _{2},1,{\frac {\theta _{1}}{\theta _{2}}})}
 
Parametrization 2: If 




X

k


∼
Γ
(

α

k


,

β

k


)



{\displaystyle X_{k}\sim \Gamma (\alpha _{k},\beta _{k})\,}
  are independent, then 







α

2



β

1



X

1





α

1



β

2



X

2





∼

F

(
2

α

1


,
2

α

2


)


{\displaystyle {\frac {\alpha _{2}\beta _{1}X_{1}}{\alpha _{1}\beta _{2}X_{2}}}\sim \mathrm {F} (2\alpha _{1},2\alpha _{2})}
 , or equivalently, 






X

1



X

2




∼

β



′



(

α

1


,

α

2


,
1
,



β

2



β

1




)


{\displaystyle {\frac {X_{1}}{X_{2}}}\sim \beta ^{'}(\alpha _{1},\alpha _{2},1,{\frac {\beta _{2}}{\beta _{1}}})}
 
If X ~ Gamma(α, θ) and Y ~ Gamma(β, θ) are independently distributed, then X/(X + Y) has a beta distribution with parameters α and β.
If Xi ~ Gamma(αi, 1) are independently distributed, then the vector (X1/S, ..., Xn/S), where S = X1 + ... + Xn, follows a Dirichlet distribution with parameters α1, ..., αn.
For large k the gamma distribution converges to Gaussian distribution with mean μ = kθ and variance σ2 = kθ2.
The gamma distribution is the conjugate prior for the precision of the normal distribution with known mean.
The Wishart distribution is a multivariate generalization of the gamma distribution (samples are positive-definite matrices rather than positive real numbers).
The gamma distribution is a special case of the generalized gamma distribution, the generalized integer gamma distribution, and the generalized inverse Gaussian distribution.
Among the discrete distributions, the negative binomial distribution is sometimes considered the discrete analogue of the Gamma distribution.
Tweedie distributions – the gamma distribution is a member of the family of Tweedie exponential dispersion models.
NotesEdit

^ http://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture6.pdf
^ Park, Sung Y.; Bera, Anil K. (2009). "Maximum entropy autoregressive conditional heteroskedasticity model" (PDF). Journal of Econometrics. Elsevier: 219–230. doi:10.1016/j.jeconom.2008.12.014. Retrieved 2011-06-02. 
^ See Hogg and Craig (2005, Remark 3.3.1) for an explicit motivation
^ Scalable Recommendation with Poisson Factorization, Prem Gopalan, Jake M. Hofman, David Blei, arXiv.org 2014
^ a b Papoulis, Pillai, Probability, Random Variables, and Stochastic Processes, Fourth Edition
^ Banneheka BMSG, Ekanayake GEMUPD (2009) "A new point estimator for the median of gamma distribution". Viyodaya J Science, 14:95–103
^ Jeesen Chen, Herman Rubin, Bounds for the difference between median and mean of gamma and poisson distributions, Statistics & Probability Letters, Volume 4, Issue 6, October 1986, Pages 281-283, ISSN 0167-7152, [1].
^ Choi, K.P. "On the Medians of the Gamma Distributions and an Equation of Ramanujan", Proceedings of the American Mathematical Society, Vol. 121, No. 1 (May, 1994), pp. 245–251.
^ Berg,Christian and Pedersen, Henrik L. "Convexity of the median in the gamma distribution".
^ W.D. Penny, [www.fil.ion.ucl.ac.uk/~wpenny/publications/densities.ps KL-Divergences of Normal, Gamma, Dirichlet, and Wishart densities][full citation needed]
^ Minka, Thomas P. (2002). "Estimating a Gamma distribution" (PDF). 
^ Choi, S. C.; Wette, R. (1969). "Maximum Likelihood Estimation of the Parameters of the Gamma Distribution and Their Bias". Technometrics. 11 (4): 683–690. doi:10.1080/00401706.1969.10490731. 
^ a b c Devroye, Luc (1986). Non-Uniform Random Variate Generation. New York: Springer-Verlag. ISBN 0-387-96305-7.  See Chapter 9, Section 3.
^ a b Ahrens, J. H.; Dieter, U (January 1982). "Generating gamma variates by a modified rejection technique". Communications of the ACM. 25 (1): 47–54. doi:10.1145/358315.358390. . See Algorithm GD, p. 53.
^ Ahrens, J. H.; Dieter, U. (1974). "Computer methods for sampling from gamma, beta, Poisson and binomial distributions". Computing. 12: 223–246. CiteSeerX 10.1.1.93.3828 . doi:10.1007/BF02293108. 
^ Cheng, R.C.H., and Feast, G.M. Some simple gamma variate generators. Appl. Stat. 28 (1979), 290–295.
^ Marsaglia, G. The squeeze method for generating gamma variates. Comput, Math. Appl. 3 (1977), 321–325.
^ Marsaglia, G.; Tsang, W. W. (2000). "A simple method for generating gamma variables". ACM Transactions on Mathematical Software. 26 (3): 363–372. doi:10.1145/358407.358414. 
^ p. 43, Philip J. Boland, Statistical and Probabilistic Methods in Actuarial Science, Chapman & Hall CRC 2007
^ Aksoy, H. (2000) "Use of Gamma Distribution in Hydrological Analysis", Turk J. Engin Environ Sci, 24, 419 – 428.
^ J. G. Robson and J. B. Troy, "Nature of the maintained discharge of Q, X, and Y retinal ganglion cells of the cat", J. Opt. Soc. Am. A 4, 2301–2307 (1987)
^ M.C.M. Wright, I.M. Winter, J.J. Forster, S. Bleeck "Response to best-frequency tone bursts in the ventral cochlear nucleus is governed by ordered inter-spike interval statistics", Hearing Research 317 (2014)
^ N. Friedman, L. Cai and X. S. Xie (2006) "Linking stochastic dynamics to population distribution: An analytical framework of gene expression", Phys. Rev. Lett. 97, 168302.
^ DJ Reiss, MT Facciotti and NS Baliga (2008) "Model-based deconvolution of genome-wide DNA binding", Bioinformatics, 24, 396–403
^ MA Mendoza-Parra, M Nowicka, W Van Gool, H Gronemeyer (2013) "Characterising ChIP-seq binding patterns by model-based peak shape deconvolution", BMC Genomics, 14:834
^ Fink, D. 1995 A Compendium of Conjugate Priors. In progress report: Extension and enhancement of methods for setting data quality objectives. (DOE contract 95‑831).
^ Dubey, Satya D. (December 1970). "Compound gamma, beta and F distributions". Metrika. 16: 27–31. doi:10.1007/BF02613934. 

ReferencesEdit
R. V. Hogg and A. T. Craig (1978) Introduction to Mathematical Statistics, 4th edition. New York: Macmillan. (See Section 3.3.)'
P. G. Moschopoulos (1985) The distribution of the sum of independent gamma random variables, Annals of the Institute of Statistical Mathematics, 37, 541–544
A. M. Mathai (1982) Storage capacity of a dam with gamma type inputs, Annals of the Institute of Statistical Mathematics, 34, 591–597
External linksEdit
 
The Wikibook Statistics has a page on the topic of: Gamma distribution
Hazewinkel, Michiel, ed. (2001), "Gamma-distribution", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Weisstein, Eric W. "Gamma distribution". MathWorld. 
ModelAssist (2017) Uses of the Gamma distribution in risk modeling, including applied examples in Excel.
Engineering Statistics Handbook


Retrieved from "https://en.wikipedia.org/w/index.php?title=Gamma_distribution&oldid=774709059"







				Last edited on 10 April 2017, at 05:41
			





Content is available under CC BY-SA 3.0 unless otherwise noted.

Terms of UsePrivacyDesktop






(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.380","walltime":"0.562","ppvisitednodes":{"value":2252,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":84127,"limit":2097152},"templateargumentsize":{"value":3114,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":3,"limit":500},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  244.109      1 -total"," 38.83%   94.782      1 Template:Reflist"," 21.70%   52.966      7 Template:Cite_journal"," 11.80%   28.810      1 Template:Refimprove"," 11.78%   28.758      2 Template:Fix"," 11.46%   27.985      2 Template:Ambox","  8.92%   21.784      1 Template:Citation_needed","  8.22%   20.062      1 Template:Distinguish","  7.12%   17.391      2 Template:Delink","  6.43%   15.695      2 Template:Navbox"]},"scribunto":{"limitreport-timeusage":{"value":"0.097","limit":"10.000"},"limitreport-memusage":{"value":4872334,"limit":52428800}},"cachereport":{"origin":"mw1255","timestamp":"20170410054224","ttl":2592000,"transientcontent":false}}});});/*<![CDATA[*/(window.NORLQ=window.NORLQ||[]).push(function(){var ns,i,p,img;ns=document.getElementsByTagName('noscript');for(i=0;i<ns.length;i++){p=ns[i].nextSibling;if(p&&p.className&&p.className.indexOf('lazy-image-placeholder')>-1){img=document.createElement('img');img.setAttribute('src',p.getAttribute('data-src'));img.setAttribute('width',p.getAttribute('data-width'));img.setAttribute('height',p.getAttribute('data-height'));img.setAttribute('alt',p.getAttribute('data-alt'));p.parentNode.replaceChild(img,p);}}});/*]]>*/(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":709,"wgHostname":"mw1255"});}); 







Pareto distribution - Wikipedia
document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );
(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Pareto_distribution","wgTitle":"Pareto distribution","wgCurRevisionId":774080438,"wgRevisionId":774080438,"wgArticleId":53057,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Pages using citations with accessdate and no URL","Pages using deprecated image syntax","All articles with unsourced statements","Articles with unsourced statements from February 2012","Articles with unsourced statements from December 2010","Articles with unsourced statements from November 2012","Articles with unsourced statements from February 2011","Wikipedia articles with GND identifiers","Actuarial science","Continuous distributions","Power laws","Probability distributions with non-finite variance","Exponential family distributions"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Pareto_distribution","wgRelevantArticleId":53057,"wgRequestId":"WOXIBApAMFkAAH8mX9QAAABO","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":false,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesBetaFeatureEnabled":false,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q837683","wgCentralAuthMobileDomain":false,"wgVisualEditorToolbarScrollOffset":0,"wgEditSubmitButtonLabelPublish":false});mw.loader.state({"ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"loading","user.tokens":"loading","ext.math.styles":"ready","ext.cite.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["ext.math.scripts","ext.cite.a11y","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.legacy.wikibits","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.gadget.featured-articles-links","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});



















 






Pareto distribution

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search


Pareto Type I


Probability density function

Pareto Type I probability density functions for various α with xm = 1. As α → ∞ the distribution approaches δ(x − xm) where δ is the Dirac delta function.



Cumulative distribution function

Pareto Type I cumulative distribution functions for various α with xm = 1.


Parameters
xm > 0 scale (real)
α > 0 shape (real)


Support




x
∈
[

x


m



,
+
∞
)


{\displaystyle x\in [x_{\mathrm {m} },+\infty )}




PDF







α


x


m



α




x

α
+
1





 for 

x
≥

x

m




{\displaystyle {\frac {\alpha \,x_{\mathrm {m} }^{\alpha }}{x^{\alpha +1}}}{\text{ for }}x\geq x_{m}}




CDF




1
−


(



x


m



x


)


α



 for 

x
≥

x

m




{\displaystyle 1-\left({\frac {x_{\mathrm {m} }}{x}}\right)^{\alpha }{\text{ for }}x\geq x_{m}}




Mean






{



∞



for 

α
≤
1







α


x


m





α
−
1






for 

α
>
1








{\displaystyle {\begin{cases}\infty &{\text{for }}\alpha \leq 1\\{\frac {\alpha \,x_{\mathrm {m} }}{\alpha -1}}&{\text{for }}\alpha >1\end{cases}}}




Median





x


m





2

α





{\displaystyle x_{\mathrm {m} }{\sqrt[{\alpha }]{2}}}




Mode





x


m





{\displaystyle x_{\mathrm {m} }}




Variance






{



∞



for 

α
∈
(
0
,
2
]








x


m



2


α


(
α
−
1

)

2


(
α
−
2
)






for 

α
>
2








{\displaystyle {\begin{cases}\infty &{\text{for }}\alpha \in (0,2]\\{\frac {x_{\mathrm {m} }^{2}\alpha }{(\alpha -1)^{2}(\alpha -2)}}&{\text{for }}\alpha >2\end{cases}}}




Skewness







2
(
1
+
α
)


α
−
3








α
−
2

α




 for 

α
>
3


{\displaystyle {\frac {2(1+\alpha )}{\alpha -3}}\,{\sqrt {\frac {\alpha -2}{\alpha }}}{\text{ for }}\alpha >3}




Ex. kurtosis







6
(

α

3


+

α

2


−
6
α
−
2
)


α
(
α
−
3
)
(
α
−
4
)




 for 

α
>
4


{\displaystyle {\frac {6(\alpha ^{3}+\alpha ^{2}-6\alpha -2)}{\alpha (\alpha -3)(\alpha -4)}}{\text{ for }}\alpha >4}




Entropy




log
⁡

(

(



x


m



α


)



e

1
+



1
α





)



{\displaystyle \log \left(\left({\frac {x_{\mathrm {m} }}{\alpha }}\right)\,e^{1+{\tfrac {1}{\alpha }}}\right)}




MGF




α
(
−

x


m



t

)

α


Γ
(
−
α
,
−

x


m



t
)

 for 

t
<
0


{\displaystyle \alpha (-x_{\mathrm {m} }t)^{\alpha }\Gamma (-\alpha ,-x_{\mathrm {m} }t){\text{ for }}t<0}




CF




α
(
−
i

x


m



t

)

α


Γ
(
−
α
,
−
i

x


m



t
)


{\displaystyle \alpha (-ix_{\mathrm {m} }t)^{\alpha }\Gamma (-\alpha ,-ix_{\mathrm {m} }t)}




Fisher information






(





α

x

m


2






−


1

x

m








−


1

x

m








1

α

2







)




{\displaystyle {\begin{pmatrix}{\frac {\alpha }{x_{m}^{2}}}&-{\frac {1}{x_{m}}}\\-{\frac {1}{x_{m}}}&{\frac {1}{\alpha ^{2}}}\end{pmatrix}}}




The Pareto distribution, named after the Italian civil engineer, economist, and sociologist Vilfredo Pareto, is a power law probability distribution that is used in description of social, scientific, geophysical, actuarial, and many other types of observable phenomena.



Contents


1 Definition
2 Properties

2.1 Cumulative distribution function
2.2 Probability density function
2.3 Moments and characteristic function
2.4 Conditional distributions
2.5 A characterization theorem
2.6 Geometric mean
2.7 Harmonic mean


3 Generalized Pareto distributions

3.1 Pareto types I–IV
3.2 Feller–Pareto distribution


4 Applications
5 Relation to other distributions

5.1 Relation to the exponential distribution
5.2 Relation to the log-normal distribution
5.3 Relation to the generalized Pareto distribution
5.4 Relation to Zipf's law
5.5 Relation to the "Pareto principle"


6 Lorenz curve and Gini coefficient
7 Parameter estimation
8 Graphical representation
9 Random sample generation
10 Variants

10.1 Bounded Pareto distribution

10.1.1 Generating bounded Pareto random variables


10.2 Symmetric Pareto distribution


11 Multivariate Pareto distribution
12 See also
13 Notes
14 References
15 External links



Definition[edit]
If X is a random variable with a Pareto (Type I) distribution,[1] then the probability that X is greater than some number x, i.e. the survival function (also called tail function), is given by







F
¯


(
x
)
=
Pr
(
X
>
x
)
=


{





(



x


m



x


)


α




x
≥

x


m



,




1


x
<

x


m



,








{\displaystyle {\overline {F}}(x)=\Pr(X>x)={\begin{cases}\left({\frac {x_{\mathrm {m} }}{x}}\right)^{\alpha }&x\geq x_{\mathrm {m} },\\1&x<x_{\mathrm {m} },\end{cases}}}



where xm is the (necessarily positive) minimum possible value of X, and α is a positive parameter. The Pareto Type I distribution is characterized by a scale parameter xm and a shape parameter α, which is known as the tail index. When this distribution is used to model the distribution of wealth, then the parameter α is called the Pareto index.
Properties[edit]
Cumulative distribution function[edit]
From the definition, the cumulative distribution function of a Pareto random variable with parameters α and xm is






F

X


(
x
)
=


{



1
−


(



x


m



x


)


α




x
≥

x


m



,




0


x
<

x


m



.








{\displaystyle F_{X}(x)={\begin{cases}1-\left({\frac {x_{\mathrm {m} }}{x}}\right)^{\alpha }&x\geq x_{\mathrm {m} },\\0&x<x_{\mathrm {m} }.\end{cases}}}



Probability density function[edit]
It follows (by differentiation) that the probability density function is






f

X


(
x
)
=


{






α

x


m



α




x

α
+
1






x
≥

x


m



,




0


x
<

x


m



.








{\displaystyle f_{X}(x)={\begin{cases}{\frac {\alpha x_{\mathrm {m} }^{\alpha }}{x^{\alpha +1}}}&x\geq x_{\mathrm {m} },\\0&x<x_{\mathrm {m} }.\end{cases}}}



When plotted on linear axes, the distribution assumes the familiar J-shaped curve which approaches each of the orthogonal axes asymptotically. All segments of the curve are self-similar (subject to appropriate scaling factors). When plotted in a log-log plot, the distribution is represented by a straight line.
Moments and characteristic function[edit]

The expected value of a random variable following a Pareto distribution is








E
(
X
)
=


{



∞


α
≤
1
,







α

x


m





α
−
1





α
>
1.








{\displaystyle E(X)={\begin{cases}\infty &\alpha \leq 1,\\{\frac {\alpha x_{\mathrm {m} }}{\alpha -1}}&\alpha >1.\end{cases}}}






The variance of a random variable following a Pareto distribution is









V
a
r

(
X
)
=


{



∞


α
∈
(
1
,
2
]
,






(



x


m




α
−
1



)


2




α

α
−
2





α
>
2.








{\displaystyle \mathrm {Var} (X)={\begin{cases}\infty &\alpha \in (1,2],\\\left({\frac {x_{\mathrm {m} }}{\alpha -1}}\right)^{2}{\frac {\alpha }{\alpha -2}}&\alpha >2.\end{cases}}}






(If α ≤ 1, the variance does not exist.)


The raw moments are









μ

n

′

=


{



∞


α
≤
n
,







α

x


m



n




α
−
n





α
>
n
.








{\displaystyle \mu _{n}'={\begin{cases}\infty &\alpha \leq n,\\{\frac {\alpha x_{\mathrm {m} }^{n}}{\alpha -n}}&\alpha >n.\end{cases}}}






The moment generating function is only defined for non-positive values t ≤ 0 as








M

(
t
;
α
,

x


m



)

=
E

[

e

t
X


]

=
α
(
−

x


m



t

)

α


Γ
(
−
α
,
−

x


m



t
)


{\displaystyle M\left(t;\alpha ,x_{\mathrm {m} }\right)=E\left[e^{tX}\right]=\alpha (-x_{\mathrm {m} }t)^{\alpha }\Gamma (-\alpha ,-x_{\mathrm {m} }t)}






M

(
0
,
α
,

x


m



)

=
1.


{\displaystyle M\left(0,\alpha ,x_{\mathrm {m} }\right)=1.}






The characteristic function is given by








φ
(
t
;
α
,

x


m



)
=
α
(
−
i

x


m



t

)

α


Γ
(
−
α
,
−
i

x


m



t
)
,


{\displaystyle \varphi (t;\alpha ,x_{\mathrm {m} })=\alpha (-ix_{\mathrm {m} }t)^{\alpha }\Gamma (-\alpha ,-ix_{\mathrm {m} }t),}






where Γ(a, x) is the incomplete gamma function.

Conditional distributions[edit]
The conditional probability distribution of a Pareto-distributed random variable, given the event that it is greater than or equal to a particular number 




x

1




{\displaystyle x_{1}}

 exceeding 




x

m




{\displaystyle x_{\text{m}}}

, is a Pareto distribution with the same Pareto index 



α


{\displaystyle \alpha }

 but with minimum 




x

1




{\displaystyle x_{1}}

 instead of 




x

m




{\displaystyle x_{\text{m}}}

.
A characterization theorem[edit]
Suppose 




X

1


,

X

2


,

X

3


,
…


{\displaystyle X_{1},X_{2},X_{3},\dotsc }

 are independent identically distributed random variables whose probability distribution is supported on the interval 



[

x

m


,
∞
)


{\displaystyle [x_{\text{m}},\infty )}

 for some 




x

m


>
0


{\displaystyle x_{\text{m}}>0}

. Suppose that for all 



n


{\displaystyle n}

, the two random variables 



min
{

X

1


,
…
,

X

n


}


{\displaystyle \min\{X_{1},\dotsc ,X_{n}\}}

 and 



(

X

1


+
⋯
+

X

n


)

/

min
{

X

1


,
…
,

X

n


}


{\displaystyle (X_{1}+\dotsb +X_{n})/\min\{X_{1},\dotsc ,X_{n}\}}

 are independent. Then the common distribution is a Pareto distribution.[citation needed]
Geometric mean[edit]
The geometric mean (G) is[2]





G
=

x

m


exp
⁡

(


1
α


)

.


{\displaystyle G=x_{\text{m}}\exp \left({\frac {1}{\alpha }}\right).}



Harmonic mean[edit]
The harmonic mean (H) is[2]





H
=

x

m



(
1
+


1
α


)

.


{\displaystyle H=x_{\text{m}}\left(1+{\frac {1}{\alpha }}\right).}



Generalized Pareto distributions[edit]
See also: Generalized Pareto distribution
There is a hierarchy [1][3] of Pareto distributions known as Pareto Type I, II, III, IV, and Feller–Pareto distributions.[1][3][4] Pareto Type IV contains Pareto Type I–III as special cases. The Feller–Pareto[3][5] distribution generalizes Pareto Type IV.
Pareto types I–IV[edit]
The Pareto distribution hierarchy is summarized in the next table comparing the survival functions (complementary CDF).
When μ = 0, the Pareto distribution Type II is also known as the Lomax distribution.[6]
In this section, the symbol xm, used before to indicate the minimum value of x, is replaced by σ.

Pareto distributions








F
¯


(
x
)
=
1
−
F
(
x
)


{\displaystyle {\overline {F}}(x)=1-F(x)}


Support
Parameters


Type I






[


x
σ


]


−
α




{\displaystyle \left[{\frac {x}{\sigma }}\right]^{-\alpha }}






x
>
σ


{\displaystyle x>\sigma }






σ
>
0
,
α


{\displaystyle \sigma >0,\alpha }




Type II






[
1
+



x
−
μ

σ


]


−
α




{\displaystyle \left[1+{\frac {x-\mu }{\sigma }}\right]^{-\alpha }}






x
>
μ


{\displaystyle x>\mu }






μ
∈

R

,
σ
>
0
,
α


{\displaystyle \mu \in \mathbb {R} ,\sigma >0,\alpha }




Lomax






[
1
+


x
σ


]


−
α




{\displaystyle \left[1+{\frac {x}{\sigma }}\right]^{-\alpha }}






x
>
0


{\displaystyle x>0}






σ
>
0
,
α


{\displaystyle \sigma >0,\alpha }




Type III






[
1
+


(



x
−
μ

σ


)


1

/

γ


]


−
1




{\displaystyle \left[1+\left({\frac {x-\mu }{\sigma }}\right)^{1/\gamma }\right]^{-1}}






x
>
μ


{\displaystyle x>\mu }






μ
∈

R

,
σ
,
γ
>
0


{\displaystyle \mu \in \mathbb {R} ,\sigma ,\gamma >0}




Type IV






[
1
+


(



x
−
μ

σ


)


1

/

γ


]


−
α




{\displaystyle \left[1+\left({\frac {x-\mu }{\sigma }}\right)^{1/\gamma }\right]^{-\alpha }}






x
>
μ


{\displaystyle x>\mu }






μ
∈

R

,
σ
,
γ
>
0
,
α


{\displaystyle \mu \in \mathbb {R} ,\sigma ,\gamma >0,\alpha }




The shape parameter α is the tail index, μ is location, σ is scale, γ is an inequality parameter. Some special cases of Pareto Type (IV) are







P
(
I
V
)
(
σ
,
σ
,
1
,
α
)
=
P
(
I
)
(
σ
,
α
)
,


{\displaystyle P(IV)(\sigma ,\sigma ,1,\alpha )=P(I)(\sigma ,\alpha ),}






P
(
I
V
)
(
μ
,
σ
,
1
,
α
)
=
P
(
I
I
)
(
μ
,
σ
,
α
)
,


{\displaystyle P(IV)(\mu ,\sigma ,1,\alpha )=P(II)(\mu ,\sigma ,\alpha ),}






P
(
I
V
)
(
μ
,
σ
,
γ
,
1
)
=
P
(
I
I
I
)
(
μ
,
σ
,
γ
)
.


{\displaystyle P(IV)(\mu ,\sigma ,\gamma ,1)=P(III)(\mu ,\sigma ,\gamma ).}





The finiteness of the mean, and the existence and the finiteness of the variance depend on the tail index α (inequality index γ). In particular, fractional δ-moments are finite for some δ > 0, as shown in the table below, where δ is not necessarily an integer.

Moments of Pareto I–IV distributions (case μ = 0)






E
[
X
]


{\displaystyle E[X]}


Condition




E
[

X

δ


]


{\displaystyle E[X^{\delta }]}


Condition


Type I







σ
α


α
−
1





{\displaystyle {\frac {\sigma \alpha }{\alpha -1}}}






α
>
1


{\displaystyle \alpha >1}










σ

δ


α


α
−
δ





{\displaystyle {\frac {\sigma ^{\delta }\alpha }{\alpha -\delta }}}






δ
<
α


{\displaystyle \delta <\alpha }




Type II






σ

α
−
1





{\displaystyle {\frac {\sigma }{\alpha -1}}}






α
>
1


{\displaystyle \alpha >1}










σ

δ


Γ
(
α
−
δ
)
Γ
(
1
+
δ
)


Γ
(
α
)





{\displaystyle {\frac {\sigma ^{\delta }\Gamma (\alpha -\delta )\Gamma (1+\delta )}{\Gamma (\alpha )}}}






−
1
<
δ
<
α


{\displaystyle -1<\delta <\alpha }




Type III




σ
Γ
(
1
−
γ
)
Γ
(
1
+
γ
)


{\displaystyle \sigma \Gamma (1-\gamma )\Gamma (1+\gamma )}






−
1
<
γ
<
1


{\displaystyle -1<\gamma <1}







σ

δ


Γ
(
1
−
γ
δ
)
Γ
(
1
+
γ
δ
)


{\displaystyle \sigma ^{\delta }\Gamma (1-\gamma \delta )\Gamma (1+\gamma \delta )}






−

γ

−
1


<
δ
<

γ

−
1




{\displaystyle -\gamma ^{-1}<\delta <\gamma ^{-1}}




Type IV







σ
Γ
(
α
−
γ
)
Γ
(
1
+
γ
)


Γ
(
α
)





{\displaystyle {\frac {\sigma \Gamma (\alpha -\gamma )\Gamma (1+\gamma )}{\Gamma (\alpha )}}}






−
1
<
γ
<
α


{\displaystyle -1<\gamma <\alpha }










σ

δ


Γ
(
α
−
γ
δ
)
Γ
(
1
+
γ
δ
)


Γ
(
α
)





{\displaystyle {\frac {\sigma ^{\delta }\Gamma (\alpha -\gamma \delta )\Gamma (1+\gamma \delta )}{\Gamma (\alpha )}}}






−

γ

−
1


<
δ
<
α

/

γ


{\displaystyle -\gamma ^{-1}<\delta <\alpha /\gamma }




Feller–Pareto distribution[edit]
Feller[3][5] defines a Pareto variable by transformation U = Y−1 − 1 of a beta random variable Y, whose probability density function is





f
(
y
)
=




y


γ

1


−
1


(
1
−
y

)


γ

2


−
1




B
(

γ

1


,

γ

2


)



,

0
<
y
<
1
;

γ

1


,

γ

2


>
0
,


{\displaystyle f(y)={\frac {y^{\gamma _{1}-1}(1-y)^{\gamma _{2}-1}}{B(\gamma _{1},\gamma _{2})}},\qquad 0<y<1;\gamma _{1},\gamma _{2}>0,}



where B( ) is the beta function. If





W
=
μ
+
σ
(

Y

−
1


−
1

)

γ


,

σ
>
0
,
γ
>
0
,


{\displaystyle W=\mu +\sigma (Y^{-1}-1)^{\gamma },\qquad \sigma >0,\gamma >0,}



then W has a Feller–Pareto distribution FP(μ, σ, γ, γ1, γ2).[1]
If 




U

1


∼
Γ
(

δ

1


,
1
)


{\displaystyle U_{1}\sim \Gamma (\delta _{1},1)}

 and 




U

2


∼
Γ
(

δ

2


,
1
)


{\displaystyle U_{2}\sim \Gamma (\delta _{2},1)}

 are independent Gamma variables, another construction of a Feller–Pareto (FP) variable is[7]





W
=
μ
+
σ


(



U

1



U

2




)


γ




{\displaystyle W=\mu +\sigma \left({\frac {U_{1}}{U_{2}}}\right)^{\gamma }}



and we write W ~ FP(μ, σ, γ, δ1, δ2). Special cases of the Feller–Pareto distribution are





F
P
(
σ
,
σ
,
1
,
1
,
α
)
=
P
(
I
)
(
σ
,
α
)


{\displaystyle FP(\sigma ,\sigma ,1,1,\alpha )=P(I)(\sigma ,\alpha )}






F
P
(
μ
,
σ
,
1
,
1
,
α
)
=
P
(
I
I
)
(
μ
,
σ
,
α
)


{\displaystyle FP(\mu ,\sigma ,1,1,\alpha )=P(II)(\mu ,\sigma ,\alpha )}






F
P
(
μ
,
σ
,
γ
,
1
,
1
)
=
P
(
I
I
I
)
(
μ
,
σ
,
γ
)


{\displaystyle FP(\mu ,\sigma ,\gamma ,1,1)=P(III)(\mu ,\sigma ,\gamma )}






F
P
(
μ
,
σ
,
γ
,
1
,
α
)
=
P
(
I
V
)
(
μ
,
σ
,
γ
,
α
)
.


{\displaystyle FP(\mu ,\sigma ,\gamma ,1,\alpha )=P(IV)(\mu ,\sigma ,\gamma ,\alpha ).}



Applications[edit]
Pareto originally used this distribution to describe the allocation of wealth among individuals since it seemed to show rather well the way that a larger portion of the wealth of any society is owned by a smaller percentage of the people in that society. He also used it to describe distribution of income.[8] This idea is sometimes expressed more simply as the Pareto principle or the "80-20 rule" which says that 20% of the population controls 80% of the wealth.[9] However, the 80-20 rule corresponds to a particular value of α, and in fact, Pareto's data on British income taxes in his Cours d'économie politique indicates that about 30% of the population had about 70% of the income. The probability density function (PDF) graph at the beginning of this article shows that the "probability" or fraction of the population that owns a small amount of wealth per person is rather high, and then decreases steadily as wealth increases. (Note that the Pareto distribution is not realistic for wealth for the lower end. In fact, net worth may even be negative.) This distribution is not limited to describing wealth or income, but to many situations in which an equilibrium is found in the distribution of the "small" to the "large". The following examples are sometimes seen as approximately Pareto-distributed:




Fitted cumulative Pareto (Lomax) distribution to maximum one-day rainfalls using CumFreq, see also distribution fitting



The sizes of human settlements (few cities, many hamlets/villages)[10]
File size distribution of Internet traffic which uses the TCP protocol (many smaller files, few larger ones)[10]
Hard disk drive error rates[11]
Clusters of Bose–Einstein condensate near absolute zero[12]
The values of oil reserves in oil fields (a few large fields, many small fields)[10]
The length distribution in jobs assigned supercomputers (a few large ones, many small ones)[citation needed]
The standardized price returns on individual stocks [10]
Sizes of sand particles [10]
Sizes of meteorites
Numbers of species per genus (There is subjectivity involved: The tendency to divide a genus into two or more increases with the number of species in it)[citation needed]
Areas burnt in forest fires
Severity of large casualty losses for certain lines of business such as general liability, commercial auto, and workers compensation.[13][14]
In hydrology the Pareto distribution is applied to extreme events such as annually maximum one-day rainfalls and river discharges. The blue picture illustrates an example of fitting the Pareto distribution to ranked annually maximum one-day rainfalls showing also the 90% confidence belt based on the binomial distribution. The rainfall data are represented by plotting positions as part of the cumulative frequency analysis.

Relation to other distributions[edit]
Relation to the exponential distribution[edit]
The Pareto distribution is related to the exponential distribution as follows. If X is Pareto-distributed with minimum xm and index α, then





Y
=
log
⁡

(


X

x


m





)



{\displaystyle Y=\log \left({\frac {X}{x_{\mathrm {m} }}}\right)}



is exponentially distributed with rate parameter α. Equivalently, if Y is exponentially distributed with rate α, then






x


m




e

Y




{\displaystyle x_{\mathrm {m} }e^{Y}}



is Pareto-distributed with minimum xm and index α.
This can be shown using the standard change of variable techniques:





Pr
(
Y
<
y
)
=
Pr

(
log
⁡

(


X

x


m





)

<
y
)

=
Pr
(
X
<

x


m




e

y


)
=
1
−


(



x


m





x


m




e

y





)


α


=
1
−

e

−
α
y


.


{\displaystyle \Pr(Y<y)=\Pr \left(\log \left({\frac {X}{x_{\mathrm {m} }}}\right)<y\right)=\Pr(X<x_{\mathrm {m} }e^{y})=1-\left({\frac {x_{\mathrm {m} }}{x_{\mathrm {m} }e^{y}}}\right)^{\alpha }=1-e^{-\alpha y}.}



The last expression is the cumulative distribution function of an exponential distribution with rate α.
Relation to the log-normal distribution[edit]
Note that the Pareto distribution and log-normal distribution are alternative distributions for describing the same types of quantities. One of the connections between the two is that they are both the distributions of the exponential of random variables distributed according to other common distributions, respectively the exponential distribution and normal distribution.[citation needed]
Relation to the generalized Pareto distribution[edit]
The Pareto distribution is a special case of the generalized Pareto distribution, which is a family of distributions of similar form, but containing an extra parameter in such a way that the support of the distribution is either bounded below (at a variable point), or bounded both above and below (where both are variable), with the Lomax distribution as a special case. This family also contains both the unshifted and shifted exponential distributions.
The Pareto distribution with scale 




x

m




{\displaystyle x_{m}}

 and shape 



α


{\displaystyle \alpha }

 is equivalent to the generalized Pareto distribution with location 



μ
=

x

m




{\displaystyle \mu =x_{m}}

, scale 



σ
=

x

m



/

α


{\displaystyle \sigma =x_{m}/\alpha }

 and shape 



ξ
=
1

/

α


{\displaystyle \xi =1/\alpha }

. Vice versa one can get the Pareto distribution from the GPD by 




x

m


=
σ

/

ξ


{\displaystyle x_{m}=\sigma /\xi }

 and 



α
=
1

/

ξ


{\displaystyle \alpha =1/\xi }

.
Relation to Zipf's law[edit]
Pareto distributions are continuous probability distributions. Zipf's law, also sometimes called the zeta distribution, may be thought of as a discrete counterpart of the Pareto distribution.
Relation to the "Pareto principle"[edit]
The "80-20 law", according to which 20% of all people receive 80% of all income, and 20% of the most affluent 20% receive 80% of that 80%, and so on, holds precisely when the Pareto index is α = log4(5) = log(5)/log(4), approximately 1.161. This result can be derived from the Lorenz curve formula given below. Moreover, the following have been shown[15] to be mathematically equivalent:

Income is distributed according to a Pareto distribution with index α > 1.
There is some number 0 ≤ p ≤ 1/2 such that 100p % of all people receive 100(1 − p) % of all income, and similarly for every real (not necessarily integer) n > 0, 100pn % of all people receive 100(1 − p)n percentage of all income.

This does not apply only to income, but also to wealth, or to anything else that can be modeled by this distribution.
This excludes Pareto distributions in which 0 < α ≤ 1, which, as noted above, have infinite expected value, and so cannot reasonably model income distribution.
Lorenz curve and Gini coefficient[edit]




Lorenz curves for a number of Pareto distributions. The case α = ∞ corresponds to perfectly equal distribution (G = 0) and the α = 1 line corresponds to complete inequality (G = 1)


The Lorenz curve is often used to characterize income and wealth distributions. For any distribution, the Lorenz curve L(F) is written in terms of the PDF f or the CDF F as





L
(
F
)
=




∫


x


m





x
(
F
)


x
f
(
x
)

d
x



∫


x


m





∞


x
f
(
x
)

d
x



=




∫

0


F


x
(

F
′

)

d

F
′




∫

0


1


x
(

F
′

)

d

F
′






{\displaystyle L(F)={\frac {\int _{x_{\mathrm {m} }}^{x(F)}xf(x)\,dx}{\int _{x_{\mathrm {m} }}^{\infty }xf(x)\,dx}}={\frac {\int _{0}^{F}x(F')\,dF'}{\int _{0}^{1}x(F')\,dF'}}}



where x(F) is the inverse of the CDF. For the Pareto distribution,





x
(
F
)
=



x


m




(
1
−
F

)


1
α








{\displaystyle x(F)={\frac {x_{\mathrm {m} }}{(1-F)^{\frac {1}{\alpha }}}}}



and the Lorenz curve is calculated to be





L
(
F
)
=
1
−
(
1
−
F

)

1
−


1
α




,


{\displaystyle L(F)=1-(1-F)^{1-{\frac {1}{\alpha }}},}



For 



0
<
α
≤
1


{\displaystyle 0<\alpha \leq 1}

 the denominator is infinite, yielding L=0. Examples of the Lorenz curve for a number of Pareto distributions are shown in the graph on the right.
According to Oxfam (2016) the richest 62 people have as much wealth as the poorest half of the world's population.[16] We can estimate the Pareto index that would apply to this situation. Letting ε equal 



62

/

(
7
×

10

9


)


{\displaystyle 62/(7\times 10^{9})}

 we have:





L
(
1

/

2
)
=
1
−
L
(
1
−
ϵ
)


{\displaystyle L(1/2)=1-L(1-\epsilon )}



or





1
−
(
1

/

2

)

1
−


1
α




=

ϵ

1
−


1
α






{\displaystyle 1-(1/2)^{1-{\frac {1}{\alpha }}}=\epsilon ^{1-{\frac {1}{\alpha }}}}



The solution is that α equals about 1.15, and about 9% of the wealth is owned by each of the two groups. But actually the poorest 69% of the world adult population owns only about 3% of the wealth.[17]
The Gini coefficient is a measure of the deviation of the Lorenz curve from the equidistribution line which is a line connecting [0, 0] and [1, 1], which is shown in black (α = ∞) in the Lorenz plot on the right. Specifically, the Gini coefficient is twice the area between the Lorenz curve and the equidistribution line. The Gini coefficient for the Pareto distribution is then calculated (for 



α
≥
1


{\displaystyle \alpha \geq 1}

) to be





G
=
1
−
2

(

∫

0


1


L
(
F
)
d
F
)

=


1

2
α
−
1





{\displaystyle G=1-2\left(\int _{0}^{1}L(F)dF\right)={\frac {1}{2\alpha -1}}}



(see Aaberge 2005).
Parameter estimation[edit]
The likelihood function for the Pareto distribution parameters α and xm, given a sample x = (x1, x2, ..., xn), is





L
(
α
,

x


m



)
=

∏

i
=
1


n


α



x


m



α



x

i


α
+
1




=

α

n



x


m



n
α



∏

i
=
1


n




1

x

i


α
+
1




.


{\displaystyle L(\alpha ,x_{\mathrm {m} })=\prod _{i=1}^{n}\alpha {\frac {x_{\mathrm {m} }^{\alpha }}{x_{i}^{\alpha +1}}}=\alpha ^{n}x_{\mathrm {m} }^{n\alpha }\prod _{i=1}^{n}{\frac {1}{x_{i}^{\alpha +1}}}.}



Therefore, the logarithmic likelihood function is





ℓ
(
α
,

x


m



)
=
n
ln
⁡
α
+
n
α
ln
⁡

x


m



−
(
α
+
1
)

∑

i
=
1


n


ln
⁡

x

i


.


{\displaystyle \ell (\alpha ,x_{\mathrm {m} })=n\ln \alpha +n\alpha \ln x_{\mathrm {m} }-(\alpha +1)\sum _{i=1}^{n}\ln x_{i}.}



It can be seen that 



ℓ
(
α
,

x


m



)


{\displaystyle \ell (\alpha ,x_{\mathrm {m} })}

 is monotonically increasing with xm, that is, the greater the value of xm, the greater the value of the likelihood function. Hence, since x ≥ xm, we conclude that









x
^





m



=

min

i




x

i



.


{\displaystyle {\widehat {x}}_{\mathrm {m} }=\min _{i}{x_{i}}.}



To find the estimator for α, we compute the corresponding partial derivative and determine where it is zero:








∂
ℓ


∂
α



=


n
α


+
n
ln
⁡

x


m



−

∑

i
=
1


n


ln
⁡

x

i


=
0.


{\displaystyle {\frac {\partial \ell }{\partial \alpha }}={\frac {n}{\alpha }}+n\ln x_{\mathrm {m} }-\sum _{i=1}^{n}\ln x_{i}=0.}



Thus the maximum likelihood estimator for α is:








α
^



=


n


∑

i



(
ln
⁡

x

i


−
ln
⁡




x
^





m



)




.


{\displaystyle {\widehat {\alpha }}={\frac {n}{\sum _{i}\left(\ln x_{i}-\ln {\widehat {x}}_{\mathrm {m} }\right)}}.}



The expected statistical error is:[18]





σ
=




α
^



n



.


{\displaystyle \sigma ={\frac {\widehat {\alpha }}{\sqrt {n}}}.}



Malik (1970)[19] gives the exact joint distribution of 



(




x
^





m



,



α
^



)


{\displaystyle ({\hat {x}}_{\mathrm {m} },{\hat {\alpha }})}

. In particular, 







x
^





m





{\displaystyle {\hat {x}}_{\mathrm {m} }}

 and 






α
^





{\displaystyle {\hat {\alpha }}}

 are independent and 







x
^





m





{\displaystyle {\hat {x}}_{\mathrm {m} }}

 is Pareto with scale parameter xm and shape parameter nα, whereas 






α
^





{\displaystyle {\hat {\alpha }}}

 has an Inverse-gamma distribution with shape and scale parameters n−1 and nα, respectively.
Graphical representation[edit]
The characteristic curved 'long tail' distribution when plotted on a linear scale, masks the underlying simplicity of the function when plotted on a log-log graph, which then takes the form of a straight line with negative gradient: It follows from the formula for the probability density function that for x ≥ xm,





log
⁡

f

X


(
x
)
=
log
⁡

(
α



x


m



α



x

α
+
1




)

=
log
⁡
(
α

x


m



α


)
−
(
α
+
1
)
log
⁡
x
.


{\displaystyle \log f_{X}(x)=\log \left(\alpha {\frac {x_{\mathrm {m} }^{\alpha }}{x^{\alpha +1}}}\right)=\log(\alpha x_{\mathrm {m} }^{\alpha })-(\alpha +1)\log x.}



Since α is positive, the gradient −(α+1) is negative.
Random sample generation[edit]
Random samples can be generated using inverse transform sampling. Given a random variate U drawn from the uniform distribution on the unit interval (0, 1], the variate T given by





T
=



x


m




U


1
α







{\displaystyle T={\frac {x_{\mathrm {m} }}{U^{\frac {1}{\alpha }}}}}



is Pareto-distributed.[20] If U is uniformly distributed on [0, 1), it can be exchanged with (1 − U).
Variants[edit]
Bounded Pareto distribution[edit]
See also: Truncated distribution

Bounded Pareto

Parameters





L
>
0


{\displaystyle L>0}

 location (real)




H
>
L


{\displaystyle H>L}

 location (real)




α
>
0


{\displaystyle \alpha >0}

 shape (real)


Support




L
⩽
x
⩽
H


{\displaystyle L\leqslant x\leqslant H}




PDF







α

L

α



x

−
α
−
1




1
−


(


L
H


)


α







{\displaystyle {\frac {\alpha L^{\alpha }x^{-\alpha -1}}{1-\left({\frac {L}{H}}\right)^{\alpha }}}}




CDF







1
−

L

α



x

−
α




1
−


(


L
H


)


α







{\displaystyle {\frac {1-L^{\alpha }x^{-\alpha }}{1-\left({\frac {L}{H}}\right)^{\alpha }}}}




Mean







L

α



1
−


(


L
H


)


α





⋅

(


α

α
−
1



)

⋅

(


1

L

α
−
1




−


1

H

α
−
1




)

,
α
≠
1


{\displaystyle {\frac {L^{\alpha }}{1-\left({\frac {L}{H}}\right)^{\alpha }}}\cdot \left({\frac {\alpha }{\alpha -1}}\right)\cdot \left({\frac {1}{L^{\alpha -1}}}-{\frac {1}{H^{\alpha -1}}}\right),\alpha \neq 1}




Median




L


(
1
−


1
2



(
1
−


(


L
H


)


α


)

)


−


1
α






{\displaystyle L\left(1-{\frac {1}{2}}\left(1-\left({\frac {L}{H}}\right)^{\alpha }\right)\right)^{-{\frac {1}{\alpha }}}}




Variance







L

α



1
−


(


L
H


)


α





⋅

(


α

α
−
2



)

⋅

(


1

L

α
−
2




−


1

H

α
−
2




)

,
α
≠
2


{\displaystyle {\frac {L^{\alpha }}{1-\left({\frac {L}{H}}\right)^{\alpha }}}\cdot \left({\frac {\alpha }{\alpha -2}}\right)\cdot \left({\frac {1}{L^{\alpha -2}}}-{\frac {1}{H^{\alpha -2}}}\right),\alpha \neq 2}

 (this is the second moment, NOT the variance)[citation needed]


Skewness








L

α



1
−


(


L
H


)


α





⋅



α
∗
(

L

k
−
α


−

H

k
−
α


)


(
α
−
k
)



,
α
≠
j


{\displaystyle {\frac {L^{\alpha }}{1-\left({\frac {L}{H}}\right)^{\alpha }}}\cdot {\frac {\alpha *(L^{k-\alpha }-H^{k-\alpha })}{(\alpha -k)}},\alpha \neq j}


(this is a formula for the kth moment, NOT the skewness)[citation needed]


The bounded (or truncated) Pareto distribution has three parameters α, L and H. As in the standard Pareto distribution α determines the shape. L denotes the minimal value, and H denotes the maximal value. (The variance in the table on the right should be interpreted as the second moment).
The probability density function is








α

L

α



x

−
α
−
1




1
−


(


L
H


)


α







{\displaystyle {\frac {\alpha L^{\alpha }x^{-\alpha -1}}{1-\left({\frac {L}{H}}\right)^{\alpha }}}}



where L ≤ x ≤ H, and α > 0.
Generating bounded Pareto random variables[edit]
If U is uniformly distributed on (0, 1), then applying inverse-transform method [21]





U
=



1
−

L

α



x

−
α




1
−
(


L
H



)

α







{\displaystyle U={\frac {1-L^{\alpha }x^{-\alpha }}{1-({\frac {L}{H}})^{\alpha }}}}






x
=


(
−



U

H

α


−
U

L

α


−

H

α





H

α



L

α





)


−


1
α






{\displaystyle x=\left(-{\frac {UH^{\alpha }-UL^{\alpha }-H^{\alpha }}{H^{\alpha }L^{\alpha }}}\right)^{-{\frac {1}{\alpha }}}}



is a bounded Pareto-distributed.[citation needed]

Symmetric Pareto distribution[edit]
The symmetric Pareto distribution can be defined by the probability density function:[22]





f
(
x
;
α
,

x


m



)
=


{






1
2



α

x


m



α



|

x


|


−
α
−
1





|

x

|

>

x


m







0



otherwise

.








{\displaystyle f(x;\alpha ,x_{\mathrm {m} })={\begin{cases}{\tfrac {1}{2}}\alpha x_{\mathrm {m} }^{\alpha }|x|^{-\alpha -1}&|x|>x_{\mathrm {m} }\\0&{\text{otherwise}}.\end{cases}}}



It has a similar shape to a Pareto distribution for x > xm and is mirror symmetric about the vertical axis.
Multivariate Pareto distribution[edit]
The univariate Pareto distribution has been extended a multivariate Pareto distribution.[23]
See also[edit]

Bradford's law
Pareto analysis
Pareto efficiency
Pareto interpolation
Power law probability distributions
Sturgeon's law
Traffic generation model

Notes[edit]


^ a b c d Barry C. Arnold (1983). Pareto Distributions. International Co-operative Publishing House. ISBN 0-89974-012-X. 
^ a b Johnson NL, Kotz S, Balakrishnan N (1994) Continuous univariate distributions Vol 1. Wiley Series in Probability and Statistics.
^ a b c d Johnson, Kotz, and Balakrishnan (1994), (20.4).
^ Christian Kleiber & Samuel Kotz (2003). Statistical Size Distributions in Economics and Actuarial Sciences. Wiley. ISBN 0-471-15064-9. 
^ a b Feller, W. (1971). An Introduction to Probability Theory and its Applications. II (2nd ed.). New York: Wiley. p. 50.  "The densities (4.3) are sometimes called after the economist Pareto. It was thought (rather naïvely from a modern statistical standpoint) that income distributions should have a tail with a density ~ Ax−α as x → ∞."
^ Lomax, K. S. (1954). "Business failures. Another example of the analysis of failure data". Journal of the American Statistical Association. 49: 847–52. doi:10.1080/01621459.1954.10501239. 
^ Chotikapanich, Duangkamon. "Chapter 7: Pareto and Generalized Pareto Distributions". Modeling Income Distributions and Lorenz Curves. pp. 121–22. 
^ Pareto, Vilfredo, Cours d'Économie Politique: Nouvelle édition par G.-H. Bousquet et G. Busino, Librairie Droz, Geneva, 1964, pp. 299–345.
^ For a two-quantile population, where approximately 18% of the population owns 82% of the wealth, the Theil index takes the value 1.
^ a b c d e Reed, William J.; et al. (2004). "The Double Pareto-Lognormal Distribution – A New Parametric Model for Size Distributions". Communications in Statistics – Theory and Methods. 33 (8): 1733–53. CiteSeerX 10.1.1.70.4555. doi:10.1081/sta-120037438. 
^ Schroeder, Bianca; Damouras, Sotirios; Gill, Phillipa (2010-02-24). "Understanding latent sector error and how to protect against them" (PDF). 8th Usenix Conference on File and Storage Technologies (FAST 2010). Retrieved 2010-09-10. We experimented with 5 different distributions (Geometric,Weibull, Rayleigh, Pareto, and Lognormal), that are commonly used in the context of system reliability, and evaluated their ﬁt through the total squared differences between the actual and hypothesized frequencies (χ2 statistic). We found consistently across all models that the geometric distribution is a poor ﬁt, while the Pareto distribution provides the best ﬁt. 
^ Yuji Ijiri; Simon, Herbert A. (May 1975). "Some Distributions Associated with Bose–Einstein Statistics". Proc. Natl. Acad. Sci. USA. 72 (5): 1654–57. PMC 432601. PMID 16578724.  |access-date= requires |url= (help)
^ Kleiber and Kotz (2003): p. 94.
^ Seal, H. (1980). "Survival probabilities based on Pareto claim distributions". ASTIN Bulletin. 11: 61–71. 
^ Hardy, Michael (2010). "Pareto's Law". Mathematical Intelligencer. 32 (3): 38–43. doi:10.1007/s00283-010-9159-2. 
^ "62 people own the same as half the world, reveals Oxfam Davos report". Oxfam. Jan 2016. 
^ "Global Wealth Report 2013". Credit Suisse. Oct 2013. p. 22. 
^ M. E. J. Newman (2005). "Power laws, Pareto distributions and Zipf's law". Contemporary Physics. 46 (5): 323–51. arXiv:cond-mat/0412004. Bibcode:2005ConPh..46..323N. doi:10.1080/00107510500052444. 
^ H. J. Malik (1970). "Estimation of the Parameters of the Pareto Distribution". Metrika. 15. 
^ Tanizaki, Hisashi (2004). Computational Methods in Statistics and Econometrics. CRC Press. p. 133. 
^ http://www.cs.bgu.ac.il/~mps042/invtransnote.htm
^ Grabchak, M. & Samorodnitsky, D. "Do Financial Returns Have Finite or Infinite Variance? A Paradox and an Explanation" (PDF). pp. 7–8. 
^ Rootzén, Holger; Tajvidi, Nader (2006). "Multivariate generalized Pareto distributions". Bernoulli. 12 (5): 917–30. doi:10.3150/bj/1161614952. 


References[edit]

M. O. Lorenz (1905). "Methods of measuring the concentration of wealth". Publications of the American Statistical Association. 9 (70): 209–19. Bibcode:1905PAmSA...9..209L. doi:10.2307/2276207. 
Pareto V (1965) "La Courbe de la Repartition de la Richesse" (Originally published in 1896). In: Busino G, editor. Oevres Completes de Vilfredo Pareto. Geneva: Librairie Droz. pp. 1–5.
Pareto, V. (1895). La legge della domanda. Giornale degli Economisti, 10, 59–68. English translation in Rivista di Politica Economica, 87 (1997), 691–700.
Pareto, V. (1897). Cours d'économie politique. Lausanne: Ed. Rouge.

External links[edit]

Gini's Nuclear Family / Rolf Aabergé. – In: International Conference to Honor Two Eminent Social Scientists, May, 2005 – PDF
Hazewinkel, Michiel, ed. (2001), "Pareto distribution", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
syntraf1.c is a C program to generate synthetic packet traffic with bounded Pareto burst size and exponential interburst time.
"Self-Similarity in World Wide Web Traffic: Evidence and Possible Causes" /Mark E. Crovella and Azer Bestavros
Weisstein, Eric W. "Pareto distribution". MathWorld. 







v
t
e


Probability distributions







List






Discrete univariate
with finite support



Benford
Bernoulli
beta-binomial
binomial
categorical
hypergeometric
Poisson binomial
Rademacher
discrete uniform
Zipf
Zipf–Mandelbrot








Discrete univariate
with infinite support



beta negative binomial
Borel
Conway–Maxwell–Poisson
discrete phase-type
Delaporte
extended negative binomial
Gauss–Kuzmin
geometric
logarithmic
negative binomial
parabolic fractal
Poisson
Skellam
Yule–Simon
zeta








Continuous univariate
supported on a bounded interval



arcsine
ARGUS
Balding–Nichols
Bates
beta
beta rectangular
Irwin–Hall
Kumaraswamy
logit-normal
noncentral beta
raised cosine
reciprocal
triangular
U-quadratic
uniform
Wigner semicircle








Continuous univariate
supported on a semi-infinite interval



Benini
Benktander 1st kind
Benktander 2nd kind
beta prime
Burr
chi-squared
chi
Dagum
Davis
exponential-logarithmic
Erlang
exponential
F
folded normal
Flory–Schulz
Fréchet
gamma
gamma/Gompertz
generalized inverse Gaussian
Gompertz
half-logistic
half-normal
Hotelling's T-squared
hyper-Erlang
hyperexponential
hypoexponential
inverse chi-squared

scaled inverse chi-squared


inverse Gaussian
inverse gamma
Kolmogorov
Lévy
log-Cauchy
log-Laplace
log-logistic
log-normal
Lomax
matrix-exponential
Maxwell–Boltzmann
Maxwell–Jüttner
Mittag-Leffler
Nakagami
noncentral chi-squared
Pareto
phase-type
poly-Weibull
Rayleigh
relativistic Breit–Wigner
Rice
shifted Gompertz
truncated normal
type-2 Gumbel
Weibull

Discrete Weibull


Wilks's lambda








Continuous univariate
supported on the whole real line



Cauchy
exponential power
Fisher's z
Gaussian q
generalized normal
generalized hyperbolic
geometric stable
Gumbel
Holtsmark
hyperbolic secant
Johnson's SU
Landau
Laplace
asymmetric Laplace
logistic
noncentral t
normal (Gaussian)
normal-inverse Gaussian
skew normal
slash
stable
Student's t
type-1 Gumbel
Tracy–Widom
variance-gamma
Voigt








Continuous univariate
with support whose type varies



generalized extreme value
generalized Pareto
Marchenko–Pastur
q-exponential
q-Gaussian
q-Weibull
shifted log-logistic
Tukey lambda








Mixed continuous-discrete univariate



rectified Gaussian








Multivariate (joint)



Discrete
Ewens
multinomial
Dirichlet-multinomial
negative multinomial
Continuous
Dirichlet
generalized Dirichlet
multivariate normal
multivariate stable
multivariate t
normal-inverse-gamma
normal-gamma
Matrix-valued
inverse matrix gamma
inverse-Wishart
matrix normal
matrix t
matrix gamma
normal-inverse-Wishart
normal-Wishart
Wishart








Directional



Univariate (circular) directional
Circular uniform
univariate von Mises
wrapped normal
wrapped Cauchy
wrapped exponential
wrapped asymmetric Laplace
wrapped Lévy
Bivariate (spherical)
Kent
Bivariate (toroidal)
bivariate von Mises
Multivariate
von Mises–Fisher
Bingham








Degenerate and singular



Degenerate
Dirac delta function
Singular
Cantor








Families



Circular
compound Poisson
elliptical
exponential
natural exponential
location–scale
maximum entropy
mixture
Pearson
Tweedie
wrapped












v
t
e


Some common univariate probability distributions






Continuous



beta
Cauchy
chi-squared
exponential
F
gamma
Laplace
log-normal
normal
Pareto
Student's t
uniform
Weibull








Discrete



Bernoulli
binomial
discrete uniform
geometric
hypergeometric
negative binomial
Poisson









List of probability distributions







Authority control



GND: 4632300-4









 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Pareto_distribution&oldid=774080438"					
Categories: Actuarial scienceContinuous distributionsPower lawsProbability distributions with non-finite varianceExponential family distributionsHidden categories: Pages using citations with accessdate and no URLPages using deprecated image syntaxAll articles with unsourced statementsArticles with unsourced statements from February 2012Articles with unsourced statements from December 2010Articles with unsourced statements from November 2012Articles with unsourced statements from February 2011Wikipedia articles with GND identifiers 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


العربيةDanskDeutschEspañolEuskaraفارسیFrançaisGalego한국어ItalianoLietuviųMagyarNederlands日本語PolskiРусскийSimple EnglishSlovenščinaSuomiTürkçeУкраїнська中文 
Edit links 





 This page was last modified on 6 April 2017, at 04:45.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 



(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.756","walltime":"0.993","ppvisitednodes":{"value":3297,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":117331,"limit":2097152},"templateargumentsize":{"value":7079,"limit":2097152},"expansiondepth":{"value":15,"limit":40},"expensivefunctioncount":{"value":4,"limit":500},"entityaccesscount":{"value":1,"limit":400},"timingprofile":["100.00%  503.550      1 -total"," 30.47%  153.456      1 Template:Reflist"," 29.69%  149.518      7 Template:Citation_needed"," 27.10%  136.482      7 Template:Fix"," 23.70%  119.332      2 Template:Probability_distribution"," 22.46%  113.073      2 Template:Infobox"," 15.39%   77.501      7 Template:Delink"," 11.35%   57.135     10 Template:Cite_journal"," 10.88%   54.775      5 Template:Cite_book"," 10.20%   51.353     14 Template:Category_handler"]},"scribunto":{"limitreport-timeusage":{"value":"0.285","limit":"10.000"},"limitreport-memusage":{"value":4812660,"limit":52428800}},"cachereport":{"origin":"mw1233","timestamp":"20170406044521","ttl":2592000,"transientcontent":false}}});});(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":105,"wgHostname":"mw1254"});});








Student's t-distribution - Wikipedia
document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );
(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Student's_t-distribution","wgTitle":"Student's t-distribution","wgCurRevisionId":770634330,"wgRevisionId":770634330,"wgArticleId":105375,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Webarchive template wayback links","Pages using ISBN magic links","Pages using deprecated image syntax","Wikipedia articles needing clarification from November 2012","All articles with unsourced statements","Articles with unsourced statements from July 2011","Articles with unsourced statements from November 2010","Articles with unsourced statements from June 2015","Continuous distributions","Special functions","Normal distribution","Compound probability distributions","Probability distributions with non-finite variance","Infinitely divisible probability distributions","Location-scale family probability distributions"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Student's_t-distribution","wgRelevantArticleId":105375,"wgRequestId":"WOM@FQpAIC8AAFu@3E8AAAGB","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":false,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesBetaFeatureEnabled":false,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q576072","wgCentralAuthMobileDomain":false,"wgVisualEditorToolbarScrollOffset":0,"wgEditSubmitButtonLabelPublish":false});mw.loader.state({"ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"loading","user.tokens":"loading","ext.math.styles":"ready","ext.cite.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["ext.math.scripts","ext.cite.a11y","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.legacy.wikibits","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.gadget.featured-articles-links","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});



















 






Student's t-distribution

From Wikipedia, the free encyclopedia


					Jump to:					navigation, 					search

This article is about the mathematics of Student's t-distribution. For its uses in statistics, see Student's t-test.

Student's t


Probability density function




Cumulative distribution function



Parameters




ν
>
0


{\displaystyle \nu >0}

 degrees of freedom (real)


Support
x ∈ (−∞; +∞)


PDF








Γ

(



ν
+
1

2


)





ν
π



Γ

(


ν
2


)






(
1
+



x

2


ν


)


−



ν
+
1

2








{\displaystyle \textstyle {\frac {\Gamma \left({\frac {\nu +1}{2}}\right)}{{\sqrt {\nu \pi }}\,\Gamma \left({\frac {\nu }{2}}\right)}}\left(1+{\frac {x^{2}}{\nu }}\right)^{-{\frac {\nu +1}{2}}}\!}




CDF











1
2


+
x
Γ

(



ν
+
1

2


)

×










2



F

1



(


1
2


,



ν
+
1

2


;


3
2


;
−



x

2


ν


)





π
ν



Γ

(


ν
2


)










{\displaystyle {\begin{matrix}{\frac {1}{2}}+x\Gamma \left({\frac {\nu +1}{2}}\right)\times \\[0.5em]{\frac {\,_{2}F_{1}\left({\frac {1}{2}},{\frac {\nu +1}{2}};{\frac {3}{2}};-{\frac {x^{2}}{\nu }}\right)}{{\sqrt {\pi \nu }}\,\Gamma \left({\frac {\nu }{2}}\right)}}\end{matrix}}}


where 2F1 is the hypergeometric function


Mean
0 for 



ν
>
1


{\displaystyle \nu >1}

, otherwise undefined


Median
0


Mode
0


Variance







ν

ν
−
2






{\displaystyle \textstyle {\frac {\nu }{\nu -2}}}

 for 



ν
>
2


{\displaystyle \nu >2}

, ∞ for 



1
<
ν
≤
2


{\displaystyle 1<\nu \leq 2}

, otherwise undefined


Skewness
0 for 



ν
>
3


{\displaystyle \nu >3}

, otherwise undefined


Ex. kurtosis







6

ν
−
4






{\displaystyle \textstyle {\frac {6}{\nu -4}}}

 for 



ν
>
4


{\displaystyle \nu >4}

, ∞ for 



2
<
ν
≤
4


{\displaystyle 2<\nu \leq 4}

, otherwise undefined


Entropy












ν
+
1

2



[
ψ

(



1
+
ν

2


)

−
ψ

(


ν
2


)

]





+
ln
⁡


[


ν


B

(


ν
2


,


1
2


)

]






(nats)









{\displaystyle {\begin{matrix}{\frac {\nu +1}{2}}\left[\psi \left({\frac {1+\nu }{2}}\right)-\psi \left({\frac {\nu }{2}}\right)\right]\\[0.5em]+\ln {\left[{\sqrt {\nu }}B\left({\frac {\nu }{2}},{\frac {1}{2}}\right)\right]}\,{\scriptstyle {\text{(nats)}}}\end{matrix}}}



ψ: digamma function,
B: beta function




MGF
undefined


CF










K

ν

/

2



(


ν



|

t

|

)

⋅


(


ν



|

t

|

)


ν

/

2




Γ
(
ν

/

2
)

2

ν

/

2
−
1








{\displaystyle \textstyle {\frac {K_{\nu /2}\left({\sqrt {\nu }}|t|\right)\cdot \left({\sqrt {\nu }}|t|\right)^{\nu /2}}{\Gamma (\nu /2)2^{\nu /2-1}}}}

 for 



ν
>
0


{\displaystyle \nu >0}








K

ν


(
x
)


{\displaystyle K_{\nu }(x)}

: Modified Bessel function of the second kind[1]




In probability and statistics, Student's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. It was developed by William Sealy Gosset under the pseudonym Student. Whereas a normal distribution describes a full population, t-distributions describe samples drawn from a full population; accordingly, the t-distribution for each sample size is different, and the larger the sample, the more the distribution resembles a normal distribution.
The t-distribution plays a role in a number of widely used statistical analyses, including Student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. The Student's t-distribution also arises in the Bayesian analysis of data from a normal family.
If we take a sample of n observations from a normal distribution, then the t-distribution with 



ν
=
n
−
1


{\displaystyle \nu =n-1}

 degrees of freedom can be defined as the distribution of the location of the true mean, relative to the sample mean and divided by the sample standard deviation, after multiplying by the normalizing term 





n




{\displaystyle {\sqrt {n}}}

. In this way, the t-distribution can be used to say how confident you are that any given range contains the true mean.
The t-distribution is symmetric and bell-shaped, like the normal distribution, but has heavier tails, meaning that it is more prone to producing values that fall far from its mean. This makes it useful for understanding the statistical behavior of certain types of ratios of random quantities, in which variation in the denominator is amplified and may produce outlying values when the denominator of the ratio falls close to zero. The Student's t-distribution is a special case of the generalised hyperbolic distribution.



Contents


1 History and etymology
2 How Student's distribution arises from sampling
3 Definition

3.1 Probability density function
3.2 Cumulative distribution function
3.3 Special cases


4 How the t-distribution arises

4.1 Sampling distribution
4.2 Bayesian inference


5 Characterization

5.1 As the distribution of a test statistic

5.1.1 Derivation


5.2 As a maximum entropy distribution


6 Properties

6.1 Moments
6.2 Monte Carlo sampling
6.3 Integral of Student's probability density function and p-value
6.4 Differential equation


7 Non-standardized Student's t-distribution

7.1 In terms of scaling parameter σ, or σ2
7.2 In terms of inverse scaling parameter λ


8 Related distributions
9 Uses

9.1 In frequentist statistical inference

9.1.1 Hypothesis testing
9.1.2 Confidence intervals
9.1.3 Prediction intervals


9.2 In Bayesian statistics
9.3 Robust parametric modeling


10 Table of selected values
11 See also
12 Notes
13 References
14 External links



History and etymology[edit]




Statistician William Sealy Gosset, known as "Student"


In statistics, the t-distribution was first derived as a posterior distribution in 1876 by Helmert[2][3][4] and Lüroth.[5][6][7] The t-distribution also appeared in a more general form as Pearson Type IV distribution in Karl Pearson's 1895 paper.
In the English-language literature the distribution takes its name from William Sealy Gosset's 1908 paper in Biometrika under the pseudonym "Student".[8][9] Gosset worked at the Guinness Brewery in Dublin, Ireland, and was interested in the problems of small samples – for example, the chemical properties of barley where sample sizes might be as few as 3. One version of the origin of the pseudonym is that Gosset's employer preferred staff to use pen names when publishing scientific papers instead of their real name, so he used the name "Student" to hide his identity. Another version is that Guinness did not want their competitors to know that they were using the t-test to determine the quality of raw material.[10][11]
Gosset's paper refers to the distribution as the "frequency distribution of standard deviations of samples drawn from a normal population". It became well-known through the work of Ronald Fisher, who called the distribution "Student's distribution" and represented the test value with the letter t.[12][13]
How Student's distribution arises from sampling[edit]
Let X1, ..., Xn be independent and identically distributed as N(μ, σ2), i.e. this is a sample of size n from a normally distributed population with expected value μ and variance σ2.
Let








X
¯



=


1
n



∑

i
=
1


n



X

i




{\displaystyle {\bar {X}}={\frac {1}{n}}\sum _{i=1}^{n}X_{i}}



be the sample mean and let






S

2


=


1

n
−
1




∑

i
=
1


n


(

X

i


−



X
¯




)

2




{\displaystyle S^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}(X_{i}-{\bar {X}})^{2}}



be the (Bessel-corrected) sample variance. Then the random variable











X
¯



−
μ


σ

/



n







{\displaystyle {\frac {{\bar {X}}-\mu }{\sigma /{\sqrt {n}}}}}



has a standard normal distribution (i.e. normal with expected value 0 and variance 1), and the random variable











X
¯



−
μ


S

/



n







{\displaystyle {\frac {{\bar {X}}-\mu }{S/{\sqrt {n}}}}}



(where σ has been substituted for S) has a Student's t-distribution with n − 1 degrees of freedom.
Definition[edit]
Probability density function[edit]
Student's t-distribution has the probability density function given by





f
(
t
)
=



Γ
(



ν
+
1

2


)




ν
π



Γ
(


ν
2


)





(
1
+



t

2


ν


)



−



ν
+
1

2




,



{\displaystyle f(t)={\frac {\Gamma ({\frac {\nu +1}{2}})}{{\sqrt {\nu \pi }}\,\Gamma ({\frac {\nu }{2}})}}\left(1+{\frac {t^{2}}{\nu }}\right)^{\!-{\frac {\nu +1}{2}}},\!}



where 



ν


{\displaystyle \nu }

 is the number of degrees of freedom and 



Γ


{\displaystyle \Gamma }

 is the gamma function. This may also be written as





f
(
t
)
=


1



ν




B

(


1
2


,


ν
2


)





(
1
+



t

2


ν


)



−



ν
+
1

2





,


{\displaystyle f(t)={\frac {1}{{\sqrt {\nu }}\,\mathrm {B} ({\frac {1}{2}},{\frac {\nu }{2}})}}\left(1+{\frac {t^{2}}{\nu }}\right)^{\!-{\frac {\nu +1}{2}}}\!,}



where B is the Beta function.
For 



ν


{\displaystyle \nu }

 even,








Γ
(



ν
+
1

2


)




ν
π



Γ
(


ν
2


)



=



(
ν
−
1
)
(
ν
−
3
)
⋯
5
⋅
3


2


ν


(
ν
−
2
)
(
ν
−
4
)
⋯
4
⋅
2




⋅


{\displaystyle {\frac {\Gamma ({\frac {\nu +1}{2}})}{{\sqrt {\nu \pi }}\,\Gamma ({\frac {\nu }{2}})}}={\frac {(\nu -1)(\nu -3)\cdots 5\cdot 3}{2{\sqrt {\nu }}(\nu -2)(\nu -4)\cdots 4\cdot 2\,}}\cdot }



For 



ν


{\displaystyle \nu }

 odd,








Γ
(



ν
+
1

2


)




ν
π



Γ
(


ν
2


)



=



(
ν
−
1
)
(
ν
−
3
)
⋯
4
⋅
2


π


ν


(
ν
−
2
)
(
ν
−
4
)
⋯
5
⋅
3




⋅



{\displaystyle {\frac {\Gamma ({\frac {\nu +1}{2}})}{{\sqrt {\nu \pi }}\,\Gamma ({\frac {\nu }{2}})}}={\frac {(\nu -1)(\nu -3)\cdots 4\cdot 2}{\pi {\sqrt {\nu }}(\nu -2)(\nu -4)\cdots 5\cdot 3\,}}\cdot \!}



The probability density function is symmetric, and its overall shape resembles the bell shape of a normally distributed variable with mean 0 and variance 1, except that it is a bit lower and wider. As the number of degrees of freedom grows, the t-distribution approaches the normal distribution with mean 0 and variance 1.
The following images show the density of the t-distribution for increasing values of 



ν


{\displaystyle \nu }

. The normal distribution is shown as a blue line for comparison. Note that the t-distribution (red line) becomes closer to the normal distribution as 



ν


{\displaystyle \nu }

 increases.

Density of the t-distribution (red) for 1, 2, 3, 5, 10, and 30 degrees of freedom compared to the standard normal distribution (blue).
Previous plots shown in green.







1 degree of freedom










2 degrees of freedom










3 degrees of freedom












5 degrees of freedom










10 degrees of freedom










30 degrees of freedom






Cumulative distribution function[edit]
The cumulative distribution function can be written in terms of I, the regularized incomplete beta function. For t > 0,[14]





F
(
t
)
=

∫

−
∞


t


f
(
u
)

d
u
=
1
−



1
2




I

x
(
t
)



(



ν
2



,



1
2



)

,


{\displaystyle F(t)=\int _{-\infty }^{t}f(u)\,du=1-{\tfrac {1}{2}}I_{x(t)}\left({\tfrac {\nu }{2}},{\tfrac {1}{2}}\right),}



where





x
(
t
)
=


ν


t

2


+
ν



.


{\displaystyle x(t)={\frac {\nu }{t^{2}+\nu }}.}



Other values would be obtained by symmetry. An alternative formula, valid for 




t

2


<
ν


{\displaystyle t^{2}<\nu }

, is[14]






∫

−
∞


t


f
(
u
)

d
u
=



1
2



+
t



Γ

(



1
2



(
ν
+
1
)
)





π
ν



Γ

(



ν
2



)









2



F

1



(



1
2



,



1
2



(
ν
+
1
)
;



3
2



;
−




t

2


ν



)

,


{\displaystyle \int _{-\infty }^{t}f(u)\,du={\tfrac {1}{2}}+t{\frac {\Gamma \left({\tfrac {1}{2}}(\nu +1)\right)}{{\sqrt {\pi \nu }}\,\Gamma \left({\tfrac {\nu }{2}}\right)}}\,{}_{2}F_{1}\left({\tfrac {1}{2}},{\tfrac {1}{2}}(\nu +1);{\tfrac {3}{2}};-{\tfrac {t^{2}}{\nu }}\right),}



where 2F1 is a particular case of the hypergeometric function.
For information on its inverse cumulative distribution function, see quantile function#Student's t-distribution.
Special cases[edit]
Certain values of 



ν


{\displaystyle \nu }

 give an especially simple form.





ν
=
1


{\displaystyle \nu =1}




Distribution function:








F
(
t
)
=



1
2



+



1
π



arctan
⁡
(
t
)
.


{\displaystyle F(t)={\tfrac {1}{2}}+{\tfrac {1}{\pi }}\arctan(t).}






Density function:








f
(
t
)
=


1

π
(
1
+

t

2


)



.


{\displaystyle f(t)={\frac {1}{\pi (1+t^{2})}}.}






See Cauchy distribution






ν
=
2


{\displaystyle \nu =2}




Distribution function:








F
(
t
)
=



1
2



+


t

2


2
+

t

2







.


{\displaystyle F(t)={\tfrac {1}{2}}+{\frac {t}{2{\sqrt {2+t^{2}}}}}.}






Density function:








f
(
t
)
=


1


(
2
+

t

2


)



3
2





.


{\displaystyle f(t)={\frac {1}{\left(2+t^{2}\right)^{\frac {3}{2}}}}.}










ν
=
3


{\displaystyle \nu =3}




Density function:








f
(
t
)
=



6


3




π


(
3
+

t

2


)


2





.


{\displaystyle f(t)={\frac {6{\sqrt {3}}}{\pi \left(3+t^{2}\right)^{2}}}.}










ν
=
∞


{\displaystyle \nu =\infty }




Density function:








f
(
t
)
=


1

2
π




e

−



t

2


2




.


{\displaystyle f(t)={\frac {1}{\sqrt {2\pi }}}e^{-{\frac {t^{2}}{2}}}.}






See Normal distribution

How the t-distribution arises[edit]
Sampling distribution[edit]
Let x1, ..., xn be the numbers observed in a sample from a continuously distributed population with expected value μ. The sample mean and sample variance are given by:












x
¯






=




x

1


+
⋯
+

x

n



n


,





s

2





=


1

n
−
1




∑

i
=
1


n


(

x

i


−



x
¯




)

2


.






{\displaystyle {\begin{aligned}{\bar {x}}&={\frac {x_{1}+\cdots +x_{n}}{n}},\\s^{2}&={\frac {1}{n-1}}\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}.\end{aligned}}}



The resulting t-value is





t
=






x
¯



−
μ


s

/



n





.


{\displaystyle t={\frac {{\bar {x}}-\mu }{s/{\sqrt {n}}}}.}



The t-distribution with n − 1 degrees of freedom is the sampling distribution of the t-value when the samples consist of independent identically distributed observations from a normally distributed population. Thus for inference purposes t is a useful "pivotal quantity" in the case when the mean and variance (μ, σ2) are unknown population parameters, in the sense that the t-value has then a probability distribution that depends on neither μ nor σ2.
Bayesian inference[edit]
Main article: Bayesian inference
In Bayesian statistics, a (scaled, shifted) t-distribution arises as the marginal distribution of the unknown mean of a normal distribution, when the dependence on an unknown variance has been marginalised out:[15]









p
(
μ
∣
D
,
I
)
=



∫
p
(
μ
,

σ

2


∣
D
,
I
)

d

σ

2






=



∫
p
(
μ
∣
D
,

σ

2


,
I
)

p
(

σ

2


∣
D
,
I
)

d

σ

2


,






{\displaystyle {\begin{aligned}p(\mu \mid D,I)=&\int p(\mu ,\sigma ^{2}\mid D,I)\,d\sigma ^{2}\\=&\int p(\mu \mid D,\sigma ^{2},I)\,p(\sigma ^{2}\mid D,I)\,d\sigma ^{2},\end{aligned}}}



where D stands for the data {xi}, and I represents any other information that may have been used to create the model. The distribution is thus the compounding of the conditional distribution of μ given the data and σ2 with the marginal distribution of σ2 given the data.
With n data points, if uninformative, or flat, location and scale priors 



p
(
μ
∣

σ

2


,
I
)
=

const



{\displaystyle p(\mu \mid \sigma ^{2},I)={\text{const}}}

 and 



p
(

σ

2


∣
I
)
∝
1

/


σ

2




{\displaystyle p(\sigma ^{2}\mid I)\propto 1/\sigma ^{2}}

 can be taken for μ and σ2, then Bayes' theorem gives









p
(
μ
∣
D
,

σ

2


,
I
)



∼
N
(



x
¯



,

σ

2



/

n
)
,




p
(

σ

2


∣
D
,
I
)



∼

S
c
a
l
e
-
i
n
v
-

⁡

χ

2


(
ν
,

s

2


)
,






{\displaystyle {\begin{aligned}p(\mu \mid D,\sigma ^{2},I)&\sim N({\bar {x}},\sigma ^{2}/n),\\p(\sigma ^{2}\mid D,I)&\sim \operatorname {Scale-inv-} \chi ^{2}(\nu ,s^{2}),\end{aligned}}}



a normal distribution and a scaled inverse chi-squared distribution respectively, where 



ν
=
n
−
1


{\displaystyle \nu =n-1}

 and






s

2


=
∑



(

x

i


−



x
¯




)

2




n
−
1



.


{\displaystyle s^{2}=\sum {\frac {(x_{i}-{\bar {x}})^{2}}{n-1}}.}



The marginalisation integral thus becomes









p
(
μ
∣
D
,
I
)



∝

∫

0


∞




1


σ

2





exp
⁡

(
−


1

2

σ

2





n
(
μ
−



x
¯




)

2


)

⋅

σ

−
ν
−
2


exp
⁡
(
−
ν

s

2



/

2

σ

2


)

d

σ

2








∝

∫

0


∞



σ

−
ν
−
3


exp
⁡

(
−


1

2

σ

2






(
n
(
μ
−



x
¯




)

2


+
ν

s

2


)

)


d

σ

2


.






{\displaystyle {\begin{aligned}p(\mu \mid D,I)&\propto \int _{0}^{\infty }{\frac {1}{\sqrt {\sigma ^{2}}}}\exp \left(-{\frac {1}{2\sigma ^{2}}}n(\mu -{\bar {x}})^{2}\right)\cdot \sigma ^{-\nu -2}\exp(-\nu s^{2}/2\sigma ^{2})\,d\sigma ^{2}\\&\propto \int _{0}^{\infty }\sigma ^{-\nu -3}\exp \left(-{\frac {1}{2\sigma ^{2}}}\left(n(\mu -{\bar {x}})^{2}+\nu s^{2}\right)\right)\,d\sigma ^{2}.\end{aligned}}}



This can be evaluated by substituting 



z
=
A

/

2

σ

2




{\displaystyle z=A/2\sigma ^{2}}

, where 



A
=
n
(
μ
−



x
¯




)

2


+
ν

s

2




{\displaystyle A=n(\mu -{\bar {x}})^{2}+\nu s^{2}}

, giving





d
z
=
−


A

2

σ

4






d

σ

2


,


{\displaystyle dz=-{\frac {A}{2\sigma ^{4}}}\,d\sigma ^{2},}



so





p
(
μ
∣
D
,
I
)
∝

A

−



ν
+
1

2





∫

0


∞



z

(
ν
−
1
)

/

2


exp
⁡
(
−
z
)

d
z
.


{\displaystyle p(\mu \mid D,I)\propto A^{-{\frac {\nu +1}{2}}}\int _{0}^{\infty }z^{(\nu -1)/2}\exp(-z)\,dz.}



But the z integral is now a standard Gamma integral, which evaluates to a constant, leaving









p
(
μ
∣
D
,
I
)



∝

A

−



ν
+
1

2










∝


(
1
+



n
(
μ
−



x
¯




)

2




ν

s

2





)


−



ν
+
1

2




.






{\displaystyle {\begin{aligned}p(\mu \mid D,I)&\propto A^{-{\frac {\nu +1}{2}}}\\&\propto \left(1+{\frac {n(\mu -{\bar {x}})^{2}}{\nu s^{2}}}\right)^{-{\frac {\nu +1}{2}}}.\end{aligned}}}



This is a form of the t-distribution with an explicit scaling and shifting that will be explored in more detail in a further section below. It can be related to the standardised t-distribution by the substitution





t
=



μ
−



x
¯





s

/



n





.


{\displaystyle t={\frac {\mu -{\bar {x}}}{s/{\sqrt {n}}}}.}



The derivation above has been presented for the case of uninformative priors for μ and σ2; but it will be apparent that any priors that lead to a normal distribution being compounded with a scaled inverse chi-squared distribution will lead to a t-distribution with scaling and shifting for P(μ | D, I), although the scaling parameter corresponding to s2/n above will then be influenced both by the prior information and the data, rather than just by the data as above.
Characterization[edit]
As the distribution of a test statistic[edit]
Student's t-distribution with 



ν


{\displaystyle \nu }

 degrees of freedom can be defined as the distribution of the random variable T with[14][16]





T
=


Z

V

/

ν



=
Z



ν
V



,


{\displaystyle T={\frac {Z}{\sqrt {V/\nu }}}=Z{\sqrt {\frac {\nu }{V}}},}



where

Z is a standard normal with expected value 0 and variance 1;
V has a chi-squared distribution with 



ν


{\displaystyle \nu }

 degrees of freedom;
Z and V are independent.

A different distribution is defined as that of the random variable defined, for a given constant μ, by





(
Z
+
μ
)



ν
V



.


{\displaystyle (Z+\mu ){\sqrt {\frac {\nu }{V}}}.}



This random variable has a noncentral t-distribution with noncentrality parameter μ. This distribution is important in studies of the power of Student's t-test.
Derivation[edit]
Suppose X1, ..., Xn are independent realizations of the normally-distributed, random variable X, which has an expected value μ and variance σ2. Let








X
¯



n


=


1
n


(

X

1


+
⋯
+

X

n


)


{\displaystyle {\overline {X}}_{n}={\frac {1}{n}}(X_{1}+\cdots +X_{n})}



be the sample mean, and






S

n


2


=


1

n
−
1




∑

i
=
1


n




(

X

i


−



X
¯



n


)


2




{\displaystyle S_{n}^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}\left(X_{i}-{\overline {X}}_{n}\right)^{2}}



be an unbiased estimate of the variance from the sample. It can be shown that the random variable





V
=
(
n
−
1
)



S

n


2



σ

2






{\displaystyle V=(n-1){\frac {S_{n}^{2}}{\sigma ^{2}}}}



has a chi-squared distribution with 



ν
=
n
−
1


{\displaystyle \nu =n-1}

 degrees of freedom (by Cochran's theorem).[17] It is readily shown that the quantity





Z
=

(



X
¯



n


−
μ
)




n

σ




{\displaystyle Z=\left({\overline {X}}_{n}-\mu \right){\frac {\sqrt {n}}{\sigma }}}



is normally distributed with mean 0 and variance 1, since the sample mean 






X
¯



n




{\displaystyle {\overline {X}}_{n}}

 is normally distributed with mean μ and variance σ2/n. Moreover, it is possible to show that these two random variables (the normally distributed one Z and the chi-squared-distributed one V) are independent. Consequently[clarification needed] the pivotal quantity





T
≡


Z

V

/

ν



=

(



X
¯



n


−
μ
)




n


S

n




,


{\displaystyle T\equiv {\frac {Z}{\sqrt {V/\nu }}}=\left({\overline {X}}_{n}-\mu \right){\frac {\sqrt {n}}{S_{n}}},}



which differs from Z in that the exact standard deviation σ is replaced by the random variable Sn, has a Student's t-distribution as defined above. Notice that the unknown population variance σ2 does not appear in T, since it was in both the numerator and the denominator, so it canceled. Gosset intuitively obtained the probability density function stated above, with 



ν


{\displaystyle \nu }

 equal to n − 1, and Fisher proved it in 1925.[12]
The distribution of the test statistic T depends on 



ν


{\displaystyle \nu }

, but not μ or σ; the lack of dependence on μ and σ is what makes the t-distribution important in both theory and practice.
As a maximum entropy distribution[edit]
Student's t-distribution is the maximum entropy probability distribution for a random variate X for which 



E
(
ln
⁡
(
ν
+

X

2


)
)


{\displaystyle E(\ln(\nu +X^{2}))}

 is fixed.[18]
Properties[edit]
Moments[edit]
For 



ν
>
1


{\displaystyle \nu >1}

, the raw moments of the t-distribution are





E
(

T

k


)
=


{



0


k

 odd

,

0
<
k
<
ν






1



π


Γ

(


ν
2


)





[
Γ

(



k
+
1

2


)

Γ

(



ν
−
k

2


)


ν


k
2



]



k

 even

,

0
<
k
<
ν
.








{\displaystyle E(T^{k})={\begin{cases}0&k{\text{ odd}},\quad 0<k<\nu \\{\frac {1}{{\sqrt {\pi }}\Gamma \left({\frac {\nu }{2}}\right)}}\left[\Gamma \left({\frac {k+1}{2}}\right)\Gamma \left({\frac {\nu -k}{2}}\right)\nu ^{\frac {k}{2}}\right]&k{\text{ even}},\quad 0<k<\nu .\\\end{cases}}}



Moments of order 



ν


{\displaystyle \nu }

 or higher do not exist.[19]
The term for 



0
<
k
<
ν


{\displaystyle 0<k<\nu }

, k even, may be simplified using the properties of the gamma function to





E
(

T

k


)
=

ν


k
2





∏

i
=
1



k
2






2
i
−
1


ν
−
2
i




k

 even

,

0
<
k
<
ν
.


{\displaystyle E(T^{k})=\nu ^{\frac {k}{2}}\,\prod _{i=1}^{\frac {k}{2}}{\frac {2i-1}{\nu -2i}}\qquad k{\text{ even}},\quad 0<k<\nu .}



For a t-distribution with 



ν


{\displaystyle \nu }

 degrees of freedom, the expected value is 0 if 



ν
>
1


{\displaystyle \nu >1}

, and its variance is 





ν

ν
−
2





{\displaystyle {\frac {\nu }{\nu -2}}}

 if 



ν
>
2


{\displaystyle \nu >2}

. The skewness is 0 if 



ν
>
3


{\displaystyle \nu >3}

 and the excess kurtosis is 





6

ν
−
4





{\displaystyle {\frac {6}{\nu -4}}}

 if 



ν
>
4


{\displaystyle \nu >4}

.
Monte Carlo sampling[edit]
There are various approaches to constructing random samples from the Student's t-distribution. The matter depends on whether the samples are required on a stand-alone basis, or are to be constructed by application of a quantile function to uniform samples; e.g., in the multi-dimensional applications basis of copula-dependency.[citation needed] In the case of stand-alone sampling, an extension of the Box–Muller method and its polar form is easily deployed.[20] It has the merit that it applies equally well to all real positive degrees of freedom, ν, while many other candidate methods fail if ν is close to zero.[20]
Integral of Student's probability density function and p-value[edit]
The function A(t|ν) is the integral of Student's probability density function, f(t) between −t and t, for t ≥ 0. It thus gives the probability that a value of t less than that calculated from observed data would occur by chance. Therefore, the function A(t|ν) can be used when testing whether the difference between the means of two sets of data is statistically significant, by calculating the corresponding value of t and the probability of its occurrence if the two sets of data were drawn from the same population. This is used in a variety of situations, particularly in t-tests. For the statistic t, with ν degrees of freedom, A(t|ν) is the probability that t would be less than the observed value if the two means were the same (provided that the smaller mean is subtracted from the larger, so that t ≥ 0). It can be easily calculated from the cumulative distribution function Fν(t) of the t-distribution:





A
(
t

|

ν
)
=

F

ν


(
t
)
−

F

ν


(
−
t
)
=
1
−

I


ν

ν
+

t

2







(


ν
2


,


1
2


)

,


{\displaystyle A(t|\nu )=F_{\nu }(t)-F_{\nu }(-t)=1-I_{\frac {\nu }{\nu +t^{2}}}\left({\frac {\nu }{2}},{\frac {1}{2}}\right),}



where Ix is the regularized incomplete beta function (a, b).
For statistical hypothesis testing this function is used to construct the p-value.
Differential equation[edit]
The pdf of the t-distribution is a solution to the following differential equation:






{





(
ν
+

x

2


)


f
′

(
x
)
+
(
ν
+
1
)
x
f
(
x
)
=
0
,





f
(
1
)
=




ν

ν

/

2


(
ν
+
1

)

−


ν
2


−


1
2






B

(


ν
2


,


1
2


)









}



{\displaystyle \left\{{\begin{array}{l}\left(\nu +x^{2}\right)f'(x)+(\nu +1)xf(x)=0,\\[6pt]\displaystyle f(1)={\frac {\nu ^{\nu /2}(\nu +1)^{-{\frac {\nu }{2}}-{\frac {1}{2}}}}{B\left({\frac {\nu }{2}},{\frac {1}{2}}\right)}}\end{array}}\right\}}



where B(x,y) is a beta function.
Non-standardized Student's t-distribution[edit]
In terms of scaling parameter σ, or σ2[edit]
Student's t distribution can be generalized to a three parameter location-scale family, introducing a location parameter 



μ


{\displaystyle \mu }

 and a scale parameter 



σ


{\displaystyle \sigma }

, through the relation





X
=
μ
+
σ
T


{\displaystyle X=\mu +\sigma T}



or





T
=



X
−
μ

σ




{\displaystyle T={\frac {X-\mu }{\sigma }}}



This means that 






x
−
μ

σ




{\displaystyle {\frac {x-\mu }{\sigma }}}

 has a classic Student's t distribution with 



ν


{\displaystyle \nu }

 degrees of freedom.
The resulting non-standardized Student's t-distribution has a density defined by[21]





p
(
x
∣
ν
,
μ
,
σ
)
=



Γ
(



ν
+
1

2


)


Γ
(


ν
2


)


π
ν


σ





(
1
+


1
ν




(



x
−
μ

σ


)


2


)


−



ν
+
1

2






{\displaystyle p(x\mid \nu ,\mu ,\sigma )={\frac {\Gamma ({\frac {\nu +1}{2}})}{\Gamma ({\frac {\nu }{2}}){\sqrt {\pi \nu }}\sigma }}\left(1+{\frac {1}{\nu }}\left({\frac {x-\mu }{\sigma }}\right)^{2}\right)^{-{\frac {\nu +1}{2}}}}



Here, 



σ


{\displaystyle \sigma }

 does not correspond to a standard deviation: it is not the standard deviation of the scaled t distribution, which may not even exist; nor is it the standard deviation of the underlying normal distribution, which is unknown. 



σ


{\displaystyle \sigma }

 simply sets the overall scaling of the distribution. In the Bayesian derivation of the marginal distribution of an unknown normal mean 



μ


{\displaystyle \mu }

 above, 



σ


{\displaystyle \sigma }

 as used here corresponds to the quantity 





s

/



n






{\displaystyle \scriptstyle {s/{\sqrt {n}}}}

, where






s

2


=
∑



(

x

i


−



x
¯




)

2




n
−
1



.


{\displaystyle s^{2}=\sum {\frac {(x_{i}-{\bar {x}})^{2}}{n-1}}.}



Equivalently, the distribution can be written in terms of 




σ

2




{\displaystyle \sigma ^{2}}

, the square of this scale parameter:





p
(
x
∣
ν
,
μ
,

σ

2


)
=



Γ
(



ν
+
1

2


)


Γ
(


ν
2


)


π
ν

σ

2









(
1
+


1
ν





(
x
−
μ

)

2




σ

2




)


−



ν
+
1

2






{\displaystyle p(x\mid \nu ,\mu ,\sigma ^{2})={\frac {\Gamma ({\frac {\nu +1}{2}})}{\Gamma ({\frac {\nu }{2}}){\sqrt {\pi \nu \sigma ^{2}}}}}\left(1+{\frac {1}{\nu }}{\frac {(x-\mu )^{2}}{\sigma ^{2}}}\right)^{-{\frac {\nu +1}{2}}}}



Other properties of this version of the distribution are:[21]









E
⁡
(
X
)



=
μ



for 


ν
>
1
,




var
⁡
(
X
)



=

σ

2




ν

ν
−
2






for 


ν
>
2
,




mode
⁡
(
X
)



=
μ
.






{\displaystyle {\begin{aligned}\operatorname {E} (X)&=\mu &{\text{for }}\,\nu >1,\\\operatorname {var} (X)&=\sigma ^{2}{\frac {\nu }{\nu -2}}&{\text{for }}\,\nu >2,\\\operatorname {mode} (X)&=\mu .\end{aligned}}}



This distribution results from compounding a Gaussian distribution (normal distribution) with mean 



μ


{\displaystyle \mu }

 and unknown variance, with an inverse gamma distribution placed over the variance with parameters 



a
=
ν

/

2


{\displaystyle a=\nu /2}

 and 



b
=
ν

σ

2



/

2


{\displaystyle b=\nu \sigma ^{2}/2}

. In other words, the random variable X is assumed to have a Gaussian distribution with an unknown variance distributed as inverse gamma, and then the variance is marginalized out (integrated out). The reason for the usefulness of this characterization is that the inverse gamma distribution is the conjugate prior distribution of the variance of a Gaussian distribution. As a result, the non-standardized Student's t-distribution arises naturally in many Bayesian inference problems. See below.
Equivalently, this distribution results from compounding a Gaussian distribution with a scaled-inverse-chi-squared distribution with parameters 



ν


{\displaystyle \nu }

 and 




σ

2




{\displaystyle \sigma ^{2}}

. The scaled-inverse-chi-squared distribution is exactly the same distribution as the inverse gamma distribution, but with a different parameterization, i.e. 



ν
=
2
a
,

σ

2


=
b

/

a


{\displaystyle \nu =2a,\sigma ^{2}=b/a}

.
In terms of inverse scaling parameter λ[edit]
An alternative parameterization in terms of an inverse scaling parameter 



λ


{\displaystyle \lambda }

 (analogous to the way precision is the reciprocal of variance), defined by the relation 



λ
=


1

σ

2






{\displaystyle \lambda ={\frac {1}{\sigma ^{2}}}}

. Then the density is defined by[22]





p
(
x

|

ν
,
μ
,
λ
)
=



Γ
(



ν
+
1

2


)


Γ
(


ν
2


)





(


λ

π
ν



)



1
2





(
1
+



λ
(
x
−
μ

)

2



ν


)


−



ν
+
1

2




.


{\displaystyle p(x|\nu ,\mu ,\lambda )={\frac {\Gamma ({\frac {\nu +1}{2}})}{\Gamma ({\frac {\nu }{2}})}}\left({\frac {\lambda }{\pi \nu }}\right)^{\frac {1}{2}}\left(1+{\frac {\lambda (x-\mu )^{2}}{\nu }}\right)^{-{\frac {\nu +1}{2}}}.}



Other properties of this version of the distribution are:[22]









E
⁡
(
X
)



=
μ




for 


ν
>
1
,





var

(
X
)



=


1
λ




ν

ν
−
2






for 


ν
>
2
,





mode

(
X
)



=
μ
.






{\displaystyle {\begin{aligned}\operatorname {E} (X)&=\mu \quad \quad \quad {\text{for }}\,\nu >1,\\{\text{var}}(X)&={\frac {1}{\lambda }}{\frac {\nu }{\nu -2}}\,\quad {\text{for }}\,\nu >2,\\{\text{mode}}(X)&=\mu .\end{aligned}}}



This distribution results from compounding a Gaussian distribution with mean 



μ


{\displaystyle \mu }

 and unknown precision (the reciprocal of the variance), with a gamma distribution placed over the precision with parameters 



a
=
ν

/

2


{\displaystyle a=\nu /2}

 and 



b
=
ν

/

(
2
λ
)


{\displaystyle b=\nu /(2\lambda )}

. In other words, the random variable X is assumed to have a normal distribution with an unknown precision distributed as gamma, and then this is marginalized over the gamma distribution.
Related distributions[edit]

If X ~ t(ν) has a Student's t-distribution then X2 has an F-distribution: 




X

2


∼

F

(

ν

1


=
1
,

ν

2


=
ν
)


{\displaystyle X^{2}\sim \mathrm {F} (\nu _{1}=1,\nu _{2}=\nu )}


The noncentral t-distribution generalizes the t-distribution to include a location parameter. Unlike the nonstandardized t-distributions, the noncentral distributions are not symmetric (the median is not the same as the mode).
The discrete Student's t-distribution is defined by its probability mass function at r being proportional to:[23]







∏

j
=
1


k




1

(
r
+
j
+
a

)

2


+

b

2







r
=
…
,
−
1
,
0
,
1
,
…
.


{\displaystyle \prod _{j=1}^{k}{\frac {1}{(r+j+a)^{2}+b^{2}}}\quad \quad r=\ldots ,-1,0,1,\ldots .}


Here a, b, and k are parameters. This distribution arises from the construction of a system of discrete distributions similar to that of the Pearson distributions for continuous distributions.[24]


One can generate Student-t samples by taking the ratio of variables from the normal distribution and the square-root of chi-squared distribution. If we use instead of the normal distribution, e.g., the Irwin–Hall distribution, we obtain over-all a symmetric 4-parameter distribution, which includes the normal, the uniform, the triangular, the Student-t and the Cauchy distribution. This is also more flexible than some other symmetric generalizations of the Gaussian distribution.

Uses[edit]
In frequentist statistical inference[edit]
Student's t-distribution arises in a variety of statistical estimation problems where the goal is to estimate an unknown parameter, such as a mean value, in a setting where the data are observed with additive errors. If (as in nearly all practical statistical work) the population standard deviation of these errors is unknown and has to be estimated from the data, the t-distribution is often used to account for the extra uncertainty that results from this estimation. In most such problems, if the standard deviation of the errors were known, a normal distribution would be used instead of the t-distribution.
Confidence intervals and hypothesis tests are two statistical procedures in which the quantiles of the sampling distribution of a particular statistic (e.g. the standard score) are required. In any situation where this statistic is a linear function of the data, divided by the usual estimate of the standard deviation, the resulting quantity can be rescaled and centered to follow Student's t-distribution. Statistical analyses involving means, weighted means, and regression coefficients all lead to statistics having this form.
Quite often, textbook problems will treat this the population standard deviation as if it were known and thereby avoid the need to use the Student's t-distribution. These problems are generally of two kinds: (1) those in which the sample size is so large that one may treat a data-based estimate of the variance as if it were certain, and (2) those that illustrate mathematical reasoning, in which the problem of estimating the standard deviation is temporarily ignored because that is not the point that the author or instructor is then explaining.
Hypothesis testing[edit]
A number of statistics can be shown to have t-distributions for samples of moderate size under null hypotheses that are of interest, so that the t-distribution forms the basis for significance tests. For example, the distribution of Spearman's rank correlation coefficient ρ, in the null case (zero correlation) is well approximated by the t distribution for sample sizes above about 20.[citation needed]
Confidence intervals[edit]
Suppose the number A is so chosen that





Pr
(
−
A
<
T
<
A
)
=
0.9
,


{\displaystyle \Pr(-A<T<A)=0.9,}



when T has a t-distribution with n − 1 degrees of freedom. By symmetry, this is the same as saying that A satisfies





Pr
(
T
<
A
)
=
0.95
,


{\displaystyle \Pr(T<A)=0.95,}



so A is the "95th percentile" of this probability distribution, or 



A
=

t

(
0.05
,
n
−
1
)




{\displaystyle A=t_{(0.05,n-1)}}

. Then





Pr

(
−
A
<






X
¯



n


−
μ



S

n



n




<
A
)

=
0.9
,


{\displaystyle \Pr \left(-A<{\frac {{\overline {X}}_{n}-\mu }{\frac {S_{n}}{\sqrt {n}}}}<A\right)=0.9,}



and this is equivalent to





Pr

(



X
¯



n


−
A



S

n



n



<
μ
<



X
¯



n


+
A



S

n



n



)

=
0.9.


{\displaystyle \Pr \left({\overline {X}}_{n}-A{\frac {S_{n}}{\sqrt {n}}}<\mu <{\overline {X}}_{n}+A{\frac {S_{n}}{\sqrt {n}}}\right)=0.9.}



Therefore, the interval whose endpoints are








X
¯



n


±
A



S

n



n





{\displaystyle {\overline {X}}_{n}\pm A{\frac {S_{n}}{\sqrt {n}}}}



is a 90% confidence interval for μ. Therefore, if we find the mean of a set of observations that we can reasonably expect to have a normal distribution, we can use the t-distribution to examine whether the confidence limits on that mean include some theoretically predicted value – such as the value predicted on a null hypothesis.
It is this result that is used in the Student's t-tests: since the difference between the means of samples from two normal distributions is itself distributed normally, the t-distribution can be used to examine whether that difference can reasonably be supposed to be zero.
If the data are normally distributed, the one-sided (1 − a)-upper confidence limit (UCL) of the mean, can be calculated using the following equation:







U
C
L


1
−
a


=



X
¯



n


+

t

a
,
n
−
1





S

n



n



.


{\displaystyle \mathrm {UCL} _{1-a}={\overline {X}}_{n}+t_{a,n-1}{\frac {S_{n}}{\sqrt {n}}}.}



The resulting UCL will be the greatest average value that will occur for a given confidence interval and population size. In other words, 






X
¯



n




{\displaystyle {\overline {X}}_{n}}

 being the mean of the set of observations, the probability that the mean of the distribution is inferior to UCL1−a is equal to the confidence level 1 − a.
Prediction intervals[edit]
The t-distribution can be used to construct a prediction interval for an unobserved sample from a normal distribution with unknown mean and variance.
In Bayesian statistics[edit]
The Student's t-distribution, especially in its three-parameter (location-scale) version, arises frequently in Bayesian statistics as a result of its connection with the normal distribution. Whenever the variance of a normally distributed random variable is unknown and a conjugate prior placed over it that follows an inverse gamma distribution, the resulting marginal distribution of the variable will follow a Student's t-distribution. Equivalent constructions with the same results involve a conjugate scaled-inverse-chi-squared distribution over the variance, or a conjugate gamma distribution over the precision. If an improper prior proportional to σ−2 is placed over the variance, the t-distribution also arises. This is the case regardless of whether the mean of the normally distributed variable is known, is unknown distributed according to a conjugate normally distributed prior, or is unknown distributed according to an improper constant prior.
Related situations that also produce a t-distribution are:

The marginal posterior distribution of the unknown mean of a normally distributed variable, with unknown prior mean and variance following the above model.
The prior predictive distribution and posterior predictive distribution of a new normally distributed data point when a series of independent identically distributed normally distributed data points have been observed, with prior mean and variance as in the above model.

Robust parametric modeling[edit]
The t-distribution is often used as an alternative to the normal distribution as a model for data, which often has heavier tails than the normal distribution allows for; see e.g. Lange et al.[25] The classical approach was to identify outliers and exclude or downweight them in some way. However, it is not always easy to identify outliers (especially in high dimensions), and the t-distribution is a natural choice of model for such data and provides a parametric approach to robust statistics.
A Bayesian account can be found in Gelman et al.[26] The degrees of freedom parameter controls the kurtosis of the distribution and is correlated with the scale parameter. The likelihood can have multiple local maxima and, as such, it is often necessary to fix the degrees of freedom at a fairly low value and estimate the other parameters taking this as given. Some authors[citation needed] report that values between 3 and 9 are often good choices. Venables and Ripley[citation needed] suggest that a value of 5 is often a good choice.
Table of selected values[edit]
Most statistical textbooks list t-distribution tables. Nowadays, the better way to a fully precise critical t value or a cumulative probability is the statistical function implemented in spreadsheets, or an interactive calculating web page. The relevant spreadsheet functions are TDIST and TINV, while online calculating pages save troubles like positions of parameters or names of functions.
The following table lists a few selected values for t-distributions with ν degrees of freedom for a range of one-sided or two-sided critical regions. For an example of how to read this table, take the fourth row, which begins with 4; that means ν, the number of degrees of freedom, is 4 (and if we are dealing, as above, with n values with a fixed sum, n = 5). Take the fifth entry, in the column headed 95% for one-sided (90% for two-sided). The value of that entry is 2.132. Then the probability that T is less than 2.132 is 95% or Pr(−∞ < T < 2.132) = 0.95; this also means that Pr(−2.132 < T < 2.132) = 0.9.
This can be calculated by the symmetry of the distribution,

Pr(T < −2.132) = 1 − Pr(T > −2.132) = 1 − 0.95 = 0.05,

and so

Pr(−2.132 < T < 2.132) = 1 − 2(0.05) = 0.9.

Note that the last row also gives critical points: a t-distribution with infinitely many degrees of freedom is a normal distribution. (See Related distributions above).
The first column is the number of degrees of freedom.


One-sided
75%
80%
85%
90%
95%
97.5%
99%
99.5%
99.75%
99.9%
99.95%


Two-sided
50%
60%
70%
80%
90%
95%
98%
99%
99.5%
99.8%
99.9%


1
1.000
1.376
1.963
3.078
6.314
12.71
31.82
63.66
127.3
318.3
636.6


2
0.816
1.080
1.386
1.886
2.920
4.303
6.965
9.925
14.09
22.33
31.60


3
0.765
0.978
1.250
1.638
2.353
3.182
4.541
5.841
7.453
10.21
12.92


4
0.741
0.941
1.190
1.533
2.132
2.776
3.747
4.604
5.598
7.173
8.610


5
0.727
0.920
1.156
1.476
2.015
2.571
3.365
4.032
4.773
5.893
6.869


6
0.718
0.906
1.134
1.440
1.943
2.447
3.143
3.707
4.317
5.208
5.959


7
0.711
0.896
1.119
1.415
1.895
2.365
2.998
3.499
4.029
4.785
5.408


8
0.706
0.889
1.108
1.397
1.860
2.306
2.896
3.355
3.833
4.501
5.041


9
0.703
0.883
1.100
1.383
1.833
2.262
2.821
3.250
3.690
4.297
4.781


10
0.700
0.879
1.093
1.372
1.812
2.228
2.764
3.169
3.581
4.144
4.587


11
0.697
0.876
1.088
1.363
1.796
2.201
2.718
3.106
3.497
4.025
4.437


12
0.695
0.873
1.083
1.356
1.782
2.179
2.681
3.055
3.428
3.930
4.318


13
0.694
0.870
1.079
1.350
1.771
2.160
2.650
3.012
3.372
3.852
4.221


14
0.692
0.868
1.076
1.345
1.761
2.145
2.624
2.977
3.326
3.787
4.140


15
0.691
0.866
1.074
1.341
1.753
2.131
2.602
2.947
3.286
3.733
4.073


16
0.690
0.865
1.071
1.337
1.746
2.120
2.583
2.921
3.252
3.686
4.015


17
0.689
0.863
1.069
1.333
1.740
2.110
2.567
2.898
3.222
3.646
3.965


18
0.688
0.862
1.067
1.330
1.734
2.101
2.552
2.878
3.197
3.610
3.922


19
0.688
0.861
1.066
1.328
1.729
2.093
2.539
2.861
3.174
3.579
3.883


20
0.687
0.860
1.064
1.325
1.725
2.086
2.528
2.845
3.153
3.552
3.850


21
0.686
0.859
1.063
1.323
1.721
2.080
2.518
2.831
3.135
3.527
3.819


22
0.686
0.858
1.061
1.321
1.717
2.074
2.508
2.819
3.119
3.505
3.792


23
0.685
0.858
1.060
1.319
1.714
2.069
2.500
2.807
3.104
3.485
3.767


24
0.685
0.857
1.059
1.318
1.711
2.064
2.492
2.797
3.091
3.467
3.745


25
0.684
0.856
1.058
1.316
1.708
2.060
2.485
2.787
3.078
3.450
3.725


26
0.684
0.856
1.058
1.315
1.706
2.056
2.479
2.779
3.067
3.435
3.707


27
0.684
0.855
1.057
1.314
1.703
2.052
2.473
2.771
3.057
3.421
3.690


28
0.683
0.855
1.056
1.313
1.701
2.048
2.467
2.763
3.047
3.408
3.674


29
0.683
0.854
1.055
1.311
1.699
2.045
2.462
2.756
3.038
3.396
3.659


30
0.683
0.854
1.055
1.310
1.697
2.042
2.457
2.750
3.030
3.385
3.646


40
0.681
0.851
1.050
1.303
1.684
2.021
2.423
2.704
2.971
3.307
3.551


50
0.679
0.849
1.047
1.299
1.676
2.009
2.403
2.678
2.937
3.261
3.496


60
0.679
0.848
1.045
1.296
1.671
2.000
2.390
2.660
2.915
3.232
3.460


80
0.678
0.846
1.043
1.292
1.664
1.990
2.374
2.639
2.887
3.195
3.416


100
0.677
0.845
1.042
1.290
1.660
1.984
2.364
2.626
2.871
3.174
3.390


120
0.677
0.845
1.041
1.289
1.658
1.980
2.358
2.617
2.860
3.160
3.373


∞
0.674
0.842
1.036
1.282
1.645
1.960
2.326
2.576
2.807
3.090
3.291


The number at the beginning of each row in the table above is ν, which has been defined above as n − 1. The percentage along the top is 100%(1 − α). The numbers in the main body of the table are tα, ν. If a quantity T is distributed as a Student's t-distribution with ν degrees of freedom, then there is a probability 1 − α that T will be less than tα, ν. (Calculated as for a one-tailed or one-sided test, as opposed to a two-tailed test.)
For example, given a sample with a sample variance 2 and sample mean of 10, taken from a sample set of 11 (10 degrees of freedom), using the formula








X
¯



n


±
A



S

n



n



,


{\displaystyle {\overline {X}}_{n}\pm A{\frac {S_{n}}{\sqrt {n}}},}



we can determine that at 90% confidence, we have a true mean lying below





10
+
1.37218



2


11



=
10.58510.


{\displaystyle 10+1.37218{\frac {\sqrt {2}}{\sqrt {11}}}=10.58510.}



In other words, on average, 90% of the times that an upper threshold is calculated by this method, this upper threshold exceeds the true mean.
And, still at 90% confidence, we have a true mean lying over





10
−
1.37218



2


11



=
9.41490.


{\displaystyle 10-1.37218{\frac {\sqrt {2}}{\sqrt {11}}}=9.41490.}



In other words, on average, 90% of the times that a lower threshold is calculated by this method, this lower threshold lies below the true mean.
So that at 80% confidence (calculated from 1 − 2 × (1 − 90%) = 80%), we have a true mean lying within the interval






(
10
−
1.37218



2


11



,
10
+
1.37218



2


11



)

=
(
9.41490
,
10.58510
)
.


{\displaystyle \left(10-1.37218{\frac {\sqrt {2}}{\sqrt {11}}},10+1.37218{\frac {\sqrt {2}}{\sqrt {11}}}\right)=(9.41490,10.58510).}



In other words, on average, 80% of the times that upper and lower thresholds are calculated by this method, the true mean is both below the upper threshold and above the lower threshold. This is not the same thing as saying that there is an 80% probability that the true mean lies between a particular pair of upper and lower thresholds that have been calculated by this method; see confidence interval and prosecutor's fallacy.
See also[edit]


Statistics portal




Chi-squared distribution
F-distribution
Gamma distribution
Folded-t and half-t distributions
Hotelling's T-squared distribution
Multivariate Student distribution
t-statistic
Tau-distribution, for internally studentized residuals
Wilks' lambda distribution
Wishart distribution


Notes[edit]


^ Hurst, Simon. The Characteristic Function of the Student-t Distribution, Financial Mathematics Research Report No. FMRR006-95, Statistics Research Report No. SRR044-95 Archived February 18, 2010, at the Wayback Machine.
^ Helmert, F. R. (1875). "Über die Bestimmung des wahrscheinlichen Fehlers aus einer endlichen Anzahl wahrer Beobachtungsfehler". Z. Math. Phys., 20, 300–3.
^ Helmert, F. R. (1876a). "Über die Wahrscheinlichkeit der Potenzsummen der Beobachtungsfehler und uber einige damit in Zusammenhang stehende Fragen". Z. Math. Phys., 21, 192–218.
^ Helmert, F. R. (1876b). "Die Genauigkeit der Formel von Peters zur Berechnung des wahrscheinlichen Beobachtungsfehlers directer Beobachtungen gleicher Genauigkeit", Astron. Nachr., 88, 113–32.
^ Lüroth, J (1876). "Vergleichung von zwei Werten des wahrscheinlichen Fehlers". Astron. Nachr. 87 (14): 209–20. Bibcode:1876AN.....87..209L. doi:10.1002/asna.18760871402. 
^ Pfanzagl, J.; Sheynin, O. (1996). "A forerunner of the t-distribution (Studies in the history of probability and statistics XLIV)". Biometrika. 83 (4): 891–898. doi:10.1093/biomet/83.4.891. MR 1766040. 
^ Sheynin, O. (1995). "Helmert's work in the theory of errors". Arch. Hist. Exact Sci. 49: 73–104. doi:10.1007/BF00374700. 
^ "Student" [William Sealy Gosset] (March 1908). "The probable error of a mean" (PDF). Biometrika. 6 (1): 1–25. doi:10.1093/biomet/6.1.1. 
^ "Student" (William Sealy Gosset), original Biometrika paper as a scan.
^ M. Wendl (2016) Pseudonymous fame, Science, 351(6280), 1406.
^ Mortimer, Robert G. (2005). Mathematics for Physical Chemistry, 3rd ed. Academic Press. ISBN 0-12-508347-5 (p. 326).
^ a b Fisher, R. A. (1925). "Applications of "Student's" distribution" (PDF). Metron. 5: 90–104. 
^ Walpole, Ronald; Myers, Raymond; Myers, Sharon; Ye, Keying. (2002). Probability and Statistics for Engineers and Scientists, 7th edi. p. 237. Pearson Education. ISBN 81-7758-404-9
^ a b c Johnson, N. L., Kotz, S., Balakrishnan, N. (1995) Continuous Univariate Distributions, Volume 2, 2nd Edition. Wiley, ISBN 0-471-58494-0 (Chapter 28).
^ A. Gelman et al (1995), Bayesian Data Analysis, Chapman & Hall. ISBN 0-412-03991-5. p. 68
^ Hogg & Craig (1978), Sections 4.4 and 4.8.
^ Cochran, W. G. (April 1934). "The distribution of quadratic forms in a normal system, with applications to the analysis of covariance". Mathematical Proceedings of the Cambridge Philosophical Society. 30 (2): 178–191. Bibcode:1934PCPS...30..178C. doi:10.1017/S0305004100016595. 
^ Park, Sung Y.; Bera, Anil K. (2009). "Maximum entropy autoregressive conditional heteroskedasticity model" (PDF). Journal of Econometrics. Elsevier: 219–230. Retrieved 2011-06-02. 
^ See, for example, page 56 of Casella and Berger, Statistical Inference, 1990 Duxbury.
^ a b Bailey, R. W. (1994). "Polar Generation of Random Variates with the t-Distribution". Mathematics of Computation. 62 (206): 779–781. doi:10.2307/2153537. 
^ a b Jackman, Simon (2009). Bayesian Analysis for the Social Sciences. Wiley. p. 507. 
^ a b Bishop, C.M. (2006). Pattern recognition and machine learning. Springer. 
^ Ord, J.K. (1972) Families of Frequency Distributions, Griffin. ISBN 0-85264-137-0 (Table 5.1)
^ Ord, J.K. (1972) Families of Frequency Distributions, Griffin. ISBN 0-85264-137-0 (Chapter 5)
^ Lange, Kenneth L., Roderick JA Little, and Jeremy MG Taylor. "Robust statistical modeling using the t distribution." Journal of the American Statistical Association 84.408 (1989): 881-896.
^ Gelman, Andrew, et al. Bayesian data analysis, Chapter 12; Boca Raton, FL, USA: Chapman & Hall/CRC, 2014


References[edit]

Senn, S.; Richardson, W. (1994). "The first t-test". Statistics in Medicine. 13 (8): 785–803. doi:10.1002/sim.4780130802. PMID 8047737. 
Hogg, R. V.; Craig, A. T. (1978). Introduction to Mathematical Statistics. New York: Macmillan. 
Venables, W. N.; Ripley, B. D. (2002). Modern Applied Statistics with S (Fourth ed.). Springer. 
Gelman, Andrew; John B. Carlin; Hal S. Stern; Donald B. Rubin (2003). Bayesian Data Analysis (Second Edition). CRC/Chapman & Hall. ISBN 1-58488-388-X. 

External links[edit]

Hazewinkel, Michiel, ed. (2001), "Student distribution", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 
Earliest Known Uses of Some of the Words of Mathematics (S) (Remarks on the history of the term "Student's distribution")







v
t
e


Probability distributions







List






Discrete univariate
with finite support



Benford
Bernoulli
beta-binomial
binomial
categorical
hypergeometric
Poisson binomial
Rademacher
discrete uniform
Zipf
Zipf–Mandelbrot








Discrete univariate
with infinite support



beta negative binomial
Borel
Conway–Maxwell–Poisson
discrete phase-type
Delaporte
extended negative binomial
Gauss–Kuzmin
geometric
logarithmic
negative binomial
parabolic fractal
Poisson
Skellam
Yule–Simon
zeta








Continuous univariate
supported on a bounded interval



arcsine
ARGUS
Balding–Nichols
Bates
beta
beta rectangular
Irwin–Hall
Kumaraswamy
logit-normal
noncentral beta
raised cosine
reciprocal
triangular
U-quadratic
uniform
Wigner semicircle








Continuous univariate
supported on a semi-infinite interval



Benini
Benktander 1st kind
Benktander 2nd kind
beta prime
Burr
chi-squared
chi
Dagum
Davis
exponential-logarithmic
Erlang
exponential
F
folded normal
Flory–Schulz
Fréchet
gamma
gamma/Gompertz
generalized inverse Gaussian
Gompertz
half-logistic
half-normal
Hotelling's T-squared
hyper-Erlang
hyperexponential
hypoexponential
inverse chi-squared

scaled inverse chi-squared


inverse Gaussian
inverse gamma
Kolmogorov
Lévy
log-Cauchy
log-Laplace
log-logistic
log-normal
Lomax
matrix-exponential
Maxwell–Boltzmann
Maxwell–Jüttner
Mittag-Leffler
Nakagami
noncentral chi-squared
Pareto
phase-type
poly-Weibull
Rayleigh
relativistic Breit–Wigner
Rice
shifted Gompertz
truncated normal
type-2 Gumbel
Weibull

Discrete Weibull


Wilks's lambda








Continuous univariate
supported on the whole real line



Cauchy
exponential power
Fisher's z
Gaussian q
generalized normal
generalized hyperbolic
geometric stable
Gumbel
Holtsmark
hyperbolic secant
Johnson's SU
Landau
Laplace
asymmetric Laplace
logistic
noncentral t
normal (Gaussian)
normal-inverse Gaussian
skew normal
slash
stable
Student's t
type-1 Gumbel
Tracy–Widom
variance-gamma
Voigt








Continuous univariate
with support whose type varies



generalized extreme value
generalized Pareto
Marchenko–Pastur
q-exponential
q-Gaussian
q-Weibull
shifted log-logistic
Tukey lambda








Mixed continuous-discrete univariate



rectified Gaussian








Multivariate (joint)



Discrete
Ewens
multinomial
Dirichlet-multinomial
negative multinomial
Continuous
Dirichlet
generalized Dirichlet
multivariate normal
multivariate stable
multivariate t
normal-inverse-gamma
normal-gamma
Matrix-valued
inverse matrix gamma
inverse-Wishart
matrix normal
matrix t
matrix gamma
normal-inverse-Wishart
normal-Wishart
Wishart








Directional



Univariate (circular) directional
Circular uniform
univariate von Mises
wrapped normal
wrapped Cauchy
wrapped exponential
wrapped asymmetric Laplace
wrapped Lévy
Bivariate (spherical)
Kent
Bivariate (toroidal)
bivariate von Mises
Multivariate
von Mises–Fisher
Bingham








Degenerate and singular



Degenerate
Dirac delta function
Singular
Cantor








Families



Circular
compound Poisson
elliptical
exponential
natural exponential
location–scale
maximum entropy
mixture
Pearson
Tweedie
wrapped












v
t
e


Some common univariate probability distributions






Continuous



beta
Cauchy
chi-squared
exponential
F
gamma
Laplace
log-normal
normal
Pareto
Student's t
uniform
Weibull








Discrete



Bernoulli
binomial
discrete uniform
geometric
hypergeometric
negative binomial
Poisson









List of probability distributions










v
t
e


Statistics









Outline
Index













Descriptive statistics










Continuous data




Center



Mean

arithmetic
geometric
harmonic


Median
Mode








Dispersion



Variance
Standard deviation
Coefficient of variation
Percentile
Range
Interquartile range








Shape



Moments

Skewness
Kurtosis
L-moments













Count data



Index of dispersion








Summary tables



Grouped data
Frequency distribution
Contingency table








Dependence



Pearson product-moment correlation
Rank correlation

Spearman's rho
Kendall's tau


Partial correlation
Scatter plot








Graphics



Bar chart
Biplot
Box plot
Control chart
Correlogram
Fan chart
Forest plot
Histogram
Pie chart
Q–Q plot
Run chart
Scatter plot
Stem-and-leaf display
Radar chart



















Data collection










Study design



Population
Statistic
Effect size
Statistical power
Sample size determination
Missing data








Survey methodology



Sampling

stratified
cluster


Standard error
Opinion poll
Questionnaire








Controlled experiments



Design

control
optimal


Controlled trial
Randomized
Random assignment
Replication
Blocking
Interaction
Factorial experiment








Uncontrolled studies



Observational study
Natural experiment
Quasi-experiment



















Statistical inference










Statistical theory



Population
Statistic
Probability distribution
Sampling distribution

Order statistic


Empirical distribution

Density estimation


Statistical model

Lp space


Parameter

location
scale
shape


Parametric family

Likelihood (monotone)
Location–scale family
Exponential family


Completeness
Sufficiency
Statistical functional

Bootstrap
U
V


Optimal decision

loss function


Efficiency
Statistical distance

divergence


Asymptotics
Robustness








Frequentist inference




Point estimation



Estimating equations

Maximum likelihood
Method of moments
M-estimator
Minimum distance


Unbiased estimators

Mean-unbiased minimum-variance

Rao–Blackwellization
Lehmann–Scheffé theorem


Median unbiased


Plug-in








Interval estimation



Confidence interval
Pivot
Likelihood interval
Prediction interval
Tolerance interval
Resampling

Bootstrap
Jackknife










Testing hypotheses



1- & 2-tails
Power

Uniformly most powerful test


Permutation test

Randomization test


Multiple comparisons








Parametric tests



Likelihood-ratio
Wald
Score











Specific tests







Z (normal)
Student's t-test
F








Goodness of fit



Chi-squared
Kolmogorov–Smirnov
Anderson–Darling
Normality (Shapiro–Wilk)
Likelihood-ratio test
Model selection

Cross validation
AIC
BIC










Rank statistics



Sign

Sample median


Signed rank (Wilcoxon)

Hodges–Lehmann estimator


Rank sum (Mann–Whitney)
Nonparametric anova

1-way (Kruskal–Wallis)
2-way (Friedman)
Ordered alternative (Jonckheere–Terpstra)













Bayesian inference



Bayesian probability

prior
posterior


Credible interval
Bayes factor
Bayesian estimator

Maximum posterior estimator
























Correlation
Regression analysis













Correlation



Pearson product-moment
Partial correlation
Confounding variable
Coefficient of determination








Regression analysis



Errors and residuals
Regression model validation
Mixed effects models
Simultaneous equations models
Multivariate adaptive regression splines (MARS)








Linear regression



Simple linear regression
Ordinary least squares
General linear model
Bayesian regression








Non-standard predictors



Nonlinear regression
Nonparametric
Semiparametric
Isotonic
Robust
Heteroscedasticity
Homoscedasticity








Generalized linear model



Exponential families
Logistic (Bernoulli) / Binomial / Poisson regressions








Partition of variance



Analysis of variance (ANOVA, anova)
Analysis of covariance
Multivariate ANOVA
Degrees of freedom



















Categorical / Multivariate / Time-series / Survival analysis










Categorical



Cohen's kappa
Contingency table
Graphical model
Log-linear model
McNemar's test








Multivariate



Regression
Anova
Principal components
Canonical correlation
Discriminant analysis
Cluster analysis
Classification
Structural equation model

Factor analysis


Multivariate distributions

Elliptical distributions

Normal












Time-series




General



Decomposition
Trend
Stationarity
Seasonal adjustment
Exponential smoothing
Cointegration
Structural break
Granger causality








Specific tests



Dickey–Fuller
Johansen
Q-statistic (Ljung–Box)
Durbin–Watson
Breusch–Godfrey








Time domain



Autocorrelation (ACF)

partial (PACF)


Cross-correlation (XCF)
ARMA model
ARIMA model (Box–Jenkins)
Autoregressive conditional heteroskedasticity (ARCH)
Vector autoregression (VAR)








Frequency domain



Spectral density estimation
Fourier analysis
Wavelet











Survival




Survival function



Kaplan–Meier estimator (product limit)
Proportional hazards models
Accelerated failure time (AFT) model
First hitting time








Hazard function



Nelson–Aalen estimator








Test



Log-rank test






















Applications










Biostatistics



Bioinformatics
Clinical trials / studies
Epidemiology
Medical statistics








Engineering statistics



Chemometrics
Methods engineering
Probabilistic design
Process / quality control
Reliability
System identification








Social statistics



Actuarial science
Census
Crime statistics
Demography
Econometrics
National accounts
Official statistics
Population statistics
Psychometrics








Spatial statistics



Cartography
Environmental statistics
Geographic information system
Geostatistics
Kriging

















Category
Portal
Commons
 WikiProject









 
						Retrieved from "https://en.wikipedia.org/w/index.php?title=Student%27s_t-distribution&oldid=770634330"					
Categories: Continuous distributionsSpecial functionsNormal distributionCompound probability distributionsProbability distributions with non-finite varianceInfinitely divisible probability distributionsLocation-scale family probability distributionsHidden categories: Webarchive template wayback linksPages using ISBN magic linksPages using deprecated image syntaxWikipedia articles needing clarification from November 2012All articles with unsourced statementsArticles with unsourced statements from July 2011Articles with unsourced statements from November 2010Articles with unsourced statements from June 2015 



Navigation menu


Personal tools

Not logged inTalkContributionsCreate accountLog in 



Namespaces

Article
Talk




Variants









Views

Read
Edit
View history



More







Search



 







Navigation


Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store 



Interaction


HelpAbout WikipediaCommunity portalRecent changesContact page 



Tools


What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page 



Print/export


Create a bookDownload as PDFPrintable version 



In other projects


Wikimedia Commons 



Languages


العربيةCatalàČeštinaDeutschΕλληνικάEspañolفارسیFrançaisGalego한국어ItalianoעבריתBasa JawaNederlands日本語Norsk bokmålPolskiPortuguêsРусскийSimple EnglishSlovenčinaSlovenščinaBasa SundaSuomiSvenskaTürkçeУкраїнська中文 
Edit links 





 This page was last modified on 16 March 2017, at 17:03.
Text is available under the Creative Commons Attribution-ShareAlike License;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.


Privacy policy
About Wikipedia
Disclaimers
Contact Wikipedia
Developers
Cookie statement
Mobile view



 

 



(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.716","walltime":"0.946","ppvisitednodes":{"value":4285,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":306917,"limit":2097152},"templateargumentsize":{"value":29780,"limit":2097152},"expansiondepth":{"value":15,"limit":40},"expensivefunctioncount":{"value":4,"limit":500},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  432.365      1 -total"," 22.31%   96.478      1 Template:Reflist"," 19.73%   85.285      1 Template:Statistics"," 18.87%   81.602      1 Template:Navbox_with_collapsible_groups"," 14.28%   61.757     13 Template:Navbox"," 13.78%   59.561      9 Template:Cite_journal"," 13.02%   56.279      4 Template:Citation_needed"," 12.32%   53.287      1 Template:Clarify"," 11.70%   50.596      4 Template:Fix"," 11.09%   47.931      1 Template:Fix-span"]},"scribunto":{"limitreport-timeusage":{"value":"0.206","limit":"10.000"},"limitreport-memusage":{"value":5846763,"limit":52428800}},"cachereport":{"origin":"mw1177","timestamp":"20170404063254","ttl":2592000,"transientcontent":false}}});});(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":1044,"wgHostname":"mw1177"});});


